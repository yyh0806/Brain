# Brainç³»ç»Ÿæ¨¡å—èŒè´£è¯¦è§£ - "å»æ‹¿æ¯æ°´"å®ä¾‹åˆ†æ

## ç›®å½•
- [1. åœºæ™¯è®¾å®š](#1-åœºæ™¯è®¾å®š)
- [2. å®Œæ•´å¤„ç†æµç¨‹](#2-å®Œæ•´å¤„ç†æµç¨‹)
- [3. æ„ŸçŸ¥å±‚èŒè´£](#3-æ„ŸçŸ¥å±‚èŒè´£)
- [4. è®¤çŸ¥å±‚èŒè´£](#4-è®¤çŸ¥å±‚èŒè´£)
- [5. è§„åˆ’å±‚èŒè´£](#5-è§„åˆ’å±‚èŒè´£)
- [6. æ‰§è¡Œå±‚èŒè´£](#6-æ‰§è¡Œå±‚èŒè´£)
- [7. æ¨¡å—é—´åä½œå›¾](#7-æ¨¡å—é—´åä½œå›¾)
- [8. å…³é”®è®¾è®¡è¦ç‚¹](#8-å…³é”®è®¾è®¡è¦ç‚¹)

---

## 1. åœºæ™¯è®¾å®š

### ä»»åŠ¡æè¿°
**ç”¨æˆ·æŒ‡ä»¤**: "æœºå™¨äººï¼Œå»å¸®æˆ‘å»æ¡Œå­ä¸Šæ‹¿é‚£ä¸ªæ°´æ¯"

### ç¯å¢ƒä¿¡æ¯
```
åœºæ™¯: å®¶åº­å®¢å…
æœºå™¨äºº: æœåŠ¡å‹UGV (åœ°é¢ç§»åŠ¨æœºå™¨äºº)
ä½ç½®: åˆå§‹åœ¨æ²™å‘æ—
éšœç¢ç‰©: èŒ¶å‡ ã€æ¤…å­ã€åœ°æ¯¯
ç›®æ ‡ç‰©ä½“: æ°´æ¯ (åœ¨é¤æ¡Œä¸Š)
çº¦æŸ: éœ€è¦é¿éšœã€ä¿æŒç¨³å®šã€ä¸èƒ½æ‰“ç¿»æ°´æ¯
```

### æ—¶é—´çº¿
```
T0: æ”¶åˆ°æŒ‡ä»¤
T1: å®Œæˆæ„ŸçŸ¥
T2: å®Œæˆè®¤çŸ¥æ¨ç†
T3: å®Œæˆè§„åˆ’
T4: å¼€å§‹æ‰§è¡Œ
T5: å®Œæˆä»»åŠ¡
```

---

## 2. å®Œæ•´å¤„ç†æµç¨‹

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant DLG as å¯¹è¯ç®¡ç†å™¨
    participant COT as CoTæ¨ç†å¼•æ“
    participant WM as ä¸–ç•Œæ¨¡å‹
    participant PERC as æ„ŸçŸ¥å±‚
    participant PLAN as è§„åˆ’å±‚
    participant EXEC as æ‰§è¡Œå±‚
    participant ROBOT as æœºå™¨äººç¡¬ä»¶

    User->>DLG: "å»å¸®æˆ‘å»æ¡Œå­ä¸Šæ‹¿é‚£ä¸ªæ°´æ¯"
    DLG->>COT: è§£ææŒ‡ä»¤æ„å›¾

    PERC->>PERC: é‡‡é›†ä¼ æ„Ÿå™¨æ•°æ®
    PERC->>WM: æ›´æ–°ä¸–ç•ŒçŠ¶æ€
    WM->>COT: æä¾›å½“å‰ç¯å¢ƒä¸Šä¸‹æ–‡

    COT->>COT: æ¨ç†ä»»åŠ¡åˆ†è§£
    COT->>WM: æ£€æµ‹æ¡Œå­ä½ç½®
    COT->>WM: è¯†åˆ«æ°´æ¯çŠ¶æ€
    COT->>PLAN: è¾“å‡ºæ¨ç†ç»“è®º

    PLAN->>PLAN: ä»»åŠ¡åˆ†è§£
    PLAN->>PLAN: æ“ä½œåºåˆ—ç”Ÿæˆ
    PLAN->>EXEC: è¾“å‡ºæ“ä½œé˜Ÿåˆ—

    loop æ‰§è¡Œå¾ªç¯
        EXEC->>PERC: è·å–å®æ—¶æ„ŸçŸ¥
        PERC->>WM: æ›´æ–°çŠ¶æ€
        WM->>EXEC: ç¯å¢ƒåé¦ˆ
        EXEC->>ROBOT: å‘é€æ§åˆ¶æŒ‡ä»¤
        ROBOT->>EXEC: æ‰§è¡Œåé¦ˆ

        alt æ£€æµ‹åˆ°å¼‚å¸¸
            WM->>COT: è§¦å‘é‡è§„åˆ’
            COT->>PLAN: æ–°çš„è§„åˆ’å»ºè®®
            PLAN->>EXEC: æ›´æ–°æ“ä½œåºåˆ—
        end
    end

    EXEC->>DLG: ä»»åŠ¡å®Œæˆ
    DLG->>User: "å·²æˆåŠŸæ‹¿åˆ°æ°´æ¯"
```

---

## 3. æ„ŸçŸ¥å±‚èŒè´£

### 3.1 æ ¸å¿ƒèŒè´£
**"æˆ‘çœ‹åˆ°ä»€ä¹ˆï¼Ÿç°åœ¨çš„ç¯å¢ƒçŠ¶æ€å¦‚ä½•ï¼Ÿ"**

### 3.2 æ¨¡å—ç»„æˆ

#### 3.2.1 ä¼ æ„Ÿå™¨ç®¡ç†å™¨ (ROS2SensorManager)

**æ–‡ä»¶ä½ç½®**: `brain/perception/sensors/ros2_sensor_manager.py`

**èŒè´£**: ç®¡ç†å’Œé‡‡é›†å¤šä¼ æ„Ÿå™¨æ•°æ®

```python
# åœ¨"å»æ‹¿æ¯æ°´"ä»»åŠ¡ä¸­çš„å®é™…å·¥ä½œ

class SensorData:
    """ä¼ æ„Ÿå™¨æ•°æ®æ±‡æ€»"""

    # 1. RGBç›¸æœºæ•°æ® - è§†è§‰æ„ŸçŸ¥
    rgb_image: np.ndarray
    """ç”¨é€”: è¯†åˆ«æ¡Œå­ã€æ°´æ¯ç­‰ç‰©ä½“çš„è§†è§‰ç‰¹å¾"""

    # 2. æ·±åº¦ç›¸æœºæ•°æ® - è·ç¦»æ„ŸçŸ¥
    depth_image: np.ndarray
    """ç”¨é€”: æµ‹é‡åˆ°æ¡Œå­çš„è·ç¦»ã€æ°´æ¯çš„3Dä½ç½®"""

    # 3. æ¿€å…‰é›·è¾¾ - ç¯å¢ƒå»ºå›¾
    point_cloud: PointCloud2
    """ç”¨é€”: æ„å»ºæˆ¿é—´åœ°å›¾ï¼Œæ£€æµ‹éšœç¢ç‰©ä½ç½®"""

    # 4. IMU - å§¿æ€æ„ŸçŸ¥
    orientation: Quaternion
    """ç”¨é€”: ç¡®ä¿æœºå™¨äººç§»åŠ¨æ—¶ä¿æŒç¨³å®šï¼Œä¸ç¿»å€’"""

    # 5. è½®é€Ÿè®¡ - é‡Œç¨‹æ„ŸçŸ¥
    odometry: Odometry
    """ç”¨é€”: ä¼°ç®—æœºå™¨äººç§»åŠ¨è·ç¦»å’Œå½“å‰ä½ç½®"""

# å…·ä½“å·¥ä½œæµç¨‹
async def get_current_data():
    """è·å–å½“å‰æ„ŸçŸ¥æ•°æ®"""

    # 1. åŒæ­¥é‡‡é›†å„ä¼ æ„Ÿå™¨æ•°æ®
    rgb = await camera.get_rgb()           # æ•è·å½©è‰²å›¾åƒ
    depth = await camera.get_depth()        # æ•è·æ·±åº¦å›¾åƒ
    lidar = await lidar.scan()              # æ¿€å…‰æ‰«æ
    imu = await imu.read_orientation()      # è¯»å–å§¿æ€
    odom = await odom.read_position()       # è¯»å–é‡Œç¨‹

    # 2. æ—¶é—´æˆ³å¯¹é½
    timestamp = sync_timestamps([rgb, depth, lidar, imu, odom])

    # 3. æ•°æ®æ‰“åŒ…
    return SensorData(
        rgb_image=rgb.data,
        depth_image=depth.data,
        point_cloud=lidar.data,
        orientation=imu.data,
        odometry=odom.data,
        timestamp=timestamp
    )
```

**åœ¨"æ‹¿æ°´æ¯"ä»»åŠ¡ä¸­çš„è¾“å‡ºç¤ºä¾‹**:
```json
{
  "rgb_image": "640x480x3çš„å›¾åƒæ•°ç»„",
  "depth_image": "æ˜¾ç¤ºæ¡Œå­è·ç¦»1.5ç±³ï¼Œæ°´æ¯åœ¨æ¡Œå­ä¸Š0.8ç±³é«˜",
  "point_cloud": "æˆ¿é—´3Dç‚¹äº‘ï¼Œè¯†åˆ«å‡ºéšœç¢ç‰©ä½ç½®",
  "orientation": "æœºå™¨äººå½“å‰æœå‘é¤æ¡Œ",
  "odometry": "å½“å‰åœ¨æ²™å‘æ—(0, 0)ï¼Œè·ç¦»æ¡Œå­3ç±³"
}
```

#### 3.2.2 VLMè§†è§‰æ„ŸçŸ¥ (VLMPerception)

**æ–‡ä»¶ä½ç½®**: `brain/perception/vlm/vlm_perception.py`

**èŒè´£**: ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç†è§£åœºæ™¯è¯­ä¹‰

```python
# åœ¨"å»æ‹¿æ¯æ°´"ä»»åŠ¡ä¸­çš„å®é™…å·¥ä½œ

class VLMPerception:
    """è§†è§‰è¯­è¨€æ¨¡å‹æ„ŸçŸ¥"""

    async def analyze_scene(self, rgb_image):
        """
        åˆ†æRGBå›¾åƒï¼Œç†è§£åœºæ™¯

        è¾“å…¥: 640x480çš„RGBå›¾åƒ
        è¾“å‡º: åœºæ™¯è¯­ä¹‰ç†è§£ç»“æœ
        """

        # è°ƒç”¨Ollamaçš„LLaVAæ¨¡å‹
        prompt = f"""
        è¯·åˆ†æè¿™å¼ å›¾åƒï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
        1. å›¾ä¸­æœ‰æ¡Œå­å—ï¼Ÿä½ç½®åœ¨å“ªé‡Œï¼Ÿ
        2. æ¡Œä¸Šæœ‰æ°´æ¯å—ï¼Ÿæ˜¯ä»€ä¹ˆé¢œè‰²çš„ï¼Ÿ
        3. æ°´æ¯çš„çŠ¶æ€å¦‚ä½•ï¼ˆæ˜¯å¦æ»¡æ°´ã€æ˜¯å¦å€¾æ–œï¼‰ï¼Ÿ
        4. åˆ°è¾¾æ¡Œå­æœ‰ä»€ä¹ˆéšœç¢ç‰©ï¼Ÿ
        """

        response = await ollama.generate(
            model="llava:7b",
            images=[rgb_image],
            prompt=prompt
        )

        return SceneUnderstanding(
            objects=[
                SemanticObject(
                    type="table",
                    position={"x": 3.0, "y": 0.0, "z": 0.0},
                    confidence=0.95,
                    attributes={"material": "wood", "shape": "rectangular"}
                ),
                SemanticObject(
                    type="cup",
                    position={"x": 3.0, "y": 0.0, "z": 0.8},
                    confidence=0.92,
                    attributes={
                        "color": "blue",
                        "state": "upright",
                        "fullness": "half_full"
                    }
                )
            ],
            obstacles=[
                SemanticObject(type="chair", position={"x": 1.5, "y": 0.5}),
                SemanticObject(type="coffee_table", position={"x": 2.0, "y": -0.3})
            ],
            description="åœ¨é¤æ¡Œ(3ç±³å¤–)ä¸Šæœ‰ä¸€ä¸ªè“è‰²åŠæ»¡æ°´æ¯"
        )
```

**è¾“å‡ºç¤ºä¾‹**:
```json
{
  "description": "å‰æ–¹3ç±³å¤„æœ‰ä¸€å¼ æœ¨è´¨é¤æ¡Œï¼Œæ¡Œä¸Šæœ‰ä¸€ä¸ªè“è‰²åŠæ»¡çš„æ°´æ¯",
  "objects": [
    {"type": "table", "position": [3.0, 0.0, 0.0], "confidence": 0.95},
    {"type": "cup", "position": [3.0, 0.0, 0.8], "confidence": 0.92}
  ],
  "obstacles": [
    {"type": "chair", "position": [1.5, 0.5, 0.0]},
    {"type": "coffee_table", "position": [2.0, -0.3, 0.0]}
  ]
}
```

#### 3.2.3 å æ®æ …æ ¼åœ°å›¾ (OccupancyMapper)

**èŒè´£**: æ„å»ºç¯å¢ƒçš„2D/3Då æ®åœ°å›¾ï¼Œç”¨äºè·¯å¾„è§„åˆ’

```python
# åœ¨"å»æ‹¿æ¯æ°´"ä»»åŠ¡ä¸­çš„å®é™…å·¥ä½œ

class OccupancyMapper:
    """å æ®æ …æ ¼åœ°å›¾æ„å»ºå™¨"""

    def update_map(self, point_cloud, robot_position):
        """
        æ›´æ–°å æ®åœ°å›¾

        å·¥ä½œæµç¨‹:
        1. å°†ç‚¹äº‘æŠ•å½±åˆ°2Då¹³é¢
        2. æ …æ ¼åŒ–å¤„ç† (æ¯ä¸ªæ …æ ¼5cm x 5cm)
        3. æ ‡è®°å æ®çŠ¶æ€
        """

        # ç”Ÿæˆ2Då æ®æ …æ ¼
        occupancy_grid = np.zeros((200, 200))  # 10m x 10måŒºåŸŸ

        for point in point_cloud:
            # è½¬æ¢åˆ°æœºå™¨äººåæ ‡ç³»
            grid_x = int((point.x - robot_position.x) / 0.05)
            grid_y = int((point.y - robot_position.y) / 0.05)

            # æ ‡è®°å æ®
            if 0 <= grid_x < 200 and 0 <= grid_y < 200:
                occupancy_grid[grid_y, grid_x] = 1  # å æ®

        return OccupancyGrid(
            data=occupancy_grid,
            resolution=0.05,  # 5cmåˆ†è¾¨ç‡
            origin=robot_position,
            width=10.0,
            height=10.0
        )
```

**åœ°å›¾å¯è§†åŒ–**:
```
    10m
     â†‘
  0  .  .  .  .  T  T  T  .  .  â† æ¡Œå­å æ®åŒºåŸŸ
     .  .  .  T  T  T  T  T  .  â† æ¡Œå­å æ®åŒºåŸŸ
  5  .  .  C  C  .  .  .  .  .  â† æ¤…å­å æ®
     .  .  C  C  .  .  .  .  .  â† æ¤…å­å æ®
 10  R  .  .  .  .  .  .  .  .  â† æœºå™¨äººä½ç½®
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
     0m        5m        10m

  å›¾ä¾‹:
  R - æœºå™¨äºº (Robot)
  T - æ¡Œå­ (Table)
  C - æ¤…å­ (Chair)
  . - ç©ºé—²åŒºåŸŸ
```

#### 3.2.4 å¤šä¼ æ„Ÿå™¨èåˆ (SensorFusion)

**èŒè´£**: èåˆå¼‚æ„ä¼ æ„Ÿå™¨æ•°æ®ï¼Œæä¾›ä¸€è‡´çš„ä¸–ç•ŒçŠ¶æ€

```python
# åœ¨"å»æ‹¿æ¯æ°´"ä»»åŠ¡ä¸­çš„å®é™…å·¥ä½œ

class SensorFusion:
    """ä¼ æ„Ÿå™¨èåˆå™¨ - ä½¿ç”¨EKF (æ‰©å±•å¡å°”æ›¼æ»¤æ³¢)"""

    def fuse_sensors(self, sensor_data):
        """
        èåˆå¤šä¼ æ„Ÿå™¨æ•°æ®

        ç›®æ ‡: ç»“åˆè§†è§‰ã€æ·±åº¦ã€æ¿€å…‰ã€é‡Œç¨‹è®¡æ•°æ®
        è¾“å‡º: ç»Ÿä¸€çš„ä½å§¿ä¼°è®¡å’Œç¯å¢ƒçŠ¶æ€
        """

        # EKFçŠ¶æ€å‘é‡: [x, y, theta, vx, vy, vtheta]
        state = self.ekf_state

        # 1. é¢„æµ‹æ­¥éª¤ (åŸºäºè¿åŠ¨æ¨¡å‹)
        # ä½¿ç”¨è½®é€Ÿè®¡é¢„æµ‹ä¸‹ä¸€æ—¶åˆ»ä½ç½®
        predicted_state = self.motion_model.predict(
            state,
            wheel_odometry=sensor_data.odometry
        )

        # 2. æ›´æ–°æ­¥éª¤ (èåˆå„ä¼ æ„Ÿå™¨è§‚æµ‹)
        # èåˆæ¿€å…‰é›·è¾¾è§‚æµ‹
        predicted_state = self.ekf.update(
            predicted_state,
            observation=lidar_features,
            observation_model="lidar_landmarks"
        )

        # èåˆè§†è§‰ç‰¹å¾
        predicted_state = self.ekf.update(
            predicted_state,
            observation=visual_odometry,
            observation_model="visual_features"
        )

        # 3. è¾“å‡ºèåˆåçš„çŠ¶æ€
        return FusedPerception(
            robot_position={
                "x": predicted_state[0],  # æœºå™¨äººXåæ ‡
                "y": predicted_state[1],  # æœºå™¨äººYåæ ‡
                "theta": predicted_state[2]  # æœå‘è§’åº¦
            },
            obstacles=self.extract_obstacles(lidar, camera),
            confidence=predicted_state.covariance.diagonal()
        )
```

### 3.3 æ„ŸçŸ¥å±‚æ€»ç»“

**è¾“å…¥**: åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®æµ
**å¤„ç†**: æ•°æ®é‡‡é›†ã€ç‰¹å¾æå–ã€è¯­ä¹‰ç†è§£ã€åœ°å›¾æ„å»ºã€ä¼ æ„Ÿå™¨èåˆ
**è¾“å‡º**: ç»“æ„åŒ–çš„æ„ŸçŸ¥æ•°æ®åŒ…

```python
# æ„ŸçŸ¥å±‚çš„æœ€ç»ˆè¾“å‡º
PerceptionData(
    robot_position={"x": 0.0, "y": 0.0, "theta": 0.0},
    obstacles=[
        {"type": "chair", "position": [1.5, 0.5], "distance": 1.5},
        {"type": "coffee_table", "position": [2.0, -0.3], "distance": 2.0}
    ],
    targets=[
        {"type": "table", "position": [3.0, 0.0], "distance": 3.0},
        {"type": "cup", "position": [3.0, 0.0, 0.8], "distance": 3.0}
    ],
    occupancy_grid=occupancy_grid,  # 2Då æ®åœ°å›¾
    rgb_image=rgb_image,
    depth_image=depth_image,
    timestamp=datetime.now()
)
```

---

## 4. è®¤çŸ¥å±‚èŒè´£

### 4.1 æ ¸å¿ƒèŒè´£
**"æˆ‘ç†è§£äº†ä»€ä¹ˆï¼Ÿè¿™ä¸ªä»»åŠ¡æ„å‘³ç€ä»€ä¹ˆï¼Ÿéœ€è¦è€ƒè™‘ä»€ä¹ˆï¼Ÿ"**

### 4.2 æ¨¡å—ç»„æˆ

#### 4.2.1 ä¸–ç•Œæ¨¡å‹ (WorldModel)

**æ–‡ä»¶ä½ç½®**: `brain/cognitive/world_model.py`

**èŒè´£**: ç»´æŠ¤ç»Ÿä¸€çš„ç¯å¢ƒè¡¨ç¤ºå’ŒçŠ¶æ€è¿½è¸ª

```python
# åœ¨"å»æ‹¿æ¯æ°´"ä»»åŠ¡ä¸­çš„å®é™…å·¥ä½œ

class WorldModel:
    """ä¸–ç•Œæ¨¡å‹ - ç¯å¢ƒçŠ¶æ€çš„ä¸­å¿ƒç®¡ç†è€…"""

    def update_from_perception(self, perception_data):
        """
        æ›´æ–°ä¸–ç•Œæ¨¡å‹

        å·¥ä½œ:
        1. èåˆæ–°çš„æ„ŸçŸ¥æ•°æ®
        2. æ›´æ–°ç‰©ä½“è¿½è¸ªçŠ¶æ€
        3. æ£€æµ‹ç¯å¢ƒå˜åŒ–
        4. è¯„ä¼°å˜åŒ–æ˜¾è‘—æ€§
        """

        # 1. æ›´æ–°æœºå™¨äººä½å§¿
        self.robot_position = perception_data.robot_position

        # 2. æ›´æ–°ç‰©ä½“è¿½è¸ª (ä½¿ç”¨å¡å°”æ›¼æ»¤æ³¢)
        for target in perception_data.targets:
            if target.id in self.tracked_objects:
                # å·²è¿½è¸ªçš„ç‰©ä½“ï¼Œæ›´æ–°çŠ¶æ€
                self.tracked_objects[target.id].update(
                    position=target.position,
                    timestamp=perception_data.timestamp
                )
            else:
                # æ–°ç‰©ä½“ï¼Œåˆ›å»ºè¿½è¸ªå™¨
                self.tracked_objects[target.id] = TrackedObject(
                    id=target.id,
                    type=target.type,
                    position=target.position,
                    first_seen=perception_data.timestamp
                )

        # 3. æ›´æ–°éšœç¢ç‰©çŠ¶æ€
        self.obstacles = perception_data.obstacles

        # 4. æ£€æµ‹ç¯å¢ƒå˜åŒ–
        changes = self._detect_changes()

        return changes

    def get_context_for_planning(self):
        """
        ä¸ºè§„åˆ’å±‚æä¾›ä¸Šä¸‹æ–‡

        è¾“å‡ºåŒ…å«:
        - å½“å‰æœºå™¨äººçŠ¶æ€
        - ç›®æ ‡ç‰©ä½“ä¿¡æ¯
        - éšœç¢ç‰©åˆ†å¸ƒ
        - ç¯å¢ƒçº¦æŸ
        - æœ€è¿‘å˜åŒ–
        """

        return PlanningContext(
            current_position=self.robot_position,
            current_heading=self.robot_position["theta"],
            obstacles=self.obstacles,
            targets=[
                obj for obj in self.tracked_objects.values()
                if obj.type == "cup"  # åªè¿”å›æ¯å­ç›¸å…³ç›®æ ‡
            ],
            points_of_interest=[
                {"type": "table", "position": [3.0, 0.0], "description": "é¤æ¡Œ"},
            ],
            battery_level=85,  # å‡è®¾ç”µé‡85%
            weather={"indoor": True, "temperature": 25},
            constraints=[
                "ä¸èƒ½æ‰“ç¿»æ°´æ¯",
                "ç§»åŠ¨é€Ÿåº¦è¦æ…¢ï¼Œä¿æŒç¨³å®š"
            ],
            recent_changes=self.recent_changes
        )
```

**ä¸–ç•Œæ¨¡å‹çŠ¶æ€ç¤ºä¾‹**:
```python
WorldModelState = {
    "robot_position": {"x": 0.0, "y": 0.0, "theta": 0.0},
    "tracked_objects": {
        "cup_001": {
            "type": "cup",
            "position": [3.0, 0.0, 0.8],
            "state": "upright",
            "fullness": "half_full",
            "tracking_confidence": 0.92,
            "last_updated": "2026-01-06T10:30:05"
        },
        "table_001": {
            "type": "table",
            "position": [3.0, 0.0, 0.0],
            "material": "wood",
            "last_updated": "2026-01-06T10:30:05"
        }
    },
    "obstacles": [
        {"type": "chair", "position": [1.5, 0.5], "bypassable": True},
        {"type": "coffee_table", "position": [2.0, -0.3], "bypassable": True}
    ],
    "environment": {
        "location": "living_room",
        "lighting": "normal",
        "floor_type": "carpet"
    },
    "recent_changes": []  # åˆå§‹æ— å˜åŒ–
}
```

#### 4.2.2 CoTæ¨ç†å¼•æ“ (CoTEngine)

**æ–‡ä»¶ä½ç½®**: `brain/cognitive/reasoning/cot_engine.py`

**èŒè´£**: è¿›è¡Œé“¾å¼æ€ç»´æ¨ç†ï¼Œç”Ÿæˆå¯è¿½æº¯çš„å†³ç­–è¿‡ç¨‹

```python
# åœ¨"å»æ‹¿æ¯æ°´"ä»»åŠ¡ä¸­çš„å®é™…å·¥ä½œ

class CoTEngine:
    """é“¾å¼æ€ç»´æ¨ç†å¼•æ“"""

    async def reason(self, query, context, mode):
        """
        æ‰§è¡ŒCoTæ¨ç†

        ä»»åŠ¡: ç†è§£"å»æ‹¿æ¯æ°´"çš„å®Œæ•´å«ä¹‰
        æ¨¡å¼: PLANNING (è§„åˆ’æ¨¡å¼)
        """

        # Step 1: è¯„ä¼°ä»»åŠ¡å¤æ‚åº¦
        complexity = self.assess_complexity(query, context)
        # ç»“æœ: COMPLEX (å¤æ‚ä»»åŠ¡ - éœ€è¦å®Œæ•´æ¨ç†)

        # Step 2: æ‰§è¡Œå®Œæ•´CoTæ¨ç†
        if complexity == ComplexityLevel.COMPLEX:
            result = await self._full_cot_reasoning(
                query, context, mode
            )

        return result

    async def _full_cot_reasoning(self, query, context, mode):
        """
        å®Œæ•´CoTæ¨ç†è¿‡ç¨‹

        å®é™…æ€è€ƒé“¾:
        """

        # æ„å»ºæ¨ç†æç¤º
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æœºå™¨äººçš„å†³ç­–å¼•æ“ã€‚è¯·ä½¿ç”¨é“¾å¼æ€ç»´åˆ†æä»»åŠ¡ã€‚

## å½“å‰ç¯å¢ƒ
{context.to_prompt()}

## ä»»åŠ¡ç›®æ ‡
{query}

## æ¨ç†æ¨¡å¼
{mode.value}

## æ¨ç†è¿‡ç¨‹
è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤é€æ­¥åˆ†æï¼š

1. å½“å‰ç¯å¢ƒä¸­æœ‰å“ªäº›å…³é”®å› ç´ ï¼Ÿï¼ˆéšœç¢ç‰©ã€ç›®æ ‡ã€çº¦æŸï¼‰
2. è¿™äº›å› ç´ å¦‚ä½•å½±å“ä»»åŠ¡æ‰§è¡Œï¼Ÿ
3. å¯è¡Œçš„æ“ä½œåºåˆ—æœ‰å“ªäº›ï¼Ÿ
4. æœ€ä¼˜åºåˆ—æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
5. æœ‰å“ªäº›æ½œåœ¨é£é™©éœ€è¦è€ƒè™‘ï¼Ÿ

å¯¹äºæ¯ä¸ªæ­¥éª¤ï¼Œè¯·æä¾›ï¼š
- åˆ†æè¿‡ç¨‹
- ç»“è®º
- ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰

## æœ€ç»ˆå†³ç­–
åœ¨å®Œæˆæ‰€æœ‰åˆ†æåï¼Œç»™å‡ºï¼š
- å†³ç­–: [æ˜ç¡®çš„å†³ç­–]
- å»ºè®®: [å…·ä½“çš„æ“ä½œå»ºè®®]
- ç½®ä¿¡åº¦: [0-1çš„æ•°å€¼]
"""

        # è°ƒç”¨LLMç”Ÿæˆæ¨ç†é“¾
        response = await self.llm.chat(prompt, max_tokens=2000)

        # è§£æCoTå“åº”
        reasoning_chain = self._parse_cot_response(response)

        return ReasoningResult(
            mode=mode,
            query=query,
            complexity=complexity,
            chain=reasoning_chain.steps,
            decision=reasoning_chain.decision,
            suggestion=reasoning_chain.suggestion,
            confidence=reasoning_chain.confidence,
            raw_response=response
        )
```

**CoTæ¨ç†è¾“å‡ºç¤ºä¾‹**:
```markdown
## æ¨ç†è¿‡ç¨‹

### æ­¥éª¤1: ç¯å¢ƒå› ç´ åˆ†æ
**åˆ†æ**:
- å½“å‰ä½ç½®: æ²™å‘æ— (0, 0)
- ç›®æ ‡ç‰©ä½“: é¤æ¡Œä¸Šçš„è“è‰²æ°´æ¯ (3ç±³å¤–)
- éšœç¢ç‰©: æ¤…å­(1.5ç±³)ã€èŒ¶å‡ (2.0ç±³)
- çº¦æŸ: ä¸èƒ½æ‰“ç¿»æ°´æ¯ã€éœ€è¦ä¿æŒç¨³å®š

**ç»“è®º**: è¿™æ˜¯ä¸€ä¸ªä¸­ç­‰å¤æ‚åº¦çš„å¯¼èˆª+æŠ“å–ä»»åŠ¡ï¼Œéœ€è¦ç²¾ç¡®æ§åˆ¶å’Œé¿éšœ

**ç½®ä¿¡åº¦**: 0.9

### æ­¥éª¤2: å½±å“å› ç´ è¯„ä¼°
**åˆ†æ**:
- éšœç¢ç‰©ä½äºæœºå™¨äººä¸æ¡Œå­ä¹‹é—´ï¼Œéœ€è¦ç»•è¡Œ
- æ°´æ¯åœ¨æ¡Œé¢ä¸Šï¼Œé«˜åº¦0.8ç±³ï¼Œéœ€è¦æœºæ¢°è‡‚æŠ¬å‡
- åœ°æ¯¯å¯èƒ½å½±å“ç§»åŠ¨ç¨³å®šæ€§
- æ°´æ¯åŠæ»¡ï¼Œå€¾æ–œä¼šå¯¼è‡´æ´’æ°´

**ç»“è®º**: éœ€è¦è§„åˆ’é¿éšœè·¯å¾„ï¼Œè°ƒæ•´æœºæ¢°è‡‚é€Ÿåº¦ï¼Œç¡®ä¿ç¨³å®šæŠ“å–

**ç½®ä¿¡åº¦**: 0.85

### æ­¥éª¤3: æ“ä½œåºåˆ—è§„åˆ’
**åˆ†æ**:
å¯é€‰åºåˆ—:
- æ–¹æ¡ˆA: ç›´çº¿ç§»åŠ¨ â†’ ç»•è¿‡éšœç¢ â†’ æ¥è¿‘æ¡Œå­ â†’ æŠ“å–
- æ–¹æ¡ˆB: å³ä¾§ç»•è¡Œ â†’ æ¥è¿‘æ¡Œå­ â†’ è°ƒæ•´å§¿æ€ â†’ æŠ“å–
- æ–¹æ¡ˆC: å·¦ä¾§ç»•è¡Œ â†’ æ¥è¿‘æ¡Œå­ â†’ æŠ“å– â†’ è¿”å›

**ç»“è®º**: æ–¹æ¡ˆBæœ€ä¼˜ï¼Œå³ä¾§ç©ºé—´è¾ƒå¤§ï¼Œç»•è¡Œæ›´å®‰å…¨

**ç½®ä¿¡åº¦**: 0.8

### æ­¥éª¤4: æœ€ä¼˜æ–¹æ¡ˆç¡®è®¤
**åˆ†æ**:
- å³ä¾§ç»•è¡Œå¯ä»¥é¿å¼€æ¤…å­
- èŒ¶å‡ è·ç¦»2.0ç±³ï¼Œå³ä¾§ç»•è¡Œè·ç¦»çº¦2.2ç±³ï¼Œä»£ä»·å¯æ¥å—
- åˆ°è¾¾æ¡Œå­åï¼Œéœ€è¦è°ƒæ•´æœºå™¨äººæœå‘æ­£å¯¹æ°´æ¯

**ç»“è®º**: é‡‡ç”¨æ–¹æ¡ˆBï¼Œæ…¢é€Ÿç§»åŠ¨(0.3m/s)ï¼Œä¿æŒç¨³å®š

**ç½®ä¿¡åº¦**: 0.85

### æ­¥éª¤5: é£é™©è¯„ä¼°
**åˆ†æ**:
æ½œåœ¨é£é™©:
1. åœ°æ¯¯å¯èƒ½å¯¼è‡´æ‰“æ»‘ â†’ ä½¿ç”¨ä½åŠ é€Ÿåº¦
2. æœºæ¢°è‡‚ä¼¸å‡ºæ—¶é‡å¿ƒå˜åŒ– â†’ ç§»åŠ¨é€Ÿåº¦é™ä½
3. æ°´æ¯è¾¹ç¼˜å…‰æ»‘ â†’ è°ƒæ•´æŠ“å–åŠ›åº¦

**ç»“è®º**: é£é™©å¯æ§ï¼Œé‡‡å–ä¿å®ˆç­–ç•¥

**ç½®ä¿¡åº¦**: 0.9

## æœ€ç»ˆå†³ç­–
**å†³ç­–**: æ‰§è¡Œä»»åŠ¡ï¼Œé‡‡ç”¨å³ä¾§ç»•è¡Œç­–ç•¥
**å»ºè®®**:
1. ä»¥0.3m/sé€Ÿåº¦å‘å³å‰æ–¹ç§»åŠ¨ï¼Œç»•è¿‡æ¤…å­
2. åˆ°è¾¾æ¡Œå­å³ä¾§ï¼Œè°ƒæ•´æœå‘
3. ç¼“æ…¢æ¥è¿‘æ°´æ¯ï¼Œä¿æŒæœºæ¢°è‡‚ç¨³å®š
4. ä½¿ç”¨ä¸­ç­‰åŠ›åº¦æŠ“å–ï¼Œé¿å…æç¢
5. æŠ“å–åç¼“æ…¢æŠ¬èµ·ï¼Œé¿å…æ´’æ°´

**ç½®ä¿¡åº¦**: 0.87
```

#### 4.2.3 å¯¹è¯ç®¡ç†å™¨ (DialogueManager)

**èŒè´£**: ç®¡ç†å¤šè½®å¯¹è¯ï¼Œå¤„ç†ç”¨æˆ·äº¤äº’

```python
# åœ¨"å»æ‹¿æ¯æ°´"ä»»åŠ¡ä¸­çš„å®é™…å·¥ä½œ

class DialogueManager:
    """å¯¹è¯ç®¡ç†å™¨"""

    async def start_session(self, session_id):
        """å¼€å§‹å¯¹è¯ä¼šè¯"""
        self.session_id = session_id
        self.conversation_history = []

        # å‘é€ç¡®è®¤ä¿¡æ¯
        await self.send_information(
            "ğŸ¤– æ”¶åˆ°æŒ‡ä»¤ï¼šå»æ‹¿æ¡Œä¸Šçš„æ°´æ¯\n"
            "æ­£åœ¨åˆ†æç¯å¢ƒ..."
        )

    async def clarify_ambiguous_command(self, command, ambiguities, world_context):
        """
        æ¾„æ¸…æ¨¡ç³ŠæŒ‡ä»¤

        ä¾‹å¦‚: å¦‚æœç”¨æˆ·è¯´"æ‹¿é‚£ä¸ªæ¯å­"ï¼Œä½†æ¡Œä¸Šæœ‰å¤šä¸ªæ¯å­
        """

        # æ£€æµ‹æ¨¡ç³Šç‚¹
        if "é‚£ä¸ª" in command or "å“ªä¸ª" in command:
            # å‘ç°æ¨¡ç³Šï¼Œéœ€è¦æ¾„æ¸…
            cups = self._find_all_cups(world_context)

            if len(cups) > 1:
                # å¤šä¸ªæ¯å­ï¼Œè¯¢é—®ç”¨æˆ·
                result = await self.ask_user(
                    question="æ¡Œä¸Šæœ‰å¤šä¸ªæ¯å­ï¼Œè¯·é—®æ˜¯å“ªä¸€ä¸ªï¼Ÿ",
                    options=[
                        f"è“è‰²çš„æ¯å­ ({cups[0]['position']})",
                        f"çº¢è‰²çš„æ¯å­ ({cups[1]['position']})",
                        f"æœ€è¿‘çš„ä¸€ä¸ª"
                    ]
                )

                return {
                    "clarified": True,
                    "clarified_command": f"å»æ‹¿{result}çš„æ¯å­"
                }

        return {"clarified": False}

    async def report_progress(self, status, progress_percent, current_operation, ...):
        """æ±‡æŠ¥æ‰§è¡Œè¿›åº¦"""
        message = f"""
ğŸ“Š ä»»åŠ¡è¿›åº¦: {progress_percent:.1f}%
å½“å‰æ­¥éª¤: {current_operation}
ç¯å¢ƒçŠ¶æ€: {world_state_summary}
"""

        # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦éœ€è¦è°ƒæ•´
        user_response = await self.send_information(
            message,
            allow_adjustment=True
        )

        return user_response  # å¯èƒ½æ˜¯"ç»§ç»­"ã€"æš‚åœ"ã€"å–æ¶ˆ"ç­‰

    async def send_information(self, message, allow_adjustment=False):
        """å‘ç”¨æˆ·å‘é€ä¿¡æ¯"""
        print(f"[Brain â†’ User]: {message}")
        # è®°å½•åˆ°å¯¹è¯å†å²
        self.conversation_history.append({
            "role": "assistant",
            "content": message,
            "timestamp": datetime.now()
        })
```

**å¯¹è¯å†å²ç¤ºä¾‹**:
```markdown
[10:30:00] User: "æœºå™¨äººï¼Œå»å¸®æˆ‘å»æ¡Œå­ä¸Šæ‹¿é‚£ä¸ªæ°´æ¯"
[10:30:01] Brain: "ğŸ¤– æ”¶åˆ°æŒ‡ä»¤ï¼šå»æ‹¿æ¡Œä¸Šçš„æ°´æ¯\næ­£åœ¨åˆ†æç¯å¢ƒ..."
[10:30:02] Brain: "âœ… ä»»åŠ¡è§„åˆ’å®Œæˆ\n- ä»»åŠ¡ID: mission_001\n- æ“ä½œæ•°é‡: 7\n- é¢„è®¡æ—¶é•¿: 45ç§’\n- è§„åˆ’ç½®ä¿¡åº¦: 87%"
[10:30:03] Brain: "ğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡ [mission_001]\næ€»æ“ä½œæ•°: 7"
[10:30:20] Brain: "ğŸ“Š ä»»åŠ¡è¿›åº¦: 28.6%\nå½“å‰æ­¥éª¤: ç»•è¿‡éšœç¢ç‰©\næ­£åœ¨å‘å³å‰æ–¹ç§»åŠ¨..."
[10:30:40] Brain: "ğŸ“Š ä»»åŠ¡è¿›åº¦: 57.1%\nå½“å‰æ­¥éª¤: æ¥è¿‘ç›®æ ‡\næ­£åœ¨è°ƒæ•´æœå‘æ°´æ¯..."
[10:30:55] Brain: "ğŸ“Š ä»»åŠ¡è¿›åº¦: 85.7%\nå½“å‰æ­¥éª¤: æŠ“å–æ°´æ¯\næ­£åœ¨ä¼¸å‡ºæœºæ¢°è‡‚..."
[10:31:00] Brain: "âœ… ä»»åŠ¡ [mission_001] æ‰§è¡Œå®Œæˆ!\næ€»æ“ä½œ: 7\né‡è§„åˆ’æ¬¡æ•°: 0"
```

#### 4.2.4 æ„ŸçŸ¥ç›‘æ§å™¨ (PerceptionMonitor)

**èŒè´£**: æŒç»­ç›‘æ§æ„ŸçŸ¥å˜åŒ–ï¼Œè§¦å‘é‡è§„åˆ’

```python
# åœ¨"å»æ‹¿æ¯æ°´"ä»»åŠ¡ä¸­çš„å®é™…å·¥ä½œ

class PerceptionMonitor:
    """æ„ŸçŸ¥å˜åŒ–ç›‘æ§å™¨"""

    async def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True

        while self.monitoring:
            # è·å–æœ€æ–°æ„ŸçŸ¥
            latest_perception = await self.get_latest_perception()

            # æ£€æµ‹å˜åŒ–
            changes = self._detect_changes(latest_perception)

            # è¯„ä¼°å˜åŒ–æ˜¾è‘—æ€§
            for change in changes:
                if change.priority == ChangeType.CRITICAL:
                    # å…³é”®å˜åŒ–ï¼Œè§¦å‘é‡è§„åˆ’
                    await self._trigger_replan(change)
                elif change.priority == ChangeType.HIGH:
                    # é«˜ä¼˜å…ˆçº§ï¼Œè¯·æ±‚ç¡®è®¤
                    await self._request_confirmation(change)
                else:
                    # ä½ä¼˜å…ˆçº§ï¼Œä»…é€šçŸ¥
                    await self._notify(change)

            await asyncio.sleep(0.1)  # 10Hzç›‘æ§

    def _detect_changes(self, perception):
        """æ£€æµ‹æ„ŸçŸ¥å˜åŒ–"""
        changes = []

        # 1. æ£€æµ‹æ–°éšœç¢ç‰©
        new_obstacles = self._find_new_obstacles(perception.obstacles)
        if new_obstacles:
            changes.append(EnvironmentChange(
                change_type=ChangeType.NEW_OBSTACLE,
                description=f"æ£€æµ‹åˆ°æ–°éšœç¢ç‰©: {new_obstacles[0]['type']}",
                priority=ChangeType.HIGH,
                data=new_obstacles
            ))

        # 2. æ£€æµ‹ç›®æ ‡ç‰©ä½“ç§»åŠ¨
        if self._has_target_moved(perception.targets):
            changes.append(EnvironmentChange(
                change_type=ChangeType.TARGET_MOVED,
                description="ç›®æ ‡æ°´æ¯ä½ç½®å‘ç”Ÿå˜åŒ–",
                priority=ChangeType.CRITICAL,
                data=perception.targets
            ))

        # 3. æ£€æµ‹è·¯å¾„é˜»å¡
        if self._is_path_blocked(perception.occupancy_grid):
            changes.append(EnvironmentChange(
                change_type=ChangeType.PATH_BLOCKED,
                description="å½“å‰è·¯å¾„è¢«é˜»å¡",
                priority=ChangeType.HIGH,
                data={"blocked_segments": [...]}
            ))

        return changes
```

**ç›‘æ§äº‹ä»¶ç¤ºä¾‹**:
```python
# å‡è®¾åœºæ™¯: åœ¨æœºå™¨äººç§»åŠ¨è¿‡ç¨‹ä¸­ï¼Œæœ‰äººç§»åŠ¨äº†æ¤…å­

MonitorEvent(
    timestamp="2026-01-06T10:30:25",
    change=EnvironmentChange(
        change_type=ChangeType.NEW_OBSTACLE,
        description="æ¤…å­ä»(1.5, 0.5)ç§»åŠ¨åˆ°(1.8, 0.8)",
        priority=ChangeType.HIGH,
        data={
            "object_type": "chair",
            "old_position": [1.5, 0.5],
            "new_position": [1.8, 0.8],
            "impact": "åŸè·¯å¾„ä»å¯ç”¨ï¼Œä½†å®‰å…¨è·ç¦»å‡å°"
        }
    ),
    action=TriggerAction.REQUEST_CONFIRMATION
)

# è§¦å‘å¯¹è¯
Brain: "âš ï¸ æ£€æµ‹åˆ°ç¯å¢ƒå˜åŒ–: æ¤…å­ä½ç½®ç§»åŠ¨"
      "æ˜¯å¦éœ€è¦é‡æ–°è§„åˆ’è·¯å¾„ï¼Ÿ"
      "å½±å“: å½“å‰è·¯å¾„ä»ç„¶å¯ç”¨ï¼Œä½†å®‰å…¨è·ç¦»ä»0.5må‡å°åˆ°0.3m"
```

### 4.3 è®¤çŸ¥å±‚æ€»ç»“

**è¾“å…¥**: æ„ŸçŸ¥æ•°æ®åŒ… + ç”¨æˆ·æŒ‡ä»¤
**å¤„ç†**:
- WorldModel: çŠ¶æ€ç»´æŠ¤ã€å˜åŒ–æ£€æµ‹
- CoTEngine: ä»»åŠ¡ç†è§£ã€æ¨ç†å†³ç­–
- DialogueManager: å¯¹è¯äº¤äº’ã€è¿›åº¦æ±‡æŠ¥
- PerceptionMonitor: æŒç»­ç›‘æ§ã€äº‹ä»¶è§¦å‘

**è¾“å‡º**: è§„åˆ’ä¸Šä¸‹æ–‡ + CoTæ¨ç†ç»“æœ

```python
CognitiveOutput(
    planning_context=PlanningContext(
        current_position={"x": 0.0, "y": 0.0, "theta": 0.0},
        obstacles=[...],
        targets=[...],
        constraints=["ä¸èƒ½æ‰“ç¿»æ°´æ¯", "ä¿æŒç¨³å®š"]
    ),
    reasoning_result=ReasoningResult(
        decision="æ‰§è¡Œä»»åŠ¡ï¼Œé‡‡ç”¨å³ä¾§ç»•è¡Œç­–ç•¥",
        suggestion="ä»¥0.3m/sé€Ÿåº¦å‘å³å‰æ–¹ç§»åŠ¨...",
        confidence=0.87,
        chain=[...]
    )
)
```

---

## 5. è§„åˆ’å±‚èŒè´£

### 5.1 æ ¸å¿ƒèŒè´£
**"æˆ‘è¯¥å¦‚ä½•åˆ†è§£ä»»åŠ¡ï¼Ÿå…·ä½“è¦æ‰§è¡Œå“ªäº›æ“ä½œï¼Ÿ"**

### 5.2 æ¨¡å—ç»„æˆ

#### 5.2.1 ä»»åŠ¡çº§è§„åˆ’å™¨ (TaskPlanner)

**æ–‡ä»¶ä½ç½®**: `brain/planning/task/task_planner.py`

**èŒè´£**: å°†é«˜å±‚ä»»åŠ¡åˆ†è§£ä¸ºåŸå­æ“ä½œåºåˆ—

```python
# åœ¨"å»æ‹¿æ¯æ°´"ä»»åŠ¡ä¸­çš„å®é™…å·¥ä½œ

class TaskPlanner:
    """ä»»åŠ¡è§„åˆ’å™¨"""

    async def plan_with_perception(
        self,
        parsed_task,
        platform_type,
        planning_context,
        cot_result,
        constraints
    ):
        """
        æ„ŸçŸ¥é©±åŠ¨çš„è§„åˆ’

        è¾“å…¥:
        - parsed_task: {"task_type": "pickup", "target": "cup", "location": "table"}
        - planning_context: è®¤çŸ¥å±‚æä¾›çš„ä¸Šä¸‹æ–‡
        - cot_result: CoTæ¨ç†ç»“æœå’Œå»ºè®®
        - constraints: å®‰å…¨çº¦æŸ
        """

        # Step 1: æ„å»ºä»»åŠ¡æ ‘
        task_tree = self._build_task_tree(parsed_task)
        # ç»“æœ: TaskNode("pickup_cup", parameters={"target": "cup", "location": "table"})

        # Step 2: åˆ†è§£ä¸ºåŸå­æ“ä½œ
        raw_operations = await self._decompose_task_tree(task_tree, platform_type)
        # ä½¿ç”¨é¢„å®šä¹‰è§„åˆ™: "pickup": ["goto", "adjust_pose", "pickup", "return"]

        # ç»“æœ:
        # [
        #   Operation("goto", {"position": [3.0, 0.0]}),
        #   Operation("adjust_pose", {"heading": 0.0}),
        #   Operation("pickup", {"object": "cup", "grip_force": 0.5}),
        #   Operation("return_to_home", {})
        # ]

        # Step 3: æ ¹æ®æ„ŸçŸ¥ä¸Šä¸‹æ–‡è°ƒæ•´æ“ä½œ
        perception_adjusted_ops = self._adjust_for_perception(
            raw_operations,
            planning_context,
            platform_type
        )

        # è°ƒæ•´å†…å®¹:
        # - æ£€æµ‹åˆ°éšœç¢ç‰©ï¼Œæ’å…¥é¿éšœæ“ä½œ
        # - åœ°æ¯¯ç¯å¢ƒï¼Œé™ä½ç§»åŠ¨é€Ÿåº¦
        # - æ°´æ¯åŠæ»¡ï¼Œè°ƒæ•´æŠ“å–åŠ›åº¦

        # Step 4: æ ¹æ®CoTæ¨ç†ç»“æœä¼˜åŒ–
        if cot_result:
            perception_adjusted_ops = self._apply_cot_suggestions(
                perception_adjusted_ops,
                cot_result
            )

        # CoTå»ºè®®: "å³ä¾§ç»•è¡Œï¼Œæ…¢é€Ÿç§»åŠ¨"
        # è°ƒæ•´:
        # - gotoæ“ä½œçš„è·¯å¾„æ”¹ä¸ºå³ä¾§ç»•è¡Œ
        # - é€Ÿåº¦ä»0.5m/sé™ä½åˆ°0.3m/s

        # Step 5: åº”ç”¨çº¦æŸ
        constrained_operations = self._apply_constraints(
            perception_adjusted_ops,
            constraints
        )

        # çº¦æŸåº”ç”¨:
        # - æœ€å¤§é€Ÿåº¦é™åˆ¶: 0.3m/s
        # - å®‰å…¨è·ç¦»: 0.5m
        # - æœºæ¢°è‡‚åŠ é€Ÿåº¦: 0.2m/sÂ²

        # Step 6: ä¼˜åŒ–æ“ä½œåºåˆ—
        optimized_operations = self._optimize_sequence(constrained_operations)

        # ä¼˜åŒ–:
        # - åˆå¹¶è¿ç»­çš„gotoæ“ä½œä¸ºè·¯å¾„
        # - åˆ é™¤å†—ä½™çš„waitæ“ä½œ

        # Step 7: æ·»åŠ æ„ŸçŸ¥æ›´æ–°æ“ä½œ
        operations_with_perception = self._insert_perception_updates(
            optimized_operations,
            platform_type
        )

        # æ¯éš”3ä¸ªæ“ä½œæ’å…¥update_perception

        # Step 8: æ·»åŠ å‰ç½®/åç½®æ¡ä»¶
        final_operations = self._add_conditions(operations_with_perception)

        # æ·»åŠ æ¡ä»¶:
        # - goto: å‰ç½®["ready_to_move"], åç½®["at_target"]
        # - pickup: å‰ç½®["close_to_object"], åç½®["object_grasped"]

        # Step 9: éªŒè¯è®¡åˆ’
        validation = await self._validate_plan(final_operations, platform_type)

        return final_operations
```

**æœ€ç»ˆæ“ä½œåºåˆ—ç¤ºä¾‹**:
```python
operations = [
    # æ“ä½œ1: å¯¼èˆªåˆ°æ¡Œå­å³ä¾§ (é¿éšœè·¯å¾„)
    Operation(
        id="op_001",
        name="follow_path",
        type=OperationType.MOVEMENT,
        parameters={
            "waypoints": [
                {"position": [0.5, 0.5], "speed": 0.3},   # å³ä¾§ç»•è¡Œèµ·ç‚¹
                {"position": [1.5, 0.8], "speed": 0.3},   # ç»•è¿‡æ¤…å­
                {"position": [2.5, 0.3], "speed": 0.3},   # æ¥è¿‘æ¡Œå­
                {"position": [3.0, 0.0], "speed": 0.2}    # åˆ°è¾¾ç›®æ ‡
            ],
            "avoidance_strategy": "right_side"
        },
        preconditions=[
            Precondition("battery_sufficient", "robot.battery > 20"),
            Precondition("ready_to_move", "robot.state.ready == True")
        ],
        postconditions=[
            Postcondition("at_target", "robot.position.near([3.0, 0.0])")
        ],
        estimated_duration=15.0,
        metadata={
            "cot_applied": True,
            "avoiding": ["chair", "coffee_table"],
            "reason": "å³ä¾§ç»•è¡Œç­–ç•¥"
        }
    ),

    # æ“ä½œ2: è°ƒæ•´å§¿æ€
    Operation(
        id="op_002",
        name="adjust_pose",
        type=OperationType.MOVEMENT,
        parameters={
            "heading": 0.0,  # æ­£å¯¹æ¡Œå­
            "distance": 0.5  # è·ç¦»æ¡Œå­0.5ç±³
        },
        estimated_duration=3.0
    ),

    # æ“ä½œ3: æ›´æ–°æ„ŸçŸ¥
    Operation(
        id="op_003",
        name="update_perception",
        type=OperationType.PERCEPTION,
        parameters={},
        estimated_duration=2.0,
        metadata={"auto_inserted": True}
    ),

    # æ“ä½œ4: è¯†åˆ«å¹¶ç¡®è®¤æ°´æ¯ä½ç½®
    Operation(
        id="op_004",
        name="detect_objects",
        type=OperationType.PERCEPTION,
        parameters={
            "object_types": ["cup"],
            "area": "front",
            "confirm_before_proceed": True
        },
        estimated_duration=3.0
    ),

    # æ“ä½œ5: ç²¾ç¡®å®šä½æ°´æ¯
    Operation(
        id="op_005",
        name="goto",
        type=OperationType.MOVEMENT,
        parameters={
            "position": [3.0, 0.0],
            "speed": 0.1,  # éå¸¸æ…¢é€Ÿï¼Œç¡®ä¿ç²¾ç¡®
            "precision": "high"
        },
        estimated_duration=5.0
    ),

    # æ“ä½œ6: æŠ“å–æ°´æ¯
    Operation(
        id="op_006",
        name="pickup",
        type=OperationType.MANIPULATION,
        parameters={
            "object_id": "cup_001",
            "grip_force": 0.4,  # ä¸­ç­‰åŠ›åº¦
            "lift_speed": 0.05,  # ç¼“æ…¢æŠ¬èµ·
            "approach_distance": 0.3
        },
        preconditions=[
            Precondition("object_visible", "perception.cup.visible == True"),
            Precondition("close_to_object", "robot.distance_to(cup) < 0.5")
        ],
        postconditions=[
            Postcondition("object_grasped", "gripper.state.closed == True"),
            Postcondition("object_lifted", "cup.height > 0.8")
        ],
        estimated_duration=8.0,
        metadata={
            "careful_handling": True,
            "reason": "æ°´æ¯åŠæ»¡ï¼Œéœ€è¦ç¨³å®šæ“ä½œ"
        }
    ),

    # æ“ä½œ7: è¿”å›èµ·ç‚¹
    Operation(
        id="op_007",
        name="return_to_home",
        type=OperationType.MOVEMENT,
        parameters={
            "home_position": [0.0, 0.0],
            "speed": 0.3,
            "holding_object": True  # æºå¸¦æ°´æ¯ï¼Œé€Ÿåº¦é™ä½
        },
        estimated_duration=12.0
    )
]

# æ€»è®¡: 7ä¸ªæ“ä½œï¼Œé¢„è®¡48ç§’
```

#### 5.2.2 æŠ€èƒ½çº§è§„åˆ’å™¨ (SkillLevelPlanner)

**èŒè´£**: ç®¡ç†å¯é‡ç”¨çš„æŠ€èƒ½åº“

```python
# åœ¨"å»æ‹¿æ¯æ°´"ä»»åŠ¡ä¸­çš„å®é™…å·¥ä½œ

class SkillLevelPlanner:
    """æŠ€èƒ½çº§è§„åˆ’å™¨"""

    def __init__(self):
        # æŠ€èƒ½åº“
        self.skills = {
            "navigate_around_obstacles": {
                "description": "é¿éšœå¯¼èˆªæŠ€èƒ½",
                "prerequisites": ["localization", "obstacle_detection"],
                "operations": ["detect_objects", "plan_path", "follow_path"]
            },
            "careful_pickup": {
                "description": "è°¨æ…æŠ“å–æŠ€èƒ½",
                "prerequisites": ["object_detection", "arm_control"],
                "operations": ["detect_objects", "adjust_pose", "pickup"]
            },
            "stable_navigation": {
                "description": "ç¨³å®šå¯¼èˆªæŠ€èƒ½",
                "prerequisites": ["localization"],
                "operations": ["follow_path", "update_perception"]
            }
        }

    def compose_skills(self, task_type, context):
        """
        ç»„åˆæŠ€èƒ½

        å¯¹äº"æ‹¿æ°´æ¯"ä»»åŠ¡ï¼Œç»„åˆä»¥ä¸‹æŠ€èƒ½:
        1. navigate_around_obstacles - é¿éšœå¯¼èˆª
        2. stable_navigation - ç¨³å®šå¯¼èˆª
        3. careful_pickup - è°¨æ…æŠ“å–
        """

        if task_type == "pickup":
            # æŠ€èƒ½ç»„åˆåºåˆ—
            skill_sequence = [
                "navigate_around_obstacles",  # å¯¼èˆªåˆ°ç›®æ ‡
                "stable_navigation",           # ç¨³å®šæ¥è¿‘
                "careful_pickup"              # è°¨æ…æŠ“å–
            ]

            # å±•å¼€ä¸ºæ“ä½œåºåˆ—
            operations = []
            for skill_name in skill_sequence:
                skill = self.skills[skill_name]
                operations.extend(skill["operations"])

            return operations
```

#### 5.2.3 åŠ¨ä½œçº§è§„åˆ’å™¨ (ActionLevelPlanner)

**èŒè´£**: ç”Ÿæˆå…·ä½“çš„åŠ¨ä½œå‚æ•°

```python
# åœ¨"å»æ‹¿æ¯æ°´"ä»»åŠ¡ä¸­çš„å®é™…å·¥ä½œ

class ActionLevelPlanner:
    """åŠ¨ä½œçº§è§„åˆ’å™¨"""

    def plan_goto_action(self, target, obstacles, constraints):
        """
        è§„åˆ’gotoåŠ¨ä½œçš„å…·ä½“å‚æ•°

        è¾“å…¥:
        - target: [3.0, 0.0]  # æ¡Œå­ä½ç½®
        - obstacles: [æ¤…å­(1.5, 0.5), èŒ¶å‡ (2.0, -0.3)]
        - constraints: {"max_speed": 0.3, "safety_distance": 0.5}
        """

        # ä½¿ç”¨A*ç®—æ³•è§„åˆ’è·¯å¾„
        path = self.astar_planner.plan(
            start=self.robot_position,
            goal=target,
            obstacles=obstacles,
            safety_distance=constraints["safety_distance"]
        )

        # ç”Ÿæˆè·¯å¾„ç‚¹
        waypoints = []
        for waypoint in path:
            waypoints.append({
                "position": waypoint.position,
                "speed": self._calculate_speed(
                    waypoint,
                    obstacles,
                    constraints["max_speed"]
                ),
                "heading": waypoint.heading
            })

        return {
            "waypoints": waypoints,
            "total_distance": path.total_distance,
            "estimated_time": path.total_distance / 0.3
        }

    def plan_pickup_action(self, cup_position, cup_state):
        """
        è§„åˆ’pickupåŠ¨ä½œçš„å…·ä½“å‚æ•°

        è¾“å…¥:
        - cup_position: [3.0, 0.0, 0.8]
        - cup_state: {"fullness": "half_full", "stability": "upright"}
        """

        # æ ¹æ®æ°´æ¯çŠ¶æ€è°ƒæ•´å‚æ•°
        if cup_state["fullness"] == "half_full":
            # åŠæ»¡ï¼Œéœ€è¦ç¼“æ…¢æ“ä½œ
            lift_speed = 0.05
            grip_force = 0.4
            acceleration = 0.1
        else:
            # å…¶ä»–æƒ…å†µ
            lift_speed = 0.1
            grip_force = 0.5
            acceleration = 0.2

        return {
            "approach_distance": 0.3,
            "grip_force": grip_force,
            "lift_speed": lift_speed,
            "acceleration": acceleration,
            "final_height": 1.0  # æŠ¬èµ·åˆ°1ç±³é«˜
        }
```

### 5.3 è§„åˆ’å±‚æ€»ç»“

**è¾“å…¥**: è®¤çŸ¥å±‚çš„è§„åˆ’ä¸Šä¸‹æ–‡ + CoTæ¨ç†ç»“æœ
**å¤„ç†**: ä»»åŠ¡åˆ†è§£ã€æ“ä½œç”Ÿæˆã€æ„ŸçŸ¥è°ƒæ•´ã€çº¦æŸåº”ç”¨ã€åºåˆ—ä¼˜åŒ–
**è¾“å‡º**: ç»“æ„åŒ–çš„æ“ä½œåºåˆ—

```python
PlanOutput(
    operations=[...],  # 7ä¸ªæ“ä½œçš„åˆ—è¡¨
    estimated_total_time=48.0,  # ç§’
    parallel_groups=[[1], [2], [3], [4], [5], [6], [7]],  # é¡ºåºæ‰§è¡Œ
    metadata={
        "strategy": "perception_driven",
        "cot_applied": True,
        "perception_context_used": True
    }
)
```

---

## 6. æ‰§è¡Œå±‚èŒè´£

### 6.1 æ ¸å¿ƒèŒè´£
**"æˆ‘è¯¥å¦‚ä½•æ‰§è¡Œè¿™äº›æ“ä½œï¼Ÿéœ€è¦å‘é€ä»€ä¹ˆæ§åˆ¶æŒ‡ä»¤ï¼Ÿ"**

### 6.2 æ¨¡å—ç»„æˆ

#### 6.2.1 æ‰§è¡Œå¼•æ“ (Executor)

**æ–‡ä»¶ä½ç½®**: `brain/execution/executor.py`

**èŒè´£**: ç®¡ç†æ“ä½œé˜Ÿåˆ—ï¼Œæ‰§è¡Œå…·ä½“æ“ä½œ

```python
# åœ¨"å»æ‹¿æ¯æ°´"ä»»åŠ¡ä¸­çš„å®é™…å·¥ä½œ

class Executor:
    """æ‰§è¡Œå¼•æ“"""

    async def execute(self, operation, robot_interface):
        """
        æ‰§è¡Œå•ä¸ªæ“ä½œ

        è¾“å…¥: Operationå¯¹è±¡
        è¾“å‡º: OperationResult
        """

        # Step 1: æ£€æŸ¥å‰ç½®æ¡ä»¶
        if not await self._check_preconditions(operation):
            return OperationResult(
                status=OperationStatus.FAILED,
                error_message="å‰ç½®æ¡ä»¶ä¸æ»¡è¶³"
            )

        # Step 2: å®‰å…¨æ£€æŸ¥
        safety_check = await self._safety_check(operation)
        if not safety_check.passed:
            return OperationResult(
                status=OperationStatus.FAILED,
                error_message=f"å®‰å…¨æ£€æŸ¥å¤±è´¥: {safety_check.reason}"
            )

        # Step 3: æ‰§è¡Œæ“ä½œ
        if operation.type == OperationType.MOVEMENT:
            result = await self._execute_movement(operation, robot_interface)
        elif operation.type == OperationType.MANIPULATION:
            result = await self._execute_manipulation(operation, robot_interface)
        elif operation.type == OperationType.PERCEPTION:
            result = await self._execute_perception(operation, robot_interface)
        else:
            result = await self._execute_control(operation, robot_interface)

        # Step 4: éªŒè¯åç½®æ¡ä»¶
        if result.status == OperationStatus.SUCCESS:
            if not await self._verify_postconditions(operation):
                result.status = OperationStatus.FAILED
                result.error_message = "åç½®æ¡ä»¶ä¸æ»¡è¶³"

        return result

    async def _execute_movement(self, operation, robot_interface):
        """
        æ‰§è¡Œç§»åŠ¨æ“ä½œ

        ç¤ºä¾‹: follow_pathæ“ä½œ
        """

        waypoints = operation.parameters["waypoints"]

        for i, waypoint in enumerate(waypoints):
            logger.info(f"ç§»åŠ¨åˆ°è·¯å¾„ç‚¹ {i+1}/{len(waypoints)}: {waypoint['position']}")

            # å‘é€æ§åˆ¶æŒ‡ä»¤åˆ°æœºå™¨äºº
            await robot_interface.send_velocity_command(
                linear_x=waypoint["speed"],
                linear_y=0.0,
                angular_z=waypoint["heading"]
            )

            # ç­‰å¾…åˆ°è¾¾è¯¥è·¯å¾„ç‚¹
            await self._wait_for_waypoint_reached(
                waypoint["position"],
                timeout=waypoint["speed"] * 2 + 5
            )

            # å®æ—¶æ£€æŸ¥éšœç¢ç‰©
            if await self._check_immediate_obstacles():
                logger.warning("æ£€æµ‹åˆ°å³æ—¶éšœç¢ç‰©ï¼Œåœæ­¢ç§»åŠ¨")
                await robot_interface.send_velocity_command(0, 0, 0)
                return OperationResult(
                    status=OperationStatus.FAILED,
                    error_message="æ£€æµ‹åˆ°å³æ—¶éšœç¢ç‰©"
                )

        return OperationResult(
            status=OperationStatus.SUCCESS,
            message=f"æˆåŠŸç§»åŠ¨åˆ° {waypoints[-1]['position']}"
        )

    async def _execute_manipulation(self, operation, robot_interface):
        """
        æ‰§è¡Œæ“ä½œä»»åŠ¡

        ç¤ºä¾‹: pickupæ“ä½œ
        """

        object_id = operation.parameters["object_id"]
        grip_force = operation.parameters["grip_force"]
        lift_speed = operation.parameters["lift_speed"]

        logger.info(f"å¼€å§‹æŠ“å–ç‰©ä½“: {object_id}")

        # 1. æ¥è¿‘ç‰©ä½“
        await robot_interface.send_arm_trajectory(
            joint_positions={
                "shoulder": 0.5,
                "elbow": 0.8,
                "wrist": 0.3
            },
            speed=0.1
        )

        # 2. é—­åˆå¤¹çˆª
        await robot_interface.send_gripper_command(
            force=grip_force,
            position=0.0  # å®Œå…¨é—­åˆ
        )

        # ç­‰å¾…å¤¹çˆªé—­åˆ
        await asyncio.sleep(1.0)

        # 3. éªŒè¯æŠ“å–æˆåŠŸ
        gripper_state = await robot_interface.get_gripper_state()
        if gripper_state["object_detected"]:
            logger.info("æˆåŠŸæŠ“å–ç‰©ä½“")

            # 4. ç¼“æ…¢æŠ¬èµ·
            await robot_interface.send_arm_trajectory(
                joint_positions={
                    "shoulder": 0.5,
                    "elbow": 0.6,
                    "wrist": 0.3
                },
                speed=lift_speed
            )

            await asyncio.sleep(2.0)

            return OperationResult(
                status=OperationStatus.SUCCESS,
                message=f"æˆåŠŸæŠ“å–å¹¶æŠ¬èµ· {object_id}"
            )
        else:
            return OperationResult(
                status=OperationStatus.FAILED,
                error_message="æŠ“å–å¤±è´¥ï¼Œæœªæ£€æµ‹åˆ°ç‰©ä½“"
            )
```

#### 6.2.2 æ“ä½œåº“

**èŒè´£**: å®šä¹‰å…·ä½“çš„æ“ä½œå®ç°

```python
# åœ¨"å»æ‹¿æ¯æ°´"ä»»åŠ¡ä¸­çš„å®é™…æ“ä½œ

class MovementOperations:
    """ç§»åŠ¨æ“ä½œåº“"""

    async def follow_path(self, waypoints, avoidance_strategy):
        """è·¯å¾„è·Ÿè¸ª"""

        for waypoint in waypoints:
            # è®¡ç®—é€Ÿåº¦å’Œè½¬å‘
            linear_speed = waypoint["speed"]
            angular_velocity = self._calculate_heading(waypoint)

            # å‘é€åˆ°åº•å±‚æ§åˆ¶å™¨
            await self.robot_interface.cmd_vel(
                linear_x=linear_speed,
                angular_z=angular_velocity
            )

            # ç­‰å¾…åˆ°è¾¾
            await self._wait_for_reaching(waypoint)

class ManipulationOperations:
    """æ“ä½œæ“ä½œåº“"""

    async def pickup(self, object_id, grip_force, lift_speed):
        """æŠ“å–æ“ä½œ"""

        # 1. è§†è§‰ä¼ºæœæ¥è¿‘
        await self._visual_servo_approach(object_id)

        # 2. è°ƒæ•´å¤¹çˆªå§¿æ€
        await self._align_gripper(object_id)

        # 3. é—­åˆå¤¹çˆª
        await self.robot_interface.gripper_close(grip_force)

        # 4. éªŒè¯æŠ“å–
        if await self._verify_grasp():
            # 5. æŠ¬èµ·
            await self.robot_interface.arm_lift(lift_speed)
            return True
        return False

class PerceptionOperations:
    """æ„ŸçŸ¥æ“ä½œåº“"""

    async def detect_objects(self, object_types, area):
        """ç‰©ä½“æ£€æµ‹"""

        # è·å–å½“å‰å›¾åƒ
        rgb = await self.camera.get_rgb()
        depth = await self.camera.get_depth()

        # è¿è¡Œç‰©ä½“æ£€æµ‹
        detections = await self.detector.detect(
            rgb, depth,
            classes=object_types
        )

        return detections

    async def update_perception(self):
        """æ›´æ–°æ„ŸçŸ¥"""

        # è·å–æœ€æ–°ä¼ æ„Ÿå™¨æ•°æ®
        sensor_data = await self.sensor_manager.get_current_data()

        # æ›´æ–°ä¸–ç•Œæ¨¡å‹
        self.world_model.update_from_perception(sensor_data)

        return sensor_data
```

### 6.3 æ‰§è¡Œå±‚æ€»ç»“

**è¾“å…¥**: æ“ä½œåºåˆ—
**å¤„ç†**:
1. éå†æ“ä½œé˜Ÿåˆ—
2. æ£€æŸ¥å‰ç½®æ¡ä»¶
3. å®‰å…¨æ£€æŸ¥
4. æ‰§è¡Œæ“ä½œ
5. éªŒè¯åç½®æ¡ä»¶
6. é”™è¯¯å¤„ç†

**è¾“å‡º**: æ‰§è¡Œç»“æœ + æœºå™¨äººçŠ¶æ€æ›´æ–°

```python
ExecutionOutput(
    operation_id="op_006",
    status=OperationStatus.SUCCESS,
    message="æˆåŠŸæŠ“å–å¹¶æŠ¬èµ· cup_001",
    execution_time=7.8,
    actual_parameters={
        "grip_force": 0.4,
        "lift_speed": 0.05,
        "final_height": 1.0
    },
    robot_state={
        "position": [3.0, 0.0],
        "holding": ["cup_001"],
        "battery": 82
    }
)
```

---

## 7. æ¨¡å—é—´åä½œå›¾

### 7.1 å®Œæ•´æµç¨‹å›¾

```mermaid
flowchart TD
    Start([ç”¨æˆ·: "å»æ‹¿æ¯æ°´"]) --> DLG[å¯¹è¯ç®¡ç†å™¨<br/>DialogueManager]

    DLG --> COT[CoTæ¨ç†å¼•æ“<br/>CoTEngine]

    COT --> WM[ä¸–ç•Œæ¨¡å‹<br/>WorldModel]
    WM --> PERC[æ„ŸçŸ¥å±‚<br/>Perception Layer]

    subgraph "æ„ŸçŸ¥å±‚ - æˆ‘çœ‹åˆ°ä»€ä¹ˆï¼Ÿ"
        PERC --> SENSOR[ä¼ æ„Ÿå™¨ç®¡ç†å™¨<br/>é‡‡é›†å¤šæ¨¡æ€æ•°æ®]
        SENSOR --> VLM[VLMæ„ŸçŸ¥<br/>åœºæ™¯è¯­ä¹‰ç†è§£]
        SENSOR --> MAP[å æ®æ …æ ¼<br/>ç¯å¢ƒåœ°å›¾]
        SENSOR --> FUSION[ä¼ æ„Ÿå™¨èåˆ<br/>EKF/UKF]
    end

    PERC -->|æ„ŸçŸ¥æ•°æ®åŒ…| WM
    WM -->|è§„åˆ’ä¸Šä¸‹æ–‡| COT

    COT -->|æ¨ç†ç»“è®º| PLAN[è§„åˆ’å±‚<br/>Planning Layer]

    subgraph "è§„åˆ’å±‚ - å¦‚ä½•åˆ†è§£ï¼Ÿ"
        PLAN --> TASK[ä»»åŠ¡è§„åˆ’å™¨<br/>ç”Ÿæˆæ“ä½œåºåˆ—]
        TASK --> SKILL[æŠ€èƒ½è§„åˆ’å™¨<br/>ç»„åˆæŠ€èƒ½]
        TASK --> ACTION[åŠ¨ä½œè§„åˆ’å™¨<br/>ç”Ÿæˆå‚æ•°]
    end

    PLAN -->|æ“ä½œåºåˆ—| EXEC[æ‰§è¡Œå±‚<br/>Execution Layer]

    subgraph "æ‰§è¡Œå±‚ - å¦‚ä½•æ‰§è¡Œï¼Ÿ"
        EXEC --> EXECUTOR[æ‰§è¡Œå¼•æ“<br/>ç®¡ç†æ“ä½œé˜Ÿåˆ—]
        EXECUTOR --> OPS[æ“ä½œåº“<br/>å…·ä½“æ“ä½œå®ç°]
    end

    EXEC -->|æ§åˆ¶æŒ‡ä»¤| ROBOT[æœºå™¨äººç¡¬ä»¶<br/>Robot Hardware]

    ROBOT -->|æ‰§è¡Œåé¦ˆ| EXEC
    EXEC -->|çŠ¶æ€æ›´æ–°| WM

    WM -.->|å˜åŒ–æ£€æµ‹| MONITOR[æ„ŸçŸ¥ç›‘æ§å™¨<br/>PerceptionMonitor]
    MONITOR -.->|è§¦å‘é‡è§„åˆ’| COT

    EXEC -->|è¿›åº¦æ±‡æŠ¥| DLG
    DLG -->|ä»»åŠ¡å®Œæˆ| End([ç”¨æˆ·: "æ‹¿åˆ°æ°´æ¯äº†"])

    style PERC fill:#e1f5ff,stroke:#01579b,stroke-width:3px
    style WM fill:#f3e5f5,stroke:#4a148c,stroke-width:3px
    style COT fill:#f3e5f5,stroke:#4a148c,stroke-width:3px
    style PLAN fill:#fff3e0,stroke:#e65100,stroke-width:3px
    style EXEC fill:#e8f5e9,stroke:#1b5e20,stroke-width:3px
```

### 7.2 æ•°æ®æµå‘å›¾

```mermaid
graph LR
    subgraph "æ„ŸçŸ¥æ•°æ®æµ"
        CAM[ç›¸æœº] -->|RGBå›¾åƒ| VLM[VLMåˆ†æ]
        LIDAR[æ¿€å…‰é›·è¾¾] -->|ç‚¹äº‘| MAP[åœ°å›¾æ„å»º]
        IMU[IMU] -->|å§¿æ€| FUSION[ä¼ æ„Ÿå™¨èåˆ]
        ODOM[é‡Œç¨‹è®¡] -->|ä½ç½®| FUSION
    end

    subgraph "è®¤çŸ¥æ•°æ®æµ"
        VLM -->|è¯­ä¹‰å¯¹è±¡| WM[ä¸–ç•Œæ¨¡å‹]
        MAP -->|å æ®æ …æ ¼| WM
        FUSION -->|æœºå™¨äººä½å§¿| WM
        WM -->|ç¯å¢ƒä¸Šä¸‹æ–‡| COT[CoTæ¨ç†]
        USER[ç”¨æˆ·æŒ‡ä»¤] -->|è‡ªç„¶è¯­è¨€| COT
    end

    subgraph "è§„åˆ’æ•°æ®æµ"
        COT -->|æ¨ç†ç»“è®º| PLAN[ä»»åŠ¡è§„åˆ’å™¨]
        WM -->|æ„ŸçŸ¥ä¸Šä¸‹æ–‡| PLAN
        PLAN -->|æ“ä½œåºåˆ—| EXEC[æ‰§è¡Œå¼•æ“]
    end

    subgraph "æ‰§è¡Œæ•°æ®æµ"
        EXEC -->|æ§åˆ¶æŒ‡ä»¤| ROBOT[æœºå™¨äºº]
        ROBOT -->|ä¼ æ„Ÿå™¨æ•°æ®| PERC[æ„ŸçŸ¥å±‚]
        EXEC -->|çŠ¶æ€æ›´æ–°| WM
    end

    subgraph "ç›‘æ§æ•°æ®æµ"
        WM -->|çŠ¶æ€å˜åŒ–| MONITOR[æ„ŸçŸ¥ç›‘æ§]
        MONITOR -->|é‡è§„åˆ’ä¿¡å·| COT
    end

    style PERC fill:#e1f5ff,stroke:#01579b
    style WM fill:#f3e5f5,stroke:#4a148c
    style COT fill:#f3e5f5,stroke:#4a148c
    style PLAN fill:#fff3e0,stroke:#e65100
    style EXEC fill:#e8f5e9,stroke:#1b5e20
```

---

## 8. å…³é”®è®¾è®¡è¦ç‚¹

### 8.1 æ¨¡å—èŒè´£å¯¹æ¯”

| ç»´åº¦ | æ„ŸçŸ¥å±‚ | è®¤çŸ¥å±‚ | è§„åˆ’å±‚ | æ‰§è¡Œå±‚ |
|------|--------|--------|--------|--------|
| **æ ¸å¿ƒé—®é¢˜** | æˆ‘çœ‹åˆ°ä»€ä¹ˆï¼Ÿ | æˆ‘ç†è§£äº†ä»€ä¹ˆï¼Ÿ | å¦‚ä½•åˆ†è§£ä»»åŠ¡ï¼Ÿ | å¦‚ä½•æ‰§è¡Œï¼Ÿ |
| **è¾“å…¥** | ä¼ æ„Ÿå™¨åŸå§‹æ•°æ® | æ„ŸçŸ¥æ•°æ® + ç”¨æˆ·æŒ‡ä»¤ | è§„åˆ’ä¸Šä¸‹æ–‡ + æ¨ç†ç»“æœ | æ“ä½œåºåˆ— |
| **è¾“å‡º** | ç»“æ„åŒ–æ„ŸçŸ¥æ•°æ®åŒ… | è§„åˆ’ä¸Šä¸‹æ–‡ + æ¨ç†ç»“è®º | æ“ä½œåºåˆ— | æ‰§è¡Œç»“æœ + çŠ¶æ€æ›´æ–° |
| **æ—¶é—´å°ºåº¦** | æ¯«ç§’çº§ (å®æ—¶) | ç§’çº§ (æ¨ç†) | ç§’çº§ (è§„åˆ’) | æ¯«ç§’çº§ (æ§åˆ¶) |
| **æŠ½è±¡å±‚æ¬¡** | ä¿¡å·/ç‰¹å¾ | è¯­ä¹‰/æ¦‚å¿µ | ä»»åŠ¡/æ“ä½œ | åŠ¨ä½œ/æŒ‡ä»¤ |
| **å¤„ç†æ–¹å¼** | å¹¶è¡Œã€å®æ—¶ | ä¸²è¡Œã€æ¨ç† | ä¸²è¡Œã€æœç´¢ | ä¸²è¡Œã€æ§åˆ¶ |
| **å®¹é”™æ€§** | é«˜ (æœ‰å†—ä½™ä¼ æ„Ÿå™¨) | ä¸­ (å¯é‡æ–°æ¨ç†) | ä¸­ (å¯é‡è§„åˆ’) | ä½ (éœ€è¦ç²¾ç¡®æ§åˆ¶) |

### 8.2 æ•°æ®ä¼ é€’é“¾è·¯

```
ä¼ æ„Ÿå™¨åŸå§‹æ•°æ®
    â†“
[æ„ŸçŸ¥å±‚] ç‰¹å¾æå– + è¯­ä¹‰ç†è§£ + ä¼ æ„Ÿå™¨èåˆ
    â†“
æ„ŸçŸ¥æ•°æ®åŒ… {
  robot_position,
  obstacles,
  targets,
  occupancy_grid
}
    â†“
[è®¤çŸ¥å±‚] çŠ¶æ€ç»´æŠ¤ + å˜åŒ–æ£€æµ‹ + CoTæ¨ç†
    â†“
è§„åˆ’ä¸Šä¸‹æ–‡ + æ¨ç†ç»“è®º {
  current_position,
  constraints,
  reasoning_chain,
  decision
}
    â†“
[è§„åˆ’å±‚] ä»»åŠ¡åˆ†è§£ + æ“ä½œç”Ÿæˆ + ä¼˜åŒ–
    â†“
æ“ä½œåºåˆ— [
  Operation {name, parameters, preconditions, postconditions},
  ...
]
    â†“
[æ‰§è¡Œå±‚] æ¡ä»¶æ£€æŸ¥ + æ§åˆ¶æŒ‡ä»¤å‘é€ + çŠ¶æ€éªŒè¯
    â†“
æœºå™¨äººåŠ¨ä½œ + æ–°çš„ä¼ æ„Ÿå™¨æ•°æ®
    â†“
å¾ªç¯å›æ„ŸçŸ¥å±‚
```

### 8.3 æ„ŸçŸ¥é©±åŠ¨çš„ä½“ç°

åœ¨"å»æ‹¿æ¯æ°´"ä»»åŠ¡ä¸­ï¼Œæ„ŸçŸ¥é©±åŠ¨ä½“ç°åœ¨ä»¥ä¸‹æ–¹é¢ï¼š

1. **åˆå§‹è§„åˆ’é˜¶æ®µ**
   - æ„ŸçŸ¥å±‚æ£€æµ‹åˆ°éšœç¢ç‰© â†’ è§„åˆ’å±‚ç”Ÿæˆé¿éšœè·¯å¾„
   - VLMè¯†åˆ«æ°´æ¯åŠæ»¡ â†’ CoTæ¨ç†å»ºè®®è°¨æ…æ“ä½œ
   - åœ°æ¯¯ç¯å¢ƒæ£€æµ‹ â†’ è§„åˆ’å±‚é™ä½ç§»åŠ¨é€Ÿåº¦

2. **æ‰§è¡Œé˜¶æ®µ**
   - å®æ—¶æ„ŸçŸ¥æ›´æ–° â†’ æ£€æµ‹åˆ°æ¤…å­ç§»åŠ¨ â†’ è§¦å‘é‡è§„åˆ’
   - è§†è§‰åé¦ˆå¤¹çˆªæœªé—­åˆ â†’ è°ƒæ•´æŠ“å–åŠ›åº¦
   - IMUæ£€æµ‹å€¾æ–œ â†’ å‡æ…¢ç§»åŠ¨é€Ÿåº¦

3. **å¼‚å¸¸å¤„ç†**
   - æ„ŸçŸ¥æ£€æµ‹åˆ°æ–°éšœç¢ç‰© â†’ CoTè¯„ä¼°å½±å“ â†’ å†³å®šæ˜¯å¦é‡è§„åˆ’
   - ç›®æ ‡ç‰©ä½“ä¸¢å¤± â†’ è§¦å‘æœç´¢ç­–ç•¥
   - è·¯å¾„é˜»å¡ â†’ è§„åˆ’æ–°è·¯å¾„

### 8.4 CoTåœ¨è®¤çŸ¥å±‚çš„ä»·å€¼

CoTæ¨ç†å¼•æ“åœ¨è®¤çŸ¥å±‚ï¼ˆè€Œéè§„åˆ’å±‚ï¼‰çš„åŸå› ï¼š

**1. æœåŠ¡äºå¤šä¸ªåœºæ™¯**
- **åˆå§‹è§„åˆ’**: "å»æ‹¿æ¯æ°´"æ„å‘³ç€ä»€ä¹ˆï¼Ÿ
- **é‡è§„åˆ’**: æ¤…å­ç§»åŠ¨äº†ï¼Œæ˜¯å¦éœ€è¦è°ƒæ•´ï¼Ÿ
- **å¼‚å¸¸å¤„ç†**: æ°´æ¯æ»‘è½äº†ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ
- **æŒ‡ä»¤æ¾„æ¸…**: "é‚£ä¸ªæ¯å­"æ˜¯æŒ‡å“ªä¸ªï¼Ÿ

**2. éœ€è¦å…¨å±€ä¿¡æ¯**
- CoTéœ€è¦è®¿é—®å®Œæ•´çš„ä¸–ç•Œæ¨¡å‹
- éœ€è¦ç†è§£ç”¨æˆ·æ„å›¾å’Œå¯¹è¯å†å²
- éœ€è¦è¯„ä¼°ç¯å¢ƒå˜åŒ–çš„å½±å“

**3. è¾“å‡ºæ˜¯"å†³ç­–"è€Œé"æ“ä½œ"**
- CoTè¾“å‡º: "åº”è¯¥è°¨æ…æ“ä½œï¼Œé‡‡ç”¨å³ä¾§ç»•è¡Œ"
- TaskPlannerè¾“å‡º: `follow_path(waypoints=[...], speed=0.3)`

### 8.5 å…³é”®æ—¶é—´èŠ‚ç‚¹

| æ—¶é—´ | äº‹ä»¶ | è´Ÿè´£æ¨¡å— |
|------|------|----------|
| T0+0.0s | æ”¶åˆ°ç”¨æˆ·æŒ‡ä»¤ | å¯¹è¯ç®¡ç†å™¨ |
| T0+0.1s | å¯åŠ¨ä¼ æ„Ÿå™¨é‡‡é›† | ä¼ æ„Ÿå™¨ç®¡ç†å™¨ |
| T0+0.5s | å®Œæˆæ„ŸçŸ¥æ•°æ®èåˆ | ä¼ æ„Ÿå™¨èåˆ |
| T0+1.0s | VLMåœºæ™¯åˆ†æå®Œæˆ | VLMæ„ŸçŸ¥ |
| T0+1.5s | ä¸–ç•Œæ¨¡å‹æ›´æ–°å®Œæˆ | ä¸–ç•Œæ¨¡å‹ |
| T0+2.0s | CoTæ¨ç†å®Œæˆ | CoTå¼•æ“ |
| T0+3.0s | ä»»åŠ¡è§„åˆ’å®Œæˆ | ä»»åŠ¡è§„åˆ’å™¨ |
| T0+3.5s | å¼€å§‹æ‰§è¡Œæ“ä½œ1 | æ‰§è¡Œå¼•æ“ |
| T0+18.5s | å®Œæˆå¯¼èˆªåˆ°æ¡Œå­ | æ‰§è¡Œå¼•æ“ |
| T0+20.0s | è°ƒæ•´å§¿æ€å¹¶ç¡®è®¤ç›®æ ‡ | æ‰§è¡Œå¼•æ“ |
| T0+28.0s | æˆåŠŸæŠ“å–æ°´æ¯ | æ‰§è¡Œå¼•æ“ |
| T0+40.0s | è¿”å›èµ·ç‚¹ | æ‰§è¡Œå¼•æ“ |
| T0+40.5s | ä»»åŠ¡å®Œæˆæ±‡æŠ¥ | å¯¹è¯ç®¡ç†å™¨ |

---

## 9. æ€»ç»“

### æ„ŸçŸ¥å±‚ - "çœ¼ç›å’Œè€³æœµ"
**èŒè´£**: é‡‡é›†ã€ç†è§£ã€èåˆå¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®
**æ ¸å¿ƒèƒ½åŠ›**:
- å¤šä¼ æ„Ÿå™¨åŒæ­¥é‡‡é›†
- è§†è§‰è¯­ä¹‰ç†è§£ (VLM)
- ç¯å¢ƒåœ°å›¾æ„å»º
- ä¼ æ„Ÿå™¨æ•°æ®èåˆ

### è®¤çŸ¥å±‚ - "å¤§è„‘çš®å±‚"
**èŒè´£**: ç†è§£æ„å›¾ã€æ¨ç†å†³ç­–ã€çŠ¶æ€ç»´æŠ¤
**æ ¸å¿ƒèƒ½åŠ›**:
- ç»Ÿä¸€çš„ä¸–ç•ŒçŠ¶æ€è¡¨ç¤º
- é“¾å¼æ€ç»´æ¨ç† (CoT)
- å¤šè½®å¯¹è¯ç®¡ç†
- æŒç»­å˜åŒ–ç›‘æ§

### è§„åˆ’å±‚ - "ç­–ç•¥å®¶"
**èŒè´£**: ä»»åŠ¡åˆ†è§£ã€æ“ä½œç”Ÿæˆã€åºåˆ—ä¼˜åŒ–
**æ ¸å¿ƒèƒ½åŠ›**:
- é«˜å±‚ä»»åŠ¡åˆ†è§£
- æ„ŸçŸ¥é©±åŠ¨çš„è§„åˆ’è°ƒæ•´
- æŠ€èƒ½ç»„åˆä¸é‡ç”¨
- çº¦æŸåº”ç”¨ä¸éªŒè¯

### æ‰§è¡Œå±‚ - "è¿åŠ¨ç¥ç»"
**èŒè´£**: æ“ä½œæ‰§è¡Œã€çŠ¶æ€éªŒè¯ã€å¼‚å¸¸å¤„ç†
**æ ¸å¿ƒèƒ½åŠ›**:
- æ“ä½œé˜Ÿåˆ—ç®¡ç†
- å‰ç½®/åç½®æ¡ä»¶æ£€æŸ¥
- ç²¾ç¡®æ§åˆ¶æŒ‡ä»¤ç”Ÿæˆ
- å®æ—¶çŠ¶æ€åé¦ˆ

### å…³é”®è®¾è®¡æ€æƒ³
1. **æ„ŸçŸ¥é©±åŠ¨**: æ„ŸçŸ¥æ•°æ®é©±åŠ¨æ•´ä¸ªå†³ç­–-è§„åˆ’-æ‰§è¡Œé—­ç¯
2. **åˆ†å±‚è§£è€¦**: æ¯å±‚ä¸“æ³¨è‡ªå·±çš„èŒè´£ï¼Œé€šè¿‡æ¸…æ™°æ¥å£äº¤äº’
3. **CoTå¢å¼ºè®¤çŸ¥**: CoTä½œä¸ºè®¤çŸ¥å±‚çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼ŒæœåŠ¡å¤šä¸ªåœºæ™¯
4. **æŒç»­ç›‘æ§**: å®æ—¶æ„ŸçŸ¥å˜åŒ–è§¦å‘é‡è§„åˆ’ï¼Œç¡®ä¿é€‚åº”æ€§

---

*æ–‡æ¡£ç‰ˆæœ¬: 1.0*
*ç”Ÿæˆæ—¥æœŸ: 2026-01-06*
*é¡¹ç›®: Brain - æ„ŸçŸ¥é©±åŠ¨çš„æ™ºèƒ½æ— äººç³»ç»Ÿ*
*ç¤ºä¾‹ä»»åŠ¡: "å»æ‹¿æ¯æ°´"*
