#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®¤çŸ¥å±‚ä¸–ç•ŒçŠ¶æ€å¯è§†åŒ–æ¼”ç¤º
ä½¿ç”¨çœŸå®ROS2ä¼ æ„Ÿå™¨æ•°æ®å±•ç¤ºè®¤çŸ¥å±‚ä¸–ç•ŒçŠ¶æ€å¯è§†åŒ–

Usage:
    # Terminal 1: æ’­æ”¾rosbagï¼ˆå¯é€‰ï¼Œå¦‚æœæ²¡æœ‰å®æ—¶ä¼ æ„Ÿå™¨ï¼‰
    export ROS_DOMAIN_ID=42
    ros2 bag play <rosbagæ–‡ä»¶> --loop

    # Terminal 2: å¯åŠ¨å¯è§†åŒ–
    export ROS_DOMAIN_ID=42
    python3 scripts/show_cognitive_world_state.py

    # Terminal 3: å¯åŠ¨RVizæŸ¥çœ‹å¯è§†åŒ–
    rviz2 -d config/rviz2/cognitive_world_model_correct.rviz
"""

import sys
import os
import time
import math
import numpy as np
from datetime import datetime
from typing import Dict, Any, Optional, Tuple
from collections import deque
from threading import Thread, Lock
from concurrent.futures import ThreadPoolExecutor

# è®¾ç½®ROS Domain ID
os.environ['ROS_DOMAIN_ID'] = '42'

# Add Brain to path
sys.path.insert(0, '/media/yangyuhui/CODES1/Brain')

import rclpy
from rclpy.node import Node
from rclpy.executors import MultiThreadedExecutor

# ROS2æ¶ˆæ¯ç±»å‹
from sensor_msgs.msg import Image as SensorImage, PointCloud2
from nav_msgs.msg import Odometry
from geometry_msgs.msg import Point, TransformStamped
from tf2_ros import StaticTransformBroadcaster

# Cognitive layer imports
from brain.cognitive.world_model.world_model import WorldModel
from brain.cognitive.world_model.world_model_visualizer import WorldModelVisualizer

# VLMé…ç½®ï¼šä½¿ç”¨Mockè¿˜æ˜¯çœŸå®Ollama
USE_MOCK_VLM = False  # ğŸ”§ ä½¿ç”¨çœŸå®Ollama VLM - å›¾åƒç¼–ç å·²ä¿®å¤

# ä½¿ç”¨ç»Ÿä¸€çš„VLMå·¥å‚å‡½æ•°
from brain.perception.vlm import get_vlm_client

if USE_MOCK_VLM:
    print("ğŸ­ ä½¿ç”¨ Mock VLM å®¢æˆ·ç«¯ï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼‰")
else:
    print("ğŸ¤– ä½¿ç”¨çœŸå® Ollama LLaVA å®¢æˆ·ç«¯")


def quaternion_to_yaw(quaternion) -> float:
    """ä»å››å…ƒæ•°è®¡ç®—èˆªå‘è§’ï¼ˆyawï¼‰"""
    x, y, z, w = quaternion.x, quaternion.y, quaternion.z, quaternion.w

    # è®¡ç®—yaw
    siny_cosp = 2.0 * (w * z + x * y)
    cosy_cosp = 1.0 - 2.0 * (y * y + z * z)
    yaw = math.atan2(siny_cosp, cosy_cosp)

    return yaw


def ros_image_to_numpy(img_msg: SensorImage) -> np.ndarray:
    """ç®€å•çš„ROS Imageè½¬numpyè½¬æ¢ï¼ˆä¸ä¾èµ–cv_bridgeï¼‰"""
    if img_msg.encoding in ['rgb8', 'bgr8', 'mono8']:
        dtype = np.uint8
    else:
        raise ValueError(f"Unsupported encoding: {img_msg.encoding}")

    arr = np.frombuffer(img_msg.data, dtype=dtype)

    if img_msg.encoding in ['rgb8', 'bgr8']:
        n_channels = 3
    else:
        n_channels = 1

    if n_channels == 1:
        arr = arr.reshape((img_msg.height, img_msg.width))
    else:
        arr = arr.reshape((img_msg.height, img_msg.width, n_channels))

    if img_msg.encoding == 'bgr8':
        arr = arr[:, :, ::-1].copy()

    return arr


def parse_pointcloud(msg: PointCloud2) -> Optional[np.ndarray]:
    """è§£æç‚¹äº‘æ•°æ®"""
    try:
        # è·å–ç‚¹äº‘å­—æ®µ
        fields = {}
        for field in msg.fields:
            fields[field.name] = field

        # æ£€æŸ¥æ˜¯å¦æœ‰xyzå­—æ®µ
        if 'x' not in fields or 'y' not in fields or 'z' not in fields:
            return None

        # è§£æç‚¹äº‘
        cloud_points = []
        data = msg.data
        point_step = msg.point_step

        # éå†æ¯ä¸ªç‚¹
        for i in range(0, len(data), point_step):
            # æå–xyzåæ ‡
            x_offset = fields['x'].offset
            y_offset = fields['y'].offset
            z_offset = fields['z'].offset

            if i + x_offset + 4 > len(data) or i + y_offset + 4 > len(data) or i + z_offset + 4 > len(data):
                continue

            x_bytes = data[i + x_offset : i + x_offset + 4]
            y_bytes = data[i + y_offset : i + y_offset + 4]
            z_bytes = data[i + z_offset : i + z_offset + 4]

            x = np.frombuffer(x_bytes, dtype=np.float32)[0]
            y = np.frombuffer(y_bytes, dtype=np.float32)[0]
            z = np.frombuffer(z_bytes, dtype=np.float32)[0]

            cloud_points.append([x, y, z])

        return np.array(cloud_points)

    except Exception:
        return None


class CognitiveWorldStateViz(Node):
    """è®¤çŸ¥ä¸–ç•ŒçŠ¶æ€å¯è§†åŒ–èŠ‚ç‚¹"""

    def __init__(self, duration_seconds: float = 600.0):
        super().__init__('cognitive_world_state_viz')

        # é…ç½®
        self.duration_seconds = duration_seconds
        self.start_time = time.time()

        # çŠ¶æ€
        self.odom_count = 0
        self.pointcloud_count = 0
        self.rgb_count = 0
        self.vlm_count = 0
        self.last_display_time = 0
        self.display_interval = 5.0  # æ¯5ç§’æ˜¾ç¤ºä¸€æ¬¡çŠ¶æ€

        # ä½å§¿å†å² (ç”¨äºæ—¶é—´æˆ³å¯¹é½)
        # å­˜å‚¨ (timestamp, pose_dict) å…ƒç»„
        self.pose_history = deque(maxlen=1000)  # ä¿ç•™æœ€è¿‘1000ä¸ªä½å§¿
        self.pose_history_lock = Lock()

        # VLMå®¢æˆ·ç«¯ (å¼‚æ­¥å¤„ç†)
        self.vlm_client = get_vlm_client(use_mock=USE_MOCK_VLM)
        self.vlm_enabled = True
        self.vlm_processing_interval = 5.0  # æ¯5ç§’å¤„ç†ä¸€æ¬¡VLM (é¿å…è¿‡è½½)
        self.last_vlm_time = 0
        self.vlm_processing = False  # é˜²æ­¢é‡å¤æäº¤VLMä»»åŠ¡

        # VLMçº¿ç¨‹æ± ï¼ˆé™åˆ¶å¹¶å‘ï¼Œé¿å…çº¿ç¨‹è€—å°½ï¼‰
        self.vlm_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="vlm_worker")

        self.get_logger().info("=" * 80)
        self.get_logger().info("ğŸ¯ è®¤çŸ¥å±‚ä¸–ç•ŒçŠ¶æ€å¯è§†åŒ–")
        self.get_logger().info("=" * 80)

        # 1. åˆå§‹åŒ–WorldModel
        self.get_logger().info("æ­£åœ¨åˆå§‹åŒ–WorldModel...")
        world_config = {
            'map_resolution': 0.1,  # 10cm per cell
            'map_size': 50.0,      # 50m x 50m
        }
        self.world_model = WorldModel(config=world_config)

        # åˆå§‹åŒ–å›ºå®šå¤§å°çš„åœ°å›¾ï¼Œé˜²æ­¢å°ºå¯¸è·³å˜
        self._initialize_fixed_map()

        self.get_logger().info("âœ… WorldModelåˆå§‹åŒ–å®Œæˆ")
        self.get_logger().info(f"   åœ°å›¾åˆ†è¾¨ç‡: {self.world_model.map_resolution}m/cell")
        self.get_logger().info(f"   åœ°å›¾åŸç‚¹: {self.world_model.map_origin}")
        self.get_logger().info(f"   åœ°å›¾å°ºå¯¸: {self.world_model.current_map.shape if self.world_model.current_map is not None else 'None'}")

        # 2. åˆå§‹åŒ–å¯è§†åŒ–å™¨
        self.get_logger().info("æ­£åœ¨åˆå§‹åŒ–å¯è§†åŒ–å™¨...")
        self.visualizer = WorldModelVisualizer(
            world_model=self.world_model,
            publish_rate=2.0  # 2Hz
        )
        self.get_logger().info("âœ… å¯è§†åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
        self.get_logger().info("   å‘å¸ƒè¯é¢˜:")
        self.get_logger().info("     - /world_model/semantic_grid")
        self.get_logger().info("     - /world_model/semantic_markers")
        self.get_logger().info("     - /world_model/belief_markers")
        self.get_logger().info("     - /world_model/trajectory")
        self.get_logger().info("     - /world_model/frontiers")
        self.get_logger().info("     - /world_model/change_events")
        self.get_logger().info("     - /vlm/detections")

        # 3. å‘å¸ƒé™æ€TFå˜æ¢ (map -> odom)
        self._publish_static_tf()

        # 4. è®¢é˜…ä¼ æ„Ÿå™¨è¯é¢˜
        self._setup_subscribers()

        self.get_logger().info("=" * 80)
        self.get_logger().info("âœ… è®¤çŸ¥ä¸–ç•ŒçŠ¶æ€å¯è§†åŒ–èŠ‚ç‚¹å·²å¯åŠ¨")
        self.get_logger().info("=" * 80)

    def _publish_static_tf(self):
        """å‘å¸ƒé™æ€TFå˜æ¢ (map -> odom)"""
        try:
            # åˆ›å»ºé™æ€TFå¹¿æ’­å™¨
            self.tf_broadcaster = StaticTransformBroadcaster(self)

            # åˆ›å»ºå˜æ¢æ¶ˆæ¯
            t = TransformStamped()
            t.header.stamp = self.get_clock().now().to_msg()
            t.header.frame_id = "map"
            t.child_frame_id = "odom"

            # è®¾ç½®å˜æ¢ä¸ºæ’ç­‰å˜æ¢ï¼ˆmapå’Œodomé‡åˆï¼‰
            t.transform.translation.x = 0.0
            t.transform.translation.y = 0.0
            t.transform.translation.z = 0.0
            t.transform.rotation.x = 0.0
            t.transform.rotation.y = 0.0
            t.transform.rotation.z = 0.0
            t.transform.rotation.w = 1.0

            # å‘é€å˜æ¢
            self.tf_broadcaster.sendTransform(t)

            self.get_logger().info("âœ… å·²å‘å¸ƒé™æ€TF: map -> odom")
        except Exception as e:
            self.get_logger().warning(f"å‘å¸ƒé™æ€TFå¤±è´¥: {e}")

    def _initialize_fixed_map(self):
        """åˆå§‹åŒ–å›ºå®šå¤§å°çš„åœ°å›¾ï¼Œé˜²æ­¢å°ºå¯¸è·³å˜"""
        try:
            # åˆ›å»ºå›ºå®šå¤§å°çš„åœ°å›¾ï¼š500x500 (50m x 50m at 0.1m/cell)
            grid_size = 500
            self.world_model.current_map = np.full((grid_size, grid_size), -1, dtype=np.int8)
            self.world_model.map_origin = (-25.0, -25.0)  # ä¸­å¿ƒä¸º(0,0)

            self.get_logger().info("âœ… å·²åˆå§‹åŒ–å›ºå®šå¤§å°åœ°å›¾")
            self.get_logger().info(f"   åœ°å›¾å°ºå¯¸: {grid_size}x{grid_size}")
            self.get_logger().info(f"   åœ°å›¾èŒƒå›´: 50m x 50m")
        except Exception as e:
            self.get_logger().warning(f"åˆå§‹åŒ–å›ºå®šåœ°å›¾å¤±è´¥: {e}")

    def _sync_occupancy_map(self):
        """ç¡®ä¿åœ°å›¾å°ºå¯¸ä¿æŒå›ºå®š - æ¯æ¬¡è°ƒç”¨éƒ½å¼ºåˆ¶æ£€æŸ¥"""
        target_size = 500

        # æ¯æ¬¡éƒ½å¼ºåˆ¶æ£€æŸ¥ï¼Œç¡®ä¿åœ°å›¾å°ºå¯¸ä¸ä¼šæ”¹å˜
        if (self.world_model.current_map is None or
            self.world_model.current_map.shape[0] != target_size or
            self.world_model.current_map.shape[1] != target_size):

            # å¦‚æœå°ºå¯¸ä¸å¯¹ï¼Œç«‹å³ä¿®æ­£
            if self.odom_count % 50 == 0:  # å¶å°”æ‰“å°æ—¥å¿—
                self.get_logger().warning(f"ä¿®æ­£åœ°å›¾å°ºå¯¸: {self.world_model.current_map.shape if self.world_model.current_map is not None else 'None'} -> {target_size}x{target_size}")

            self.world_model.current_map = np.full((target_size, target_size), -1, dtype=np.int8)
            self.world_model.map_origin = (-25.0, -25.0)

    def _setup_subscribers(self):
        """è®¾ç½®ROS2è®¢é˜…è€…"""
        self.odom_sub = self.create_subscription(
            Odometry,
            '/chassis/odom',
            self.odom_callback,
            10
        )
        self.pointcloud_sub = self.create_subscription(
            PointCloud2,
            '/front_3d_lidar/lidar_points',
            self.pointcloud_callback,
            10
        )
        self.rgb_sub = self.create_subscription(
            SensorImage,
            '/front_stereo_camera/left/image_raw',
            self.rgb_callback,
            10
        )

        self.get_logger().info("ğŸ“¡ å·²åˆ›å»ºROS2è®¢é˜…è€…:")
        self.get_logger().info("   - /chassis/odom (Odometry)")
        self.get_logger().info("   - /front_3d_lidar/lidar_points (PointCloud2)")
        self.get_logger().info("   - /front_stereo_camera/left/image_raw (SensorImage)")

    def odom_callback(self, msg: Odometry):
        """é‡Œç¨‹è®¡å›è°ƒ - æ›´æ–°WorldModelå¹¶è®°å½•ä½å§¿å†å²"""
        self.odom_count += 1

        try:
            # æå–ä½å§¿
            pos = msg.pose.pose.position
            ori = msg.pose.pose.orientation
            yaw = quaternion_to_yaw(ori)

            # æ„å»ºå½“å‰ä½å§¿å­—å…¸
            current_pose = {
                'x': pos.x,
                'y': pos.y,
                'z': pos.z,
                'roll': 0.0,
                'pitch': 0.0,
                'yaw': yaw
            }

            # è·å–æ¶ˆæ¯æ—¶é—´æˆ³ (ç”¨äºæ—¶é—´æˆ³å¯¹é½)
            msg_time = msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9
            pose_timestamp = datetime.fromtimestamp(msg_time) if msg_time > 0 else datetime.now()

            # è®°å½•ä½å§¿å†å² (çº¿ç¨‹å®‰å…¨)
            with self.pose_history_lock:
                self.pose_history.append((pose_timestamp, current_pose.copy()))

            # æ„å»ºæ„ŸçŸ¥æ•°æ®
            perception_data = {
                'timestamp': datetime.now(),
                'pose': current_pose,
                'velocity': {
                    'linear_x': msg.twist.twist.linear.x,
                    'linear_y': msg.twist.twist.linear.y,
                    'linear_z': msg.twist.twist.linear.z,
                    'angular_x': msg.twist.twist.angular.x,
                    'angular_y': msg.twist.twist.angular.y,
                    'angular_z': msg.twist.twist.angular.z
                }
            }

            # æ›´æ–°WorldModel
            self.world_model.update_from_perception(perception_data)

            # åŒæ­¥OccupancyMapperçš„åœ°å›¾åˆ°WorldModel
            self._sync_occupancy_map()

            # å®šæœŸæ˜¾ç¤ºçŠ¶æ€
            self._try_display_status()

            # æ£€æŸ¥è¿è¡Œæ—¶é•¿
            elapsed = time.time() - self.start_time
            if elapsed >= self.duration_seconds:
                self.get_logger().info("=" * 80)
                self.get_logger().info("âœ… æ¼”ç¤ºå®Œæˆ")
                self.get_logger().info(f"   è¿è¡Œæ—¶é•¿: {elapsed:.1f}ç§’")
                self.get_logger().info(f"   é‡Œç¨‹è®¡æ›´æ–°: {self.odom_count}æ¬¡")
                self.get_logger().info(f"   ç‚¹äº‘æ›´æ–°: {self.pointcloud_count}æ¬¡")
                self.get_logger().info("=" * 80)
                rclpy.shutdown()

        except Exception as e:
            self.get_logger().error(f"é‡Œç¨‹è®¡å›è°ƒé”™è¯¯: {e}")

    def pointcloud_callback(self, msg: PointCloud2):
        """ç‚¹äº‘å›è°ƒ - æ›´æ–°å æ®åœ°å›¾"""
        self.pointcloud_count += 1

        try:
            # è§£æç‚¹äº‘
            points = parse_pointcloud(msg)
            if points is None or len(points) == 0:
                return

            # ç®€å•çš„å æ®åœ°å›¾æ›´æ–°ï¼ˆç›´æ¥åœ¨å›ºå®šåœ°å›¾ä¸Šæ ‡è®°ï¼‰
            if self.world_model.current_map is not None:
                self._update_occupancy_from_pointcloud(points)

        except Exception as e:
            # åªè®°å½•é”™è¯¯ï¼Œä¸æ‰“å°åˆ°ç»ˆç«¯é¿å…åˆ·å±
            pass

    def _update_occupancy_from_pointcloud(self, points: np.ndarray):
        """ä»ç‚¹äº‘æ›´æ–°å æ®åœ°å›¾ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        try:
            resolution = self.world_model.map_resolution
            origin_x, origin_y = self.world_model.map_origin
            grid = self.world_model.current_map

            # è¿‡æ»¤æœ‰æ•ˆç‚¹ï¼ˆå»é™¤NaNå’Œå¼‚å¸¸å€¼ï¼‰
            valid_mask = (
                np.isfinite(points[:, 0]) &
                np.isfinite(points[:, 1]) &
                np.isfinite(points[:, 2]) &
                (np.abs(points[:, 0]) < 50.0) &  # é™åˆ¶èŒƒå›´50ç±³
                (np.abs(points[:, 1]) < 50.0) &
                (points[:, 2] > -2.0) & (points[:, 2] < 5.0)  # åˆç†çš„é«˜åº¦èŒƒå›´
            )
            valid_points = points[valid_mask]

            if len(valid_points) == 0:
                return

            # ç‚¹äº‘å·²ç»åœ¨æ­£ç¡®çš„åæ ‡ç³»ä¸­ï¼ˆä¼ æ„Ÿå™¨åæ ‡ç³»æˆ–mapåæ ‡ç³»ï¼‰
            # ç›´æ¥è½¬æ¢ä¸ºæ …æ ¼åæ ‡
            world_x = valid_points[:, 0]
            world_y = valid_points[:, 1]

            # è½¬æ¢åˆ°æ …æ ¼åæ ‡
            gx = ((world_x - origin_x) / resolution).astype(int)
            gy = ((world_y - origin_y) / resolution).astype(int)

            # æ£€æŸ¥è¾¹ç•Œå¹¶æ ‡è®°å æ®
            height, width = grid.shape
            valid_mask = (gx >= 0) & (gx < width) & (gy >= 0) & (gy < height)
            gx_valid = gx[valid_mask]
            gy_valid = gy[valid_mask]

            # æ ‡è®°ä¸ºå æ®ï¼ˆç›´æ¥è¦†ç›–ï¼Œä¸åªæ˜¯æœªçŸ¥åŒºåŸŸï¼‰
            grid[gy_valid, gx_valid] = 100

        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œé¿å…åˆ·å±
            pass

    def rgb_callback(self, msg: SensorImage):
        """RGBå›è°ƒ - VLMè¯­ä¹‰ç†è§£"""
        self.rgb_count += 1

        try:
            # æ£€æŸ¥VLMæ˜¯å¦å¯ç”¨å’Œå¤„ç†é—´éš”
            current_time = time.time()
            if not self.vlm_enabled:
                return
            if (current_time - self.last_vlm_time) < self.vlm_processing_interval:
                return
            if self.vlm_processing:  # å¦‚æœæ­£åœ¨å¤„ç†ï¼Œè·³è¿‡
                return

            # è·å–å›¾åƒæ—¶é—´æˆ³
            msg_time = msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9
            image_timestamp = datetime.fromtimestamp(msg_time) if msg_time > 0 else datetime.now()

            # ğŸ”‘ å…³é”®ï¼šæŸ¥æ‰¾å›¾åƒæ—¶åˆ»å¯¹åº”çš„æœºå™¨äººä½å§¿ï¼ˆæ—¶é—´æˆ³å¯¹é½ï¼‰
            robot_pose = self._find_pose_at_timestamp(image_timestamp)

            # æ ‡è®°ä¸ºå¤„ç†ä¸­
            self.vlm_processing = True

            # ä½¿ç”¨çº¿ç¨‹æ± å¼‚æ­¥å¤„ç†VLMï¼ˆé¿å…é˜»å¡å›è°ƒï¼‰
            def process_vlm():
                try:
                    # ç¼–ç å›¾åƒ
                    image_data = self.vlm_client._encode_image_from_ros(msg)

                    # è°ƒç”¨VLMåˆ†æ
                    self.get_logger().info(f"ğŸ” VLMåˆ†æå›¾åƒ (æ—¶é—´æˆ³: {image_timestamp.strftime('%H:%M:%S.%f')[:-3]})")

                    vlm_result = self.vlm_client.analyze_image(
                        image_data=image_data,
                        prompt=self._create_vlm_prompt(),
                        robot_pose=robot_pose,
                        timestamp=image_timestamp
                    )

                    # æ›´æ–°WorldModelè¯­ä¹‰ç‰©ä½“
                    if 'error' not in vlm_result:
                        self._update_semantic_objects(vlm_result, robot_pose)
                        self.vlm_count += 1
                        self.last_vlm_time = current_time

                        # æ‰“å°ç»“æœ
                        objects = vlm_result.get('objects', [])
                        self.get_logger().info(f"ğŸ“¦ æ›´æ–°äº† {len(objects)} ä¸ªè¯­ä¹‰ç‰©ä½“åˆ°WorldModel")
                        self.get_logger().info(f"âœ… VLMæ£€æµ‹åˆ° {len(objects)} ä¸ªç‰©ä½“")
                        for obj in objects[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                            self.get_logger().info(f"   - {obj}")
                    else:
                        self.get_logger().warning(f"VLMé”™è¯¯: {vlm_result.get('error')}")

                except Exception as e:
                    self.get_logger().error(f"VLMå¤„ç†å¼‚å¸¸: {e}")
                finally:
                    # æ ‡è®°å¤„ç†å®Œæˆ
                    self.vlm_processing = False

            # æäº¤åˆ°çº¿ç¨‹æ± ï¼ˆè€Œä¸æ˜¯åˆ›å»ºæ–°çº¿ç¨‹ï¼‰
            self.vlm_executor.submit(process_vlm)

        except Exception as e:
            self.get_logger().error(f"RGBå›è°ƒé”™è¯¯: {e}")
            self.vlm_processing = False  # ç¡®ä¿åœ¨å‡ºé”™æ—¶é‡ç½®æ ‡å¿—

    def _find_pose_at_timestamp(self, target_timestamp: datetime) -> Dict[str, float]:
        """
        æŸ¥æ‰¾ç»™å®šæ—¶é—´æˆ³å¯¹åº”çš„æœºå™¨äººä½å§¿ï¼ˆæ—¶é—´æˆ³å¯¹é½ï¼‰

        Args:
            target_timestamp: ç›®æ ‡æ—¶é—´æˆ³

        Returns:
            å¯¹åº”çš„æœºå™¨äººä½å§¿å­—å…¸ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›å½“å‰ä½å§¿
        """
        with self.pose_history_lock:
            if not self.pose_history:
                # æ²¡æœ‰å†å²ä½å§¿ï¼Œè¿”å›å½“å‰ä½å§¿
                return self.world_model.robot_position.copy() if hasattr(self.world_model, 'robot_position') else {'x': 0, 'y': 0, 'z': 0, 'yaw': 0}

            # æ‰¾åˆ°æœ€æ¥è¿‘çš„ä½å§¿
            min_diff = float('inf')
            closest_pose = None

            for pose_time, pose in self.pose_history:
                time_diff = abs((pose_time - target_timestamp).total_seconds())
                if time_diff < min_diff:
                    min_diff = time_diff
                    closest_pose = pose

            # å¦‚æœæ—¶é—´å·®å¤ªå¤§ï¼ˆ>1ç§’ï¼‰ï¼Œå¯èƒ½æœ‰é—®é¢˜
            if min_diff > 1.0:
                self.get_logger().warning(
                    f"âš ï¸  ä½å§¿æ—¶é—´æˆ³å¯¹é½å·®å¼‚è¾ƒå¤§: {min_diff:.2f}ç§’ "
                    f"(å›¾åƒ: {target_timestamp.strftime('%H:%M:%S.%f')[:-3]}, "
                    f"ä½å§¿: {closest_pose.get('x', 0):.2f}, {closest_pose.get('y', 0):.2f})"
                )

            return closest_pose or self.world_model.robot_position.copy()

    def _create_vlm_prompt(self) -> str:
        """åˆ›å»ºVLMæç¤ºè¯"""
        return (
            "Analyze this image and identify all visible objects. "
            "For each object, provide: 1) Object name (door, person, building, car, obstacle, etc.), "
            "2) Relative position (left, center, right, far, near), "
            "3) Size estimate. "
            "Format your response as a JSON object: "
            "{\"objects\": [{\"name\": \"door\", \"position\": \"left\", \"size\": \"large\"}]}"
        )

    def _update_semantic_objects(self, vlm_result: Dict[str, Any], robot_pose: Dict[str, float]):
        """
        å°†VLMç»“æœæ›´æ–°åˆ°WorldModelçš„è¯­ä¹‰ç‰©ä½“

        Args:
            vlm_result: VLMåˆ†æç»“æœ
            robot_pose: æœºå™¨äººä½å§¿ï¼ˆå›¾åƒæ—¶åˆ»ï¼‰
        """
        try:
            objects = vlm_result.get('objects', [])
            if not objects:
                return

            # è·å–æœºå™¨äººä½ç½®
            robot_x = robot_pose.get('x', 0.0)
            robot_y = robot_pose.get('y', 0.0)
            robot_yaw = robot_pose.get('yaw', 0.0)

            for obj in objects:
                obj_name = obj.get('name', 'unknown').lower()
                position_hint = obj.get('position', 'center')
                size_hint = obj.get('size', 'medium')

                # æ ¹æ®ä½ç½®æç¤ºè®¡ç®—ç›¸å¯¹åæ ‡
                offset_x, offset_y = self._calculate_position_offset(position_hint, size_hint)

                # è½¬æ¢ä¸ºä¸–ç•Œåæ ‡ï¼ˆè€ƒè™‘æœºå™¨äººæœå‘ï¼‰
                world_x = robot_x + offset_x * math.cos(robot_yaw) - offset_y * math.sin(robot_yaw)
                world_y = robot_y + offset_x * math.sin(robot_yaw) + offset_y * math.cos(robot_yaw)

                # åˆ›å»ºæˆ–æ›´æ–°è¯­ä¹‰ç‰©ä½“
                obj_id = f"vlm_{self.vlm_count}_{len(self.world_model.semantic_objects)}"

                # ä½¿ç”¨SemanticObjectç±»ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                from brain.cognitive.world_model.semantic.semantic_object import SemanticObject, ObjectState

                semantic_obj = SemanticObject(
                    id=obj_id,
                    label=obj_name,
                    world_position=(world_x, world_y),
                    confidence=0.7,  # VLMé»˜è®¤ç½®ä¿¡åº¦
                    state=ObjectState.DETECTED,
                    attributes={'source': 'vlm'}  # ğŸ”‘ æ ‡è®°ä¸ºVLMæ£€æµ‹ï¼Œç”¨äºå¯è§†åŒ–è¿‡æ»¤
                )

                # æ·»åŠ åˆ°WorldModel
                self.world_model.semantic_objects[obj_id] = semantic_obj

            self.get_logger().info(f"ğŸ“¦ æ›´æ–°äº† {len(objects)} ä¸ªè¯­ä¹‰ç‰©ä½“åˆ°WorldModel")

        except Exception as e:
            self.get_logger().error(f"æ›´æ–°è¯­ä¹‰ç‰©ä½“å¤±è´¥: {e}")

    def _calculate_position_offset(self, position_hint: str, size_hint: str) -> Tuple[float, float]:
        """
        æ ¹æ®ä½ç½®æç¤ºè®¡ç®—ç›¸å¯¹åç§»é‡

        Args:
            position_hint: ä½ç½®æç¤º (left, center, right, far, near)
            size_hint: å¤§å°æç¤º (small, medium, large)

        Returns:
            (offset_x, offset_y) ç›¸å¯¹æœºå™¨äººçš„åç§»ï¼ˆç±³ï¼‰
        """
        # åŸºç¡€è·ç¦»ï¼ˆæ ¹æ®å¤§å°ï¼‰
        size_distance = {
            'small': 1.0,
            'medium': 2.0,
            'large': 4.0
        }.get(size_hint.lower(), 2.0)

        # æ°´å¹³åç§»ï¼ˆæ ¹æ®å·¦å³ï¼‰
        horizontal_offset = {
            'left': -1.5,
            'center': 0.0,
            'right': 1.5
        }.get(position_hint.lower(), 0.0)

        # å‚ç›´åç§»ï¼ˆæ ¹æ®è¿œè¿‘ï¼‰
        vertical_offset = {
            'near': 0.5,
            'center': size_distance,
            'far': size_distance * 1.5
        }.get(position_hint.lower(), size_distance)

        return (horizontal_offset, vertical_offset)

    def _try_display_status(self):
        """å°è¯•æ˜¾ç¤ºçŠ¶æ€"""
        current_time = time.time()
        elapsed = current_time - self.start_time

        # å®šæœŸæ˜¾ç¤ºçŠ¶æ€
        if elapsed - self.last_display_time >= self.display_interval:
            self.last_display_time = current_time
            self._display_status()

    def _display_status(self):
        """æ˜¾ç¤ºå½“å‰çŠ¶æ€"""
        print("\n" + "=" * 80)
        print(f"ğŸ“Š è®¤çŸ¥ä¸–ç•ŒçŠ¶æ€å¯è§†åŒ– (è¿è¡Œæ—¶é•¿: {time.time() - self.start_time:.1f}ç§’)")
        print("=" * 80)

        # 1. æœºå™¨äººçŠ¶æ€
        robot_position = self.world_model.robot_position
        print(f"\nğŸ¤– æœºå™¨äººä½ç½®:")
        print(f"   x: {robot_position.get('x', 0):.3f} m")
        print(f"   y: {robot_position.get('y', 0):.3f} m")
        print(f"   z: {robot_position.get('z', 0):.3f} m")
        print(f"   yaw: {self.world_model.robot_heading * 180 / math.pi:.1f}Â°")

        # 2. å æ®æ …æ ¼
        print(f"\nğŸ—ºï¸  å æ®æ …æ ¼:")
        if self.world_model.current_map is not None:
            grid = self.world_model.current_map
            print(f"   å½¢çŠ¶: {grid.shape}")
            print(f"   åˆ†è¾¨ç‡: {self.world_model.map_resolution} m/cell")
            print(f"   æ€»å•å…ƒæ•°: {grid.size:,}")
            print(f"   æœªçŸ¥: {np.sum(grid == -1):,}")
            print(f"   ç©ºé—²: {np.sum(grid == 0):,}")
            print(f"   å æ®: {np.sum(grid == 100):,}")
        else:
            print(f"   (æ …æ ¼æœªåˆå§‹åŒ–)")

        # 3. è¯­ä¹‰ç‰©ä½“
        semantic_count = len(self.world_model.semantic_objects)
        print(f"\nğŸ“¦ è¯­ä¹‰ç‰©ä½“: {semantic_count}")

        if semantic_count > 0:
            print(f"   ç‰©ä½“åˆ—è¡¨:")
            for i, (obj_id, obj) in enumerate(list(self.world_model.semantic_objects.items())[:5]):
                print(f"   [{i+1}] {obj_id}: {obj.label}")
                if hasattr(obj, 'world_position') and obj.world_position:
                    wx, wy = obj.world_position
                    print(f"       ä½ç½®: ({wx:.2f}, {wy:.2f}), ç½®ä¿¡åº¦: {obj.confidence:.2f}")

        # 4. è·Ÿè¸ªç‰©ä½“
        tracked_count = len(self.world_model.tracked_objects)
        print(f"\nğŸ¯ è·Ÿè¸ªç‰©ä½“: {tracked_count}")

        # 5. æ¢ç´¢å‰æ²¿
        frontier_count = len(self.world_model.exploration_frontiers)
        print(f"\nğŸ” æ¢ç´¢å‰æ²¿: {frontier_count}")

        # 6. ä½å§¿å†å²
        pose_history_count = len(self.world_model.pose_history)
        print(f"\nğŸ“ ä½å§¿å†å²: {pose_history_count} ä¸ªè®°å½•")

        # 7. å› æœå›¾ç»Ÿè®¡ï¼ˆä¸‰æ¨¡æ€èåˆ - å› æœåœ°å›¾ï¼‰
        if hasattr(self.world_model, 'causal_graph'):
            stats = self.world_model.get_causal_graph_statistics()
            print(f"\nğŸ”— å› æœå›¾ç»Ÿè®¡:")
            print(f"   èŠ‚ç‚¹æ•°: {stats['num_nodes']}")
            print(f"   è¾¹æ•°: {stats['num_edges']}")
            print(f"   é«˜ç½®ä¿¡åº¦è¾¹ (>0.7): {stats['high_confidence_edges']}")
            print(f"   å¹³å‡ç½®ä¿¡åº¦: {stats['avg_confidence']:.2f}")
            if stats['num_edges'] == 0:
                print(f"   ğŸ’¡ æç¤º: ç§»åŠ¨æœºå™¨äººæˆ–ç‰©ä½“ä¼šè§¦å‘å› æœæ£€æµ‹")

        # 8. ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   é‡Œç¨‹è®¡æ›´æ–°: {self.odom_count} æ¬¡")
        print(f"   ç‚¹äº‘æ›´æ–°: {self.pointcloud_count} æ¬¡")
        print(f"   RGBå›¾åƒæ¥æ”¶: {self.rgb_count} æ¬¡")
        print(f"   VLMåˆ†æ: {self.vlm_count} æ¬¡")
        print(f"   æ›´æ–°é¢‘ç‡: {self.odom_count / (time.time() - self.start_time):.2f} Hz")

        # VLMç»Ÿè®¡
        vlm_stats = self.vlm_client.get_statistics()
        if vlm_stats['total_requests'] > 0:
            print(f"\nğŸ¤– VLMç»Ÿè®¡:")
            print(f"   æ€»è¯·æ±‚: {vlm_stats['total_requests']}")
            print(f"   æˆåŠŸ: {vlm_stats['successful_requests']}")
            print(f"   å¤±è´¥: {vlm_stats['failed_requests']}")
            print(f"   æˆåŠŸç‡: {vlm_stats['success_rate']:.1%}")

        print("\n" + "=" * 80)
        print("ğŸ’¡ RViz2ä¸­åº”è¯¥çœ‹åˆ°:")
        print("   - ç°è‰²/ç™½è‰²/é»‘è‰²åœ°å›¾ (æœªçŸ¥/ç©ºé—²/å æ®)")
        print("   - å½©è‰²ç‰©ä½“æ ‡ç­¾ (é—¨=è“è‰², äºº=çº¢è‰², å»ºç­‘=ç»¿è‰²)")
        print("   - ç»¿è‰²æœºå™¨äººè½¨è¿¹çº¿")
        print("   - ç®­å¤´+æ–‡å­— (æ¢ç´¢è¾¹ç•Œ, ä¼˜å…ˆçº§+è·ç¦»)")
        print("=" * 80 + "\n")


def main(args=None):
    """ä¸»å‡½æ•°"""
    rclpy.init(args=args)

    # åˆ›å»ºå¯è§†åŒ–èŠ‚ç‚¹
    viz_node = CognitiveWorldStateViz(duration_seconds=600.0)

    # ä½¿ç”¨å¤šçº¿ç¨‹æ‰§è¡Œå™¨
    executor = MultiThreadedExecutor()
    executor.add_node(viz_node)
    executor.add_node(viz_node.visualizer)  # æ·»åŠ å¯è§†åŒ–å™¨èŠ‚ç‚¹åˆ°æ‰§è¡Œå™¨

    print("\n" + "=" * 80)
    print("ğŸš€ è®¤çŸ¥å±‚ä¸–ç•ŒçŠ¶æ€å¯è§†åŒ–æ¼”ç¤º")
    print("=" * 80)
    print("\nğŸ“¡ æ­£åœ¨è®¢é˜…ROS2è¯é¢˜:")
    print("   /chassis/odom")
    print("   /front_3d_lidar/lidar_points")
    print("   /front_stereo_camera/left/image_raw")
    print("\nğŸ“¤ æ­£åœ¨å‘å¸ƒå¯è§†åŒ–è¯é¢˜:")
    print("   /world_model/semantic_grid")
    print("   /world_model/semantic_markers")
    print("   /world_model/belief_markers")
    print("   /world_model/trajectory")
    print("   /world_model/frontiers")
    print("   /world_model/change_events")
    print("   /vlm/detections")
    print("\nğŸ’¡ åœ¨å¦ä¸€ä¸ªç»ˆç«¯å¯åŠ¨RViz2æŸ¥çœ‹å¯è§†åŒ–:")
    print("   rviz2 -d config/rviz2/cognitive_world_model_correct.rviz")
    print("\nâ±ï¸  æ¼”ç¤ºæ—¶é•¿: 600ç§’ (10åˆ†é’Ÿ)")
    print("ğŸ“Š çŠ¶æ€æ˜¾ç¤ºé—´éš”: 5ç§’")
    print("\nğŸ’¡ æç¤º: å¦‚æœæ²¡æœ‰å®æ—¶ä¼ æ„Ÿå™¨ï¼Œå¯ä»¥å…ˆæ’­æ”¾rosbag:")
    print("   export ROS_DOMAIN_ID=42")
    print("   ros2 bag play <rosbagæ–‡ä»¶> --loop")
    print("\næŒ‰Ctrl+Cåœæ­¢\n")
    print("=" * 80 + "\n")

    try:
        executor.spin()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    finally:
        # æ¸…ç†VLMçº¿ç¨‹æ± 
        print("\nğŸ§¹ æ¸…ç†èµ„æº...")
        if hasattr(viz_node, 'vlm_executor'):
            viz_node.vlm_executor.shutdown(wait=True)
            print("   âœ… VLMçº¿ç¨‹æ± å·²å…³é—­")

        # ä¿å­˜æœ€ç»ˆçŠ¶æ€
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        print(f"\nâœ… å·²åœæ­¢")
        print(f"   æ€»é‡Œç¨‹è®¡æ›´æ–°: {viz_node.odom_count}æ¬¡")
        print(f"   æ€»ç‚¹äº‘æ›´æ–°: {viz_node.pointcloud_count}æ¬¡")
        print(f"   VLMæ£€æµ‹æ¬¡æ•°: {viz_node.vlm_count}æ¬¡")

        viz_node.destroy_node()

        # ä¼˜é›…åœ°å…³é—­ROS2
        try:
            rclpy.shutdown()
        except (AttributeError, Exception):
            pass


if __name__ == '__main__':
    main()
