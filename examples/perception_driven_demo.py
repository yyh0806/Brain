#!/usr/bin/env python3
"""
æ„ŸçŸ¥é©±åŠ¨æ™ºèƒ½è§„åˆ’ç³»ç»Ÿæ¼”ç¤º

æœ¬ç¤ºä¾‹æ¼”ç¤ºï¼š
1. æ„ŸçŸ¥é©±åŠ¨çš„ä»»åŠ¡è§„åˆ’ - ç»“åˆå®æ—¶ç¯å¢ƒæ•°æ®ç”Ÿæˆè®¡åˆ’
2. CoTæ¨ç† - å¯è¿½æº¯çš„å†³ç­–é“¾
3. å¤šè½®å¯¹è¯ - æŒ‡ä»¤æ¾„æ¸…ã€æ‰§è¡Œç¡®è®¤ã€è¿›åº¦æ±‡æŠ¥
4. å“åº”å¼é‡è§„åˆ’ - æ„ŸçŸ¥å˜åŒ–è‡ªåŠ¨è§¦å‘è®¡åˆ’è°ƒæ•´

è¿è¡Œæ–¹å¼ï¼š
    cd /media/yangyuhui/CODES1/Brain
    python3 examples/perception_driven_demo.py
"""

import asyncio
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from loguru import logger


# ============================================================
# æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’å›è°ƒ
# ============================================================

class MockUserInterface:
    """æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’ç•Œé¢"""
    
    def __init__(self, auto_responses: dict = None):
        self.auto_responses = auto_responses or {}
        self.interaction_log = []
    
    async def user_callback(self, prompt: str, options: list) -> str:
        """æ¨¡æ‹Ÿç”¨æˆ·å“åº”"""
        self.interaction_log.append({
            "timestamp": datetime.now().isoformat(),
            "prompt": prompt,
            "options": options
        })
        
        print(f"\n{'='*60}")
        print(f"ğŸ¤– ç³»ç»Ÿ: {prompt}")
        if options:
            print(f"   é€‰é¡¹: {options}")
        
        # è‡ªåŠ¨å“åº”é€»è¾‘
        for keyword, response in self.auto_responses.items():
            if keyword in prompt:
                print(f"ğŸ‘¤ ç”¨æˆ·(è‡ªåŠ¨): {response}")
                return response
        
        # é»˜è®¤å“åº”
        if options:
            response = options[0]
        elif "ç¡®è®¤" in prompt or "ç»§ç»­" in prompt:
            response = "ç¡®è®¤"
        else:
            response = "ç»§ç»­"
        
        print(f"ğŸ‘¤ ç”¨æˆ·(è‡ªåŠ¨): {response}")
        return response


# ============================================================
# æ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•°æ®ç”Ÿæˆå™¨
# ============================================================

class MockSensorDataGenerator:
    """æ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.tick = 0
        self.scenario = "normal"
    
    def set_scenario(self, scenario: str):
        """è®¾ç½®åœºæ™¯ï¼šnormal, obstacle_appear, target_move, low_battery"""
        self.scenario = scenario
        logger.info(f"ä¼ æ„Ÿå™¨åœºæ™¯åˆ‡æ¢ä¸º: {scenario}")
    
    def generate(self) -> dict:
        """ç”Ÿæˆä¼ æ„Ÿå™¨æ•°æ®"""
        self.tick += 1
        
        base_data = {
            "gps": {
                "data": {
                    "latitude": 39.9042 + self.tick * 0.0001,
                    "longitude": 116.4074,
                    "altitude": 50 + self.tick
                }
            },
            "imu": {
                "data": {
                    "orientation": {"yaw": 45, "pitch": 0, "roll": 0}
                }
            },
            "battery": 100 - self.tick * 2,
            "detections": []
        }
        
        # æ ¹æ®åœºæ™¯æ·»åŠ ä¸åŒçš„æ£€æµ‹ç»“æœ
        if self.scenario == "obstacle_appear" and self.tick > 2:
            base_data["detections"].append({
                "id": "obstacle_1",
                "type": "obstacle",
                "x": 30, "y": 10, "z": 50,
                "confidence": 0.95
            })
        
        if self.scenario == "target_move" and self.tick > 3:
            base_data["detections"].append({
                "id": "target_1",
                "type": "person",
                "x": 50 + self.tick * 2, "y": 20, "z": 0,
                "confidence": 0.88,
                "is_target": True
            })
        
        if self.scenario == "low_battery":
            base_data["battery"] = max(15, 30 - self.tick * 3)
        
        return base_data


# ============================================================
# æ¼”ç¤ºåœºæ™¯
# ============================================================

async def demo_basic_perception_planning():
    """æ¼”ç¤º1: åŸºç¡€æ„ŸçŸ¥é©±åŠ¨è§„åˆ’"""
    print("\n" + "="*70)
    print("ğŸ“Œ æ¼”ç¤º1: åŸºç¡€æ„ŸçŸ¥é©±åŠ¨è§„åˆ’")
    print("="*70)
    
    from brain.cognitive.world_model import WorldModel
    from brain.cognitive.reasoning.cot_engine import CoTEngine
    from brain.cognitive.reasoning.reasoning_result import ReasoningMode
    from brain.cognitive.dialogue.dialogue_manager import DialogueManager
    
    # åˆ›å»ºç»„ä»¶
    world_model = WorldModel()
    cot_engine = CoTEngine()
    
    user_interface = MockUserInterface({
        "å“ªä¸ª": "ä¸œè¾¹çš„å»ºç­‘"
    })
    dialogue = DialogueManager(user_callback=user_interface.user_callback)
    dialogue.start_session("demo_basic")
    
    # æ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•°æ®
    sensor_data = {
        "gps": {"data": {"latitude": 39.9, "longitude": 116.4, "altitude": 50}},
        "imu": {"data": {"orientation": {"yaw": 45}}},
        "battery": 85,
        "detections": [
            {"id": "building_1", "type": "building", "x": 100, "y": 50, "z": 0, "confidence": 0.9},
            {"id": "tree_1", "type": "tree", "x": 30, "y": 10, "z": 0, "confidence": 0.85, "is_obstacle": True}
        ]
    }
    
    # æ›´æ–°ä¸–ç•Œæ¨¡å‹
    changes = world_model.update_from_perception(sensor_data)
    print(f"\nğŸ“¡ æ„ŸçŸ¥æ›´æ–°: æ£€æµ‹åˆ° {len(changes)} ä¸ªå˜åŒ–")
    
    # è·å–è§„åˆ’ä¸Šä¸‹æ–‡
    context = world_model.get_context_for_planning()
    print(f"\nğŸŒ è§„åˆ’ä¸Šä¸‹æ–‡:")
    print(context.to_prompt_context())
    
    # æ¨¡æ‹Ÿæ¨¡ç³ŠæŒ‡ä»¤
    command = "å»é‚£è¾¹çœ‹çœ‹"
    print(f"\nğŸ’¬ ç”¨æˆ·æŒ‡ä»¤: {command}")
    
    # æ£€æµ‹åˆ°æ¨¡ç³Šï¼Œè¯·æ±‚æ¾„æ¸…
    ambiguities = [{"aspect": "ä½ç½®", "question": "è¯·é—®æ˜¯å“ªä¸ªæ–¹å‘ï¼Ÿ", "options": ["ä¸œè¾¹çš„å»ºç­‘", "è¥¿è¾¹çš„æ ‘"]}]
    clarification = await dialogue.clarify_ambiguous_command(command, ambiguities, context.to_prompt_context())
    print(f"\nâœ… æ¾„æ¸…ç»“æœ: {clarification['clarified_command']}")
    
    # CoTæ¨ç†
    print(f"\nğŸ§  å¼€å§‹CoTæ¨ç†...")
    reasoning = await cot_engine.reason(
        query=f"è§„åˆ’ä»»åŠ¡: {clarification['clarified_command']}",
        context={
            "obstacles": len(context.obstacles),
            "targets": len(context.targets),
            "battery_level": context.battery_level,
            "constraints": context.constraints
        },
        mode=ReasoningMode.PLANNING
    )
    
    print(f"\nğŸ“‹ æ¨ç†ç»“æœ:")
    print(f"   å¤æ‚åº¦: {reasoning.complexity.value}")
    print(f"   ç½®ä¿¡åº¦: {reasoning.confidence:.2f}")
    print(f"   å†³ç­–: {reasoning.decision}")
    print(f"   å»ºè®®: {reasoning.suggestion}")
    print(f"\n   æ¨ç†é“¾æ‘˜è¦:")
    print(reasoning.get_chain_summary())
    
    dialogue.end_session()
    print("\nâœ… æ¼”ç¤º1å®Œæˆ")


async def demo_perception_driven_replan():
    """æ¼”ç¤º2: æ„ŸçŸ¥é©±åŠ¨çš„é‡è§„åˆ’"""
    print("\n" + "="*70)
    print("ğŸ“Œ æ¼”ç¤º2: æ„ŸçŸ¥é©±åŠ¨çš„é‡è§„åˆ’")
    print("="*70)
    
    from brain.cognitive.world_model import WorldModel, ChangeType
    from brain.cognitive.cot_engine import CoTEngine, ReasoningMode
    from brain.cognitive.monitoring.perception_monitor import PerceptionMonitor
    from brain.cognitive.dialogue_manager import DialogueManager
    
    # åˆ›å»ºç»„ä»¶
    world_model = WorldModel()
    cot_engine = CoTEngine()
    
    user_interface = MockUserInterface({
        "ç¡®è®¤": "ç¡®è®¤æ‰§è¡Œ",
        "éšœç¢": "ç¡®è®¤ç»•è¡Œ"
    })
    dialogue = DialogueManager(user_callback=user_interface.user_callback)
    dialogue.start_session("demo_replan")
    
    perception_monitor = PerceptionMonitor(world_model)
    sensor_gen = MockSensorDataGenerator()
    
    # è®¾ç½®å›è°ƒ
    replan_events = []
    
    async def on_replan(event):
        replan_events.append(event)
        print(f"\nâš ï¸ è§¦å‘é‡è§„åˆ’: {event.change.description}")
    
    async def on_confirm(event) -> bool:
        result = await dialogue.request_confirmation(
            action=f"å¤„ç†å˜åŒ–: {event.change.description}",
            reason=event.trigger.description,
            details=event.change.data
        )
        return result
    
    perception_monitor.set_replan_callback(on_replan)
    perception_monitor.set_confirmation_callback(on_confirm)
    
    print("\nğŸš€ å¼€å§‹ä»»åŠ¡æ‰§è¡Œæ¨¡æ‹Ÿ...")
    
    # æ¨¡æ‹Ÿæ‰§è¡Œå¾ªç¯
    for step in range(5):
        print(f"\n--- æ‰§è¡Œæ­¥éª¤ {step + 1} ---")
        
        # ç¬¬3æ­¥å‡ºç°éšœç¢ç‰©
        if step == 2:
            sensor_gen.set_scenario("obstacle_appear")
        
        # è·å–ä¼ æ„Ÿå™¨æ•°æ®
        sensor_data = sensor_gen.generate()
        print(f"   ç”µæ± : {sensor_data['battery']}%")
        print(f"   æ£€æµ‹åˆ°ç‰©ä½“: {len(sensor_data['detections'])}ä¸ª")
        
        # å¤„ç†ä¼ æ„Ÿå™¨æ›´æ–°
        events = await perception_monitor.process_sensor_update(sensor_data)
        
        if events:
            for event in events:
                print(f"   ğŸ“¢ äº‹ä»¶: {event.change.description} (åŠ¨ä½œ: {event.action.value})")
                
                # å¦‚æœéœ€è¦é‡è§„åˆ’
                if event.action.value in ["replan", "confirm_replan"]:
                    # ä½¿ç”¨CoTè¯„ä¼°
                    reasoning = await cot_engine.reason(
                        query="ç¯å¢ƒå˜åŒ–ï¼Œæ˜¯å¦éœ€è¦è°ƒæ•´è®¡åˆ’ï¼Ÿ",
                        context={
                            "changes": event.change.description,
                            "change_data": event.change.data
                        },
                        mode=ReasoningMode.REPLANNING
                    )
                    
                    print(f"\n   ğŸ§  CoTè¯„ä¼°:")
                    print(f"      å†³ç­–: {reasoning.decision}")
                    print(f"      ç½®ä¿¡åº¦: {reasoning.confidence:.2f}")
        
        await asyncio.sleep(0.1)
    
    dialogue.end_session()
    
    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"   è§¦å‘çš„é‡è§„åˆ’äº‹ä»¶: {len(replan_events)}")
    print(f"   æ„ŸçŸ¥ç›‘æ§çŠ¶æ€: {perception_monitor.get_status()}")
    
    print("\nâœ… æ¼”ç¤º2å®Œæˆ")


async def demo_multi_turn_dialogue():
    """æ¼”ç¤º3: å¤šè½®å¯¹è¯äº¤äº’"""
    print("\n" + "="*70)
    print("ğŸ“Œ æ¼”ç¤º3: å¤šè½®å¯¹è¯äº¤äº’")
    print("="*70)
    
    from brain.cognitive.dialogue_manager import DialogueManager
    from brain.cognitive.world_model import WorldModel
    
    # åˆ›å»ºç»„ä»¶
    world_model = WorldModel()
    
    # è®¾ç½®è‡ªåŠ¨å“åº”
    auto_responses = {
        "ä¸œè¾¹": "ä¸œè¾¹çš„Aç‚¹",
        "ç¡®è®¤": "ç¡®è®¤",
        "æš‚åœ": "ç»§ç»­",
        "å‰æ–¹å‡ºç°": "ç¡®è®¤ç»•è¡Œ",
        "å®Œæˆ": "å¥½çš„"
    }
    
    user_interface = MockUserInterface(auto_responses)
    dialogue = DialogueManager(user_callback=user_interface.user_callback)
    dialogue.start_session("demo_dialogue")
    
    print("\nğŸ’¬ å¼€å§‹å¤šè½®å¯¹è¯æ¼”ç¤º...")
    
    # åœºæ™¯1: æŒ‡ä»¤æ¾„æ¸…
    print("\n--- åœºæ™¯1: æŒ‡ä»¤æ¾„æ¸… ---")
    clarification = await dialogue.clarify_ambiguous_command(
        command="å»æ‹ç…§",
        ambiguities=[{
            "aspect": "ä½ç½®",
            "question": "å»å“ªé‡Œæ‹ç…§ï¼Ÿ",
            "options": ["ä¸œè¾¹çš„Aç‚¹", "è¥¿è¾¹çš„Bç‚¹", "å½“å‰ä½ç½®"]
        }],
        world_context="å½“å‰ä½ç½®(0,0,50)ï¼Œä¸œè¾¹100ç±³æœ‰Aç‚¹ï¼Œè¥¿è¾¹80ç±³æœ‰Bç‚¹"
    )
    print(f"   æ¾„æ¸…ç»“æœ: {clarification}")
    
    # åœºæ™¯2: æ‰§è¡Œç¡®è®¤
    print("\n--- åœºæ™¯2: æ‰§è¡Œç¡®è®¤ ---")
    confirmed = await dialogue.request_confirmation(
        action="èµ·é£å¹¶é£å‘Aç‚¹",
        reason="è¿™æ˜¯ä»»åŠ¡çš„ç¬¬ä¸€æ­¥",
        details={"ç›®æ ‡é«˜åº¦": "100ç±³", "é¢„è®¡æ—¶é—´": "30ç§’"}
    )
    print(f"   ç¡®è®¤ç»“æœ: {confirmed}")
    
    # åœºæ™¯3: è¿›åº¦æ±‡æŠ¥
    print("\n--- åœºæ™¯3: è¿›åº¦æ±‡æŠ¥ ---")
    adjustment = await dialogue.report_progress(
        status="é£è¡Œä¸­",
        progress_percent=50,
        current_operation="goto",
        world_state_summary="é«˜åº¦80ç±³ï¼Œè·ç¦»ç›®æ ‡50ç±³",
        allow_adjustment=True
    )
    print(f"   ç”¨æˆ·è°ƒæ•´: {adjustment}")
    
    # åœºæ™¯4: é”™è¯¯æŠ¥å‘Š
    print("\n--- åœºæ™¯4: é”™è¯¯æŠ¥å‘Š ---")
    choice = await dialogue.report_error(
        error="å‰æ–¹æ£€æµ‹åˆ°ç§»åŠ¨ç‰©ä½“",
        operation="goto",
        suggestions=["ç»•è¡Œ", "æ‚¬åœç­‰å¾…", "ä¸­æ­¢ä»»åŠ¡"],
        allow_choice=True
    )
    print(f"   ç”¨æˆ·é€‰æ‹©: {choice}")
    
    # åœºæ™¯5: ä¿¡æ¯é€šçŸ¥
    print("\n--- åœºæ™¯5: ä¿¡æ¯é€šçŸ¥ ---")
    await dialogue.send_information("ä»»åŠ¡å®Œæˆï¼å…±æ‹æ‘„5å¼ ç…§ç‰‡ã€‚")
    
    # æ˜¾ç¤ºå¯¹è¯å†å²
    print("\nğŸ“œ å¯¹è¯å†å²:")
    for i, msg in enumerate(dialogue.get_conversation_history()):
        print(f"   {i+1}. [{msg['role']}] {msg['content'][:50]}...")
    
    dialogue.end_session()
    print("\nâœ… æ¼”ç¤º3å®Œæˆ")


async def demo_cot_reasoning():
    """æ¼”ç¤º4: CoTé“¾å¼æ€ç»´æ¨ç†"""
    print("\n" + "="*70)
    print("ğŸ“Œ æ¼”ç¤º4: CoTé“¾å¼æ€ç»´æ¨ç†")
    print("="*70)
    
    from brain.cognitive.reasoning.cot_engine import CoTEngine
    from brain.cognitive.reasoning.reasoning_result import ReasoningMode, ComplexityLevel
    
    cot_engine = CoTEngine()
    
    # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„ä»»åŠ¡
    test_cases = [
        {
            "query": "å‘å‰é£10ç±³",
            "context": {"obstacles": 0, "battery_level": 90},
            "expected_complexity": ComplexityLevel.SIMPLE
        },
        {
            "query": "å»ä¸œè¾¹æ‹ç…§ï¼Œæ³¨æ„é¿å¼€éšœç¢ç‰©",
            "context": {"obstacles": 3, "battery_level": 60, "constraints": ["é¿å¼€ç¦é£åŒº"]},
            "expected_complexity": ComplexityLevel.MODERATE
        },
        {
            "query": "æœç´¢åŒºåŸŸå†…çš„å¯ç–‘ç›®æ ‡ï¼Œæ‹ç…§è®°å½•ï¼Œå®æ—¶æ±‡æŠ¥",
            "context": {
                "obstacles": 5, 
                "targets": 2, 
                "battery_level": 40,
                "constraints": ["é¿å¼€ç¦é£åŒº", "ä¿æŒé€šä¿¡"],
                "recent_changes": ["æ–°å‘ç°ç›®æ ‡", "å¤©æ°”å˜åŒ–"]
            },
            "expected_complexity": ComplexityLevel.COMPLEX
        }
    ]
    
    for i, case in enumerate(test_cases):
        print(f"\n--- æµ‹è¯•æ¡ˆä¾‹ {i+1} ---")
        print(f"   ä»»åŠ¡: {case['query']}")
        
        # è¯„ä¼°å¤æ‚åº¦
        complexity = cot_engine.assess_complexity(case['query'], case['context'])
        print(f"   è¯„ä¼°å¤æ‚åº¦: {complexity.value} (é¢„æœŸ: {case['expected_complexity'].value})")
        
        # æ‰§è¡Œæ¨ç†
        result = await cot_engine.reason(
            query=case['query'],
            context=case['context'],
            mode=ReasoningMode.PLANNING
        )
        
        print(f"   å®é™…å¤æ‚åº¦: {result.complexity.value}")
        print(f"   ç½®ä¿¡åº¦: {result.confidence:.2f}")
        print(f"   æ¨ç†æ­¥éª¤æ•°: {len(result.chain)}")
        print(f"   å†³ç­–: {result.decision[:100]}...")
        print(f"   å»ºè®®: {result.suggestion[:100]}...")
    
    print("\nâœ… æ¼”ç¤º4å®Œæˆ")


async def demo_full_integration():
    """æ¼”ç¤º5: å®Œæ•´é›†æˆæ¼”ç¤º"""
    print("\n" + "="*70)
    print("ğŸ“Œ æ¼”ç¤º5: å®Œæ•´é›†æˆæ¼”ç¤º - æ„ŸçŸ¥é©±åŠ¨çš„æ™ºèƒ½ä»»åŠ¡æ‰§è¡Œ")
    print("="*70)
    
    from brain.cognitive.world_model import WorldModel
    from brain.cognitive.reasoning.cot_engine import CoTEngine
    from brain.cognitive.reasoning.reasoning_result import ReasoningMode
    from brain.cognitive.dialogue.dialogue_manager import DialogueManager
    from brain.cognitive.monitoring.perception_monitor import PerceptionMonitor
    from brain.llm.cot_prompts import CoTPrompts
    
    # åˆ›å»ºæ‰€æœ‰ç»„ä»¶
    world_model = WorldModel()
    cot_engine = CoTEngine()
    cot_prompts = CoTPrompts()
    perception_monitor = PerceptionMonitor(world_model)
    
    user_interface = MockUserInterface({
        "ç¡®è®¤": "ç¡®è®¤",
        "ä¸œè¾¹": "ä¸œè¾¹50ç±³å¤„çš„ç›®æ ‡ç‚¹",
        "éšœç¢": "ç¡®è®¤ç»•è¡Œ",
        "ç»§ç»­": "ç»§ç»­æ‰§è¡Œ"
    })
    dialogue = DialogueManager(user_callback=user_interface.user_callback)
    
    sensor_gen = MockSensorDataGenerator()
    
    # å¼€å§‹å¯¹è¯ä¼šè¯
    dialogue.start_session("integration_demo")
    
    print("\nğŸ¯ æ¨¡æ‹Ÿå®Œæ•´ä»»åŠ¡æµç¨‹:")
    print("   1. ç”¨æˆ·ä¸‹è¾¾æŒ‡ä»¤")
    print("   2. ç³»ç»Ÿè¯·æ±‚æ¾„æ¸…")
    print("   3. CoTæ¨ç†ç”Ÿæˆè®¡åˆ’")
    print("   4. æ‰§è¡Œè¿‡ç¨‹ä¸­æ„ŸçŸ¥å˜åŒ–")
    print("   5. è‡ªåŠ¨è§¦å‘é‡è§„åˆ’")
    print("   6. å¤šè½®å¯¹è¯ç¡®è®¤")
    print("   7. ä»»åŠ¡å®Œæˆæ±‡æŠ¥")
    
    # Step 1: ç”¨æˆ·æŒ‡ä»¤
    user_command = "å»é‚£è¾¹çš„ç›®æ ‡ç‚¹æ‰§è¡Œä¾¦å¯Ÿä»»åŠ¡"
    print(f"\nğŸ“ Step 1 - ç”¨æˆ·æŒ‡ä»¤: {user_command}")
    
    # Step 2: è·å–æ„ŸçŸ¥æ•°æ®
    print(f"\nğŸ“¡ Step 2 - è·å–æ„ŸçŸ¥æ•°æ®...")
    sensor_data = sensor_gen.generate()
    sensor_data["detections"] = [
        {"id": "target_1", "type": "poi", "x": 50, "y": 0, "z": 0, "is_target": True, "confidence": 0.9},
        {"id": "obstacle_1", "type": "tree", "x": 25, "y": 5, "z": 0, "is_obstacle": True, "confidence": 0.85}
    ]
    
    world_model.update_from_perception(sensor_data)
    planning_context = world_model.get_context_for_planning()
    print(f"   æ„ŸçŸ¥çŠ¶æ€: {world_model.get_summary()}")
    
    # Step 3: æŒ‡ä»¤æ¾„æ¸…
    print(f"\nğŸ’¬ Step 3 - æŒ‡ä»¤æ¾„æ¸…...")
    clarification = await dialogue.clarify_ambiguous_command(
        command=user_command,
        ambiguities=[{
            "aspect": "ä½ç½®",
            "question": "è¯·ç¡®è®¤ç›®æ ‡ä½ç½®",
            "options": ["ä¸œè¾¹50ç±³å¤„çš„ç›®æ ‡ç‚¹", "åŒ—è¾¹30ç±³å¤„çš„å…´è¶£ç‚¹"]
        }],
        world_context=planning_context.to_prompt_context()
    )
    clarified_command = clarification.get("clarified_command", user_command)
    
    # Step 4: CoTè§„åˆ’
    print(f"\nğŸ§  Step 4 - CoTè§„åˆ’...")
    planning_prompt = cot_prompts.build_planning_prompt(
        task_description=clarified_command,
        perception_context=planning_context.to_prompt_context(),
        available_operations="takeoff, goto, hover, scan_area, capture_image, return_to_home, land"
    )
    
    planning_reasoning = await cot_engine.reason(
        query=f"è§„åˆ’ä»»åŠ¡: {clarified_command}",
        context={
            "perception": planning_context.to_prompt_context(),
            "obstacles": len(planning_context.obstacles),
            "targets": len(planning_context.targets),
            "battery_level": planning_context.battery_level
        },
        mode=ReasoningMode.PLANNING
    )
    
    print(f"   è§„åˆ’æ¨ç†å®Œæˆ:")
    print(f"   - å¤æ‚åº¦: {planning_reasoning.complexity.value}")
    print(f"   - ç½®ä¿¡åº¦: {planning_reasoning.confidence:.2f}")
    print(f"   - å†³ç­–: {planning_reasoning.decision}")
    
    # Step 5: è¯·æ±‚æ‰§è¡Œç¡®è®¤
    print(f"\nâœ… Step 5 - è¯·æ±‚ç¡®è®¤...")
    confirmed = await dialogue.request_confirmation(
        action="æ‰§è¡Œè§„åˆ’çš„ä»»åŠ¡",
        reason=f"åŸºäºCoTæ¨ç†ï¼Œç½®ä¿¡åº¦{planning_reasoning.confidence:.0%}",
        details={"æ“ä½œæ•°": "5", "é¢„è®¡æ—¶é—´": "120ç§’"}
    )
    
    if confirmed:
        print("\nğŸš€ Step 6 - å¼€å§‹æ‰§è¡Œ...")
        
        # æ¨¡æ‹Ÿæ‰§è¡Œè¿‡ç¨‹
        operations = ["takeoff", "goto", "hover", "scan_area", "capture_image"]
        
        for i, op in enumerate(operations):
            progress = (i + 1) / len(operations) * 100
            
            # æ›´æ–°æ„ŸçŸ¥
            if i == 2:
                # æ¨¡æ‹Ÿæ‰§è¡Œä¸­å‡ºç°æ–°éšœç¢
                sensor_gen.set_scenario("obstacle_appear")
            
            sensor_data = sensor_gen.generate()
            changes = world_model.update_from_perception(sensor_data)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜¾è‘—å˜åŒ–
            significant_changes = world_model.detect_significant_changes()
            
            if significant_changes:
                print(f"\n   âš ï¸ æ£€æµ‹åˆ°ç¯å¢ƒå˜åŒ–!")
                for change in significant_changes:
                    print(f"      - {change.description}")
                
                # CoTè¯„ä¼°æ˜¯å¦éœ€è¦é‡è§„åˆ’
                replan_reasoning = await cot_engine.reason(
                    query="ç¯å¢ƒå˜åŒ–ï¼Œæ˜¯å¦éœ€è¦è°ƒæ•´è®¡åˆ’ï¼Ÿ",
                    context={
                        "changes": [c.description for c in significant_changes],
                        "current_operation": op,
                        "progress": f"{progress:.0f}%"
                    },
                    mode=ReasoningMode.REPLANNING
                )
                
                print(f"   ğŸ§  é‡è§„åˆ’è¯„ä¼°: {replan_reasoning.decision}")
                
                if "replan" in replan_reasoning.decision.lower() or "è°ƒæ•´" in replan_reasoning.decision:
                    await dialogue.report_and_confirm(
                        message=f"æ£€æµ‹åˆ°ç¯å¢ƒå˜åŒ–ï¼Œå»ºè®®è°ƒæ•´è®¡åˆ’",
                        suggestion=replan_reasoning.suggestion
                    )
            
            # æ±‡æŠ¥è¿›åº¦
            if i % 2 == 0:
                await dialogue.report_progress(
                    status=f"æ‰§è¡Œä¸­ - {op}",
                    progress_percent=progress,
                    current_operation=op,
                    world_state_summary=f"ç”µæ± {sensor_data['battery']}%",
                    allow_adjustment=False
                )
            
            print(f"   âœ“ å®Œæˆæ“ä½œ: {op} ({progress:.0f}%)")
            await asyncio.sleep(0.1)
        
        # ä»»åŠ¡å®Œæˆ
        print(f"\nğŸ‰ Step 7 - ä»»åŠ¡å®Œæˆ!")
        await dialogue.send_information(
            "âœ… ä¾¦å¯Ÿä»»åŠ¡å®Œæˆï¼\n"
            "- æ‹æ‘„ç…§ç‰‡: 3å¼ \n"
            "- æ‰«æé¢ç§¯: 100å¹³æ–¹ç±³\n"
            "- å‘ç°ç›®æ ‡: 1ä¸ª\n"
            "- é‡è§„åˆ’æ¬¡æ•°: 1æ¬¡"
        )
    
    # æ˜¾ç¤ºç»Ÿè®¡
    print(f"\nğŸ“Š ä»»åŠ¡ç»Ÿè®¡:")
    print(f"   å¯¹è¯è½®æ¬¡: {len(dialogue.get_conversation_history())}")
    print(f"   æ¨ç†æ¬¡æ•°: {len(cot_engine.reasoning_history)}")
    print(f"   ä¸–ç•Œæ¨¡å‹å˜åŒ–: {len(world_model.change_history)}")
    
    dialogue.end_session()
    print("\nâœ… æ¼”ç¤º5å®Œæˆ")


async def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("ğŸ§  æ„ŸçŸ¥é©±åŠ¨æ™ºèƒ½è§„åˆ’ç³»ç»Ÿ - åŠŸèƒ½æ¼”ç¤º")
    print("="*70)
    print("""
æœ¬æ¼”ç¤ºå±•ç¤ºç³»ç»Ÿçš„æ ¸å¿ƒèƒ½åŠ›ï¼š
1. åŸºç¡€æ„ŸçŸ¥é©±åŠ¨è§„åˆ’ - ç»“åˆç¯å¢ƒæ•°æ®çš„æ™ºèƒ½è§„åˆ’
2. æ„ŸçŸ¥é©±åŠ¨é‡è§„åˆ’ - ç¯å¢ƒå˜åŒ–è‡ªåŠ¨è§¦å‘è®¡åˆ’è°ƒæ•´  
3. å¤šè½®å¯¹è¯äº¤äº’ - æ¾„æ¸…/ç¡®è®¤/æ±‡æŠ¥
4. CoTé“¾å¼æ€ç»´æ¨ç† - å¯è¿½æº¯çš„å†³ç­–è¿‡ç¨‹
5. å®Œæ•´é›†æˆæ¼”ç¤º - ç«¯åˆ°ç«¯ä»»åŠ¡æ‰§è¡Œæµç¨‹
""")
    
    try:
        # æ¼”ç¤º1: åŸºç¡€æ„ŸçŸ¥é©±åŠ¨è§„åˆ’
        await demo_basic_perception_planning()
        
        # æ¼”ç¤º2: æ„ŸçŸ¥é©±åŠ¨é‡è§„åˆ’
        await demo_perception_driven_replan()
        
        # æ¼”ç¤º3: å¤šè½®å¯¹è¯
        await demo_multi_turn_dialogue()
        
        # æ¼”ç¤º4: CoTæ¨ç†
        await demo_cot_reasoning()
        
        # æ¼”ç¤º5: å®Œæ•´é›†æˆ
        await demo_full_integration()
        
        print("\n" + "="*70)
        print("ğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("="*70)
        print("""
å…³é”®ç‰¹æ€§æ€»ç»“ï¼š
âœ… æ„ŸçŸ¥ä¸è§„åˆ’æ·±åº¦èåˆ - æ¯æ¬¡è§„åˆ’éƒ½ç»“åˆå®æ—¶æ„ŸçŸ¥æ•°æ®
âœ… å“åº”å¼é‡è§„åˆ’ - ç¯å¢ƒå˜åŒ–è‡ªåŠ¨è§¦å‘ï¼Œè€Œéè¢«åŠ¨ç­‰å¾…å¤±è´¥
âœ… å¤šè½®å¯¹è¯èƒ½åŠ› - æ”¯æŒæ¾„æ¸…ã€ç¡®è®¤ã€æ±‡æŠ¥ä¸‰ç§å¯¹è¯æ¨¡å¼
âœ… CoTå¯è¿½æº¯æ¨ç† - æ‰€æœ‰å†³ç­–éƒ½æœ‰æ¨ç†é“¾è®°å½•
âœ… è‡ªé€‚åº”å¤æ‚åº¦ - ç®€å•ä»»åŠ¡å¿«é€Ÿæ‰§è¡Œï¼Œå¤æ‚ä»»åŠ¡æ·±åº¦æ¨ç†
""")
        
    except Exception as e:
        logger.error(f"æ¼”ç¤ºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logger.remove()
    logger.add(
        sys.stdout,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{message}</cyan>",
        level="INFO"
    )
    
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())

