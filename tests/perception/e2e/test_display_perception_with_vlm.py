#!/usr/bin/env python3
"""
E2E Test: Display Complete PerceptionData with Real VLM Analysis

This test captures and displays complete PerceptionData with real VLM scene understanding
using ollama llava:7b.

Usage:
    export ROS_DOMAIN_ID=42
    ros2 bag play /home/yangyuhui/sim_data_bag --loop
    ollama run llava:7b  # In another terminal
    python test_display_perception_with_vlm.py
"""

import sys
import os
import time
import json
import asyncio
from datetime import datetime
from dataclasses import dataclass
from typing import Optional, Dict, Any, List
import numpy as np

# Add Brain to path
sys.path.insert(0, '/media/yangyuhui/CODES1/Brain')

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image as SensorImage
from sensor_msgs.msg import PointCloud2
from sensor_msgs.msg import Imu as SensorImu
from nav_msgs.msg import Odometry

# Perception layer imports
from brain.perception.sensors.ros2_sensor_manager import PerceptionData
from brain.perception.understanding.vlm_perception import VLMPerception
from brain.perception.data.models import Pose3D, Velocity
from brain.perception.utils.coordinates import quaternion_to_euler


def ros_image_to_numpy(img_msg: SensorImage) -> np.ndarray:
    """Convert ROS2 Image message to numpy array (simple replacement for cv_bridge)"""
    if img_msg.encoding in ['rgb8', 'bgr8', 'mono8']:
        dtype = np.uint8
    elif img_msg.encoding == 'mono16':
        dtype = np.uint16
    else:
        raise ValueError(f"Unsupported encoding: {img_msg.encoding}")

    arr = np.frombuffer(img_msg.data, dtype=dtype)

    if img_msg.encoding in ['rgb8', 'bgr8']:
        n_channels = 3
    elif img_msg.encoding in ['mono8', 'mono16']:
        n_channels = 1
    else:
        n_channels = arr.size // (img_msg.height * img_msg.width)

    if n_channels == 1:
        arr = arr.reshape((img_msg.height, img_msg.width))
    else:
        arr = arr.reshape((img_msg.height, img_msg.width, n_channels))

    if img_msg.encoding == 'bgr8':
        arr = arr[:, :, ::-1].copy()

    return arr


class PerceptionDataWithVLM(Node):
    """Capture and display PerceptionData with real VLM analysis"""

    def __init__(self, num_frames: int = 3):
        super().__init__('perception_data_vlm')

        self.num_frames = num_frames
        self.frames_captured = 0
        self.start_time = time.time()

        # VLM service
        try:
            self.vlm = VLMPerception()
            self.get_logger().info("âœ… VLM initialized (ollama llava:7b)")
        except Exception as e:
            self.get_logger().error(f"âŒ VLM initialization failed: {e}")
            self.vlm = None

        # Sensor data buffers
        self.current_odometry = None
        self.current_rgb_image_msg = None
        self.current_rgb_image_np = None
        self.current_pointcloud = None
        self.current_imu = None

        # Storage for captured data
        self.captured_data: List[Dict[str, Any]] = []

        # ROS2 subscribers
        self._setup_subscribers()

        self.get_logger().info("âœ… PerceptionData with VLM initialized")
        self.get_logger().info(f"ğŸ“ Will capture {self.num_frames} frames")

    def _setup_subscribers(self):
        """Setup ROS2 topic subscribers"""
        self.create_subscription(Odometry, '/chassis/odom', self.odom_callback, 10)
        self.create_subscription(SensorImage, '/front_stereo_camera/left/image_raw', self.rgb_callback, 10)
        self.create_subscription(PointCloud2, '/front_3d_lidar/lidar_points', self.pointcloud_callback, 10)
        self.create_subscription(SensorImu, '/chassis/imu', self.imu_callback, 10)

        self.get_logger().info("ğŸ“¡ Subscribers created:")
        self.get_logger().info("   - /chassis/odom (Odometry)")
        self.get_logger().info("   - /front_stereo_camera/left/image_raw (RGB Image)")
        self.get_logger().info("   - /front_3d_lidar/lidar_points (Pointcloud)")
        self.get_logger().info("   - /chassis/imu (IMU)")

    def odom_callback(self, msg: Odometry):
        """Odometry callback"""
        self.current_odometry = msg
        self._try_capture_frame()

    def rgb_callback(self, msg: SensorImage):
        """RGB image callback"""
        try:
            self.current_rgb_image_msg = msg
            self.current_rgb_image_np = ros_image_to_numpy(msg)

            if len(self.current_rgb_image_np.shape) == 2:
                self.current_rgb_image_np = np.stack([self.current_rgb_image_np] * 3, axis=-1)

            self._try_capture_frame()
        except Exception as e:
            self.get_logger().error(f"RGB callback error: {e}")

    def pointcloud_callback(self, msg: PointCloud2):
        """Pointcloud callback"""
        self.current_pointcloud = msg
        self._try_capture_frame()

    def imu_callback(self, msg: SensorImu):
        """IMU callback"""
        self.current_imu = msg
        self._try_capture_frame()

    def _try_capture_frame(self):
        """Try to capture a complete frame"""
        if self.frames_captured >= self.num_frames:
            return

        if self.current_odometry is None or self.current_rgb_image_np is None:
            return

        self.frames_captured += 1
        self.get_logger().info(f"ğŸ“¸ Capturing frame {self.frames_captured}/{self.num_frames}...")

        # Create PerceptionData and process VLM
        perception_data = self._create_perception_data()

        # Display PerceptionData structure and content
        self._display_perception_data(perception_data, self.frames_captured)

        # Store for later
        self.captured_data.append(self._perception_to_dict(perception_data))

        # Clear buffers
        self.current_odometry = None
        self.current_rgb_image_msg = None
        self.current_rgb_image_np = None
        self.current_pointcloud = None
        self.current_imu = None

        # Wait between frames
        if self.frames_captured < self.num_frames:
            time.sleep(2.0)

    def _create_perception_data(self) -> PerceptionData:
        """Create PerceptionData object from current sensor data"""
        # Extract pose from odometry
        pose = None
        if self.current_odometry:
            pos = self.current_odometry.pose.pose.position
            ori = self.current_odometry.pose.pose.orientation
            roll, pitch, yaw = quaternion_to_euler((ori.x, ori.y, ori.z, ori.w))

            pose = Pose3D(
                x=pos.x,
                y=pos.y,
                z=pos.z,
                roll=roll,
                pitch=pitch,
                yaw=yaw
            )

        # Extract velocity from odometry
        velocity = None
        if self.current_odometry:
            twist = self.current_odometry.twist.twist
            velocity = Velocity(
                linear_x=twist.linear.x,
                linear_y=twist.linear.y,
                linear_z=twist.linear.z,
                angular_x=twist.angular.x,
                angular_y=twist.angular.y,
                angular_z=twist.angular.z
            )

        # Create PerceptionData
        perception = PerceptionData(timestamp=datetime.now())
        perception.pose = pose
        perception.velocity = velocity
        perception.rgb_image = self.current_rgb_image_np

        # Process VLM using direct ollama call (not async)
        if self.vlm and self.current_rgb_image_np is not None:
            print("\nğŸ¤– Triggering VLM scene analysis...")
            vlm_start = time.time()

            try:
                # Use direct ollama API call for scene understanding
                import ollama

                # Resize image for VLM (optional, to speed up processing)
                img = self.current_rgb_image_np
                from PIL import Image as PILImage
                pil_img = PILImage.fromarray(img)

                # Save to temp file and send to ollama
                import io
                buf = io.BytesIO()
                pil_img.save(buf, format='JPEG')
                img_bytes = buf.getvalue()

                # Call ollama llava:7b
                response = ollama.generate(
                    model='llava:7b',
                    prompt="""è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾åƒä¸­çš„åœºæ™¯ã€‚
è¯·ç”¨ä¸­æ–‡å›ç­”ï¼ŒåŒ…æ‹¬ï¼š
1. åœºæ™¯æ¦‚è¿°ï¼ˆå®¤å†…/å®¤å¤–ï¼Œç¯å¢ƒç±»å‹ï¼‰
2. çœ‹åˆ°çš„ç‰©ä½“ï¼ˆè‡³å°‘3ä¸ªï¼‰
3. ç©ºé—´å…³ç³»ï¼ˆç‰©ä½“çš„ç›¸å¯¹ä½ç½®ï¼‰
4. å¯¼èˆªå»ºè®®ï¼ˆæ˜¯å¦å¯ä»¥é€šè¡Œï¼Œæœ‰ä»€ä¹ˆéœ€è¦æ³¨æ„çš„ï¼‰""",
                    images=[img_bytes]
                )

                vlm_time = time.time() - vlm_start
                print(f"   âœ… VLM analysis completed in {vlm_time:.2f} seconds")
                print(f"   ğŸ“ VLM Response: {response['response'][:200]}...")

                # Store VLM result
                perception.scene_description = type('SceneDescription', (), {
                    'summary': response['response']
                })()

            except Exception as e:
                print(f"   âš ï¸  VLM analysis failed: {e}")
                import traceback
                traceback.print_exc()

        # Sensor status
        perception.sensor_status = {
            'odometry': self.current_odometry is not None,
            'rgb_camera': self.current_rgb_image_np is not None,
            'pointcloud': self.current_pointcloud is not None,
            'imu': self.current_imu is not None,
            'vlm': self.vlm is not None
        }

        return perception

    def _display_perception_data(self, data: PerceptionData, frame_id: int):
        """Display complete PerceptionData"""
        print("\n" + "=" * 70)
        print(f"PerceptionData Frame #{frame_id}")
        print("=" * 70)
        print(f"Timestamp: {data.timestamp}")

        print("\nğŸ“‹ PerceptionDataå­—æ®µåˆ—è¡¨:")
        fields = [
            ("1.", "timestamp", "datetime - æ•°æ®æ—¶é—´æˆ³"),
            ("2.", "pose", "Pose3D - 3Dä½ç½®å’Œå§¿æ€"),
            ("3.", "velocity", "Velocity - 6è‡ªç”±åº¦é€Ÿåº¦"),
            ("4.", "rgb_image", "np.ndarray - RGBå›¾åƒ (HÃ—WÃ—3)"),
            ("5.", "rgb_image_right", "np.ndarray - å³RGBå›¾åƒ"),
            ("6.", "depth_image", "np.ndarray - æ·±åº¦å›¾åƒ"),
            ("7.", "laser_ranges", "List[float] - æ¿€å…‰é›·è¾¾è·ç¦»æµ‹é‡"),
            ("8.", "laser_angles", "List[float] - æ¿€å…‰é›·è¾¾è§’åº¦"),
            ("9.", "pointcloud", "np.ndarray - 3Dç‚¹äº‘"),
            ("10.", "obstacles", "List[Dict] - éšœç¢ç‰©åˆ—è¡¨"),
            ("11.", "occupancy_grid", "np.ndarray - å æ®æ …æ ¼"),
            ("12.", "sensor_status", "Dict[str, bool] - ä¼ æ„Ÿå™¨çŠ¶æ€"),
            ("13.", "semantic_objects", "List[DetectedObject] - VLMè¯†åˆ«çš„ç‰©ä½“"),
            ("14.", "scene_description", "SceneDescription - VLMåœºæ™¯æè¿°"),
            ("15.", "spatial_relations", "List[Dict] - ç©ºé—´å…³ç³»"),
            ("16.", "navigation_hints", "List[str] - å¯¼èˆªæç¤º"),
        ]
        for num, field_name, field_desc in fields:
            print(f"{num:>3} {field_name:20s} - {field_desc}")

        print("\n" + "-" * 70)
        print("å®é™…æ•°æ®å†…å®¹:")
        print("-" * 70)

        # Pose
        if data.pose:
            print(f"\nğŸ“ ä½å§¿ä¿¡æ¯ (pose):")
            print(f"   ä½ç½®: x={data.pose.x:.3f}, y={data.pose.y:.3f}, z={data.pose.z:.3f}")
            print(f"   å§¿æ€: roll={data.pose.roll:.2f}, pitch={data.pose.pitch:.2f}, yaw={data.pose.yaw:.2f}")

        # Velocity
        if data.velocity:
            print(f"\nğŸš€ é€Ÿåº¦ä¿¡æ¯ (velocity):")
            print(f"   çº¿é€Ÿåº¦: x={data.velocity.linear_x:.3f}, y={data.velocity.linear_y:.3f}, z={data.velocity.linear_z:.3f} m/s")
            print(f"   è§’é€Ÿåº¦: x={data.velocity.angular_x:.3f}, y={data.velocity.angular_y:.3f}, z={data.velocity.angular_z:.3f} rad/s")

        # RGB Image
        if data.rgb_image is not None:
            print(f"\nğŸ“· RGBå›¾åƒ (rgb_image):")
            print(f"   å½¢çŠ¶: {data.rgb_image.shape}")
            print(f"   å¤§å°: {data.rgb_image.nbytes / (1024*1024):.2f} MB")
            print(f"   æ•°æ®ç±»å‹: {data.rgb_image.dtype}")

        # VLM Scene Description
        if data.scene_description:
            print(f"\nğŸ¤– VLMåœºæ™¯æè¿° (scene_description):")
            if hasattr(data.scene_description, 'summary'):
                summary = data.scene_description.summary
                # Truncate if too long
                if len(summary) > 500:
                    summary = summary[:500] + "..."
                print(f"   {summary}")

        # Sensor Status
        print(f"\nğŸ“¡ ä¼ æ„Ÿå™¨çŠ¶æ€ (sensor_status):")
        for sensor, status in data.sensor_status.items():
            status_icon = "âœ…" if status else "âŒ"
            print(f"   {status_icon} {sensor}")

        print("\n" + "=" * 70)

    def _perception_to_dict(self, data: PerceptionData) -> Dict[str, Any]:
        """Convert PerceptionData to dictionary"""
        result = {"timestamp": data.timestamp.isoformat()}

        if data.pose:
            result["pose"] = {
                "x": data.pose.x,
                "y": data.pose.y,
                "z": data.pose.z,
                "roll": data.pose.roll,
                "pitch": data.pose.pitch,
                "yaw": data.pose.yaw,
            }

        if data.velocity:
            result["velocity"] = {
                "linear_x": data.velocity.linear_x,
                "linear_y": data.velocity.linear_y,
                "linear_z": data.velocity.linear_z,
                "angular_x": data.velocity.angular_x,
                "angular_y": data.velocity.angular_y,
                "angular_z": data.velocity.angular_z,
            }

        if data.rgb_image is not None:
            result["rgb_image"] = {
                "shape": list(data.rgb_image.shape),
                "dtype": str(data.rgb_image.dtype),
                "size_mb": data.rgb_image.nbytes / (1024*1024),
            }

        if data.scene_description and hasattr(data.scene_description, 'summary'):
            result["scene_description"] = {
                "summary": data.scene_description.summary
            }

        result["sensor_status"] = data.sensor_status

        return result

    def save_to_json(self, filename: str):
        """Save captured data to JSON"""
        data = {
            "metadata": {
                "capture_time": datetime.now().isoformat(),
                "num_frames": len(self.captured_data),
                "vlm_enabled": self.vlm is not None,
            },
            "frames": self.captured_data
        }

        with open(filename, 'w') as f:
            json.dump(data, f, indent=2, default=str)

        self.get_logger().info(f"ğŸ’¾ Saved to {filename}")

    def save_to_markdown(self, filename: str):
        """Save captured data to Markdown"""
        lines = []

        lines.append("# PerceptionData Display Report with VLM\n")
        lines.append(f"**Capture Time**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        lines.append(f"**Total Frames**: {len(self.captured_data)}\n")
        lines.append(f"**VLM Enabled**: {self.vlm is not None}\n")
        lines.append("\n---\n")

        for i, frame_data in enumerate(self.captured_data):
            lines.append(f"## Frame {i+1}\n")
            lines.append(f"**Timestamp**: {frame_data['timestamp']}\n\n")

            if 'pose' in frame_data:
                pose = frame_data['pose']
                lines.append("### ğŸ“ ä½å§¿ä¿¡æ¯\n")
                lines.append(f"- ä½ç½®: x={pose['x']:.3f}, y={pose['y']:.3f}, z={pose['z']:.3f}\n")
                lines.append(f"- å§¿æ€: roll={pose['roll']:.2f}, pitch={pose['pitch']:.2f}, yaw={pose['yaw']:.2f}\n\n")

            if 'velocity' in frame_data:
                vel = frame_data['velocity']
                lines.append("### ğŸš€ é€Ÿåº¦ä¿¡æ¯\n")
                lines.append(f"- çº¿é€Ÿåº¦: x={vel['linear_x']:.3f}, y={vel['linear_y']:.3f} m/s\n")
                lines.append(f"- è§’é€Ÿåº¦: z={vel['angular_z']:.3f} rad/s\n\n")

            if 'rgb_image' in frame_data:
                img = frame_data['rgb_image']
                lines.append("### ğŸ“· RGBå›¾åƒ\n")
                lines.append(f"- å½¢çŠ¶: {img['shape']}\n")
                lines.append(f"- å¤§å°: {img['size_mb']:.2f} MB\n\n")

            if 'scene_description' in frame_data:
                lines.append("### ğŸ¤– VLMåœºæ™¯æè¿°\n")
                lines.append(f"{frame_data['scene_description']['summary']}\n\n")

            lines.append("---\n")

        with open(filename, 'w') as f:
            f.write('\n'.join(lines))

        self.get_logger().info(f"ğŸ’¾ Saved to {filename}")


def main(args=None):
    """Main function"""
    os.environ['ROS_DOMAIN_ID'] = '42'

    rclpy.init(args=args)

    # Create display node
    display = PerceptionDataWithVLM(num_frames=3)

    print("\n" + "=" * 70)
    print("ğŸ¯ PerceptionDataå®Œæ•´å±•ç¤ºæµ‹è¯• (å«VLMåœºæ™¯ç†è§£) - 3å¸§")
    print("=" * 70)
    print("\nç¯å¢ƒé…ç½®:")
    print("  â€¢ ROS_DOMAIN_ID: 42")
    print("  â€¢ rosbag: /home/yangyuhui/sim_data_bag")
    print("  â€¢ VLM: ollama llava:7b")
    print("\næ­£åœ¨æ•è·æ•°æ®...\n")

    try:
        # Spin until all frames captured
        while display.frames_captured < display.num_frames:
            rclpy.spin_once(display, timeout_sec=0.1)

    except KeyboardInterrupt:
        print("\nâš ï¸  Test interrupted by user")
    finally:
        # Save results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_file = f"/media/yangyuhui/CODES1/Brain/tests/perception/e2e/perception_data_vlm_{timestamp}.json"
        md_file = f"/media/yangyuhui/CODES1/Brain/tests/perception/e2e/perception_data_vlm_{timestamp}.md"

        display.save_to_json(json_file)
        display.save_to_markdown(md_file)

        # Print summary
        print("\n" + "=" * 70)
        print("ğŸ“Š æµ‹è¯•å®Œæˆ")
        print("=" * 70)
        print(f"  â€¢ æ•è·å¸§æ•°: {display.frames_captured}")
        print(f"  â€¢ VLMå·²å¯ç”¨: {display.vlm is not None}")
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"  â€¢ {json_file}")
        print(f"  â€¢ {md_file}")
        print("=" * 70)

        # Cleanup
        display.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
