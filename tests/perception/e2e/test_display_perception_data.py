#!/usr/bin/env python3
"""
E2E Test: Display Complete PerceptionData Structure and Content

This test captures and displays the complete PerceptionData structure with all fields,
including real VLM scene understanding using ollama llava:7b.

Usage:
    export ROS_DOMAIN_ID=42
    ros2 bag play /home/yangyuhui/sim_data_bag --loop
    ollama run llava:7b  # In another terminal
    python test_display_perception_data.py

Requirements:
    - ollama run llava:7b (VLM model running)
    - rosbag playback with ROS_DOMAIN_ID=42
"""

import sys
import os
import time
import json
from datetime import datetime
from dataclasses import dataclass, field
from typing import Optional, Dict, Any, List
import numpy as np
import cv2

# Add Brain to path
sys.path.insert(0, '/media/yangyuhui/CODES1/Brain')

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image as SensorImage
from sensor_msgs.msg import PointCloud2
from sensor_msgs.msg import Imu as SensorImu
from nav_msgs.msg import Odometry

# Perception layer imports
from brain.perception.sensors.ros2_sensor_manager import PerceptionData
from brain.perception.understanding.vlm_perception import VLMPerception
from brain.perception.data.models import Pose3D, Velocity
from brain.perception.utils.math_utils import compute_laser_angles


def ros_image_to_numpy(img_msg: SensorImage) -> np.ndarray:
    """
    Convert ROS2 Image message to numpy array.
    Simple replacement for cv_bridge using only numpy.

    Supports: rgb8, bgr8, mono8
    """
    # Determine dtype from encoding
    if img_msg.encoding in ['rgb8', 'bgr8', 'mono8']:
        dtype = np.uint8
    elif img_msg.encoding == 'mono16':
        dtype = np.uint16
    else:
        raise ValueError(f"Unsupported encoding: {img_msg.encoding}")

    # Convert from buffer to numpy array
    arr = np.frombuffer(img_msg.data, dtype=dtype)

    # Determine number of channels
    if img_msg.encoding in ['rgb8', 'bgr8']:
        n_channels = 3
    elif img_msg.encoding in ['mono8', 'mono16']:
        n_channels = 1
    else:
        # Try to infer from image dimensions
        n_channels = arr.size // (img_msg.height * img_msg.width)

    # Reshape to image dimensions
    if n_channels == 1:
        arr = arr.reshape((img_msg.height, img_msg.width))
    else:
        arr = arr.reshape((img_msg.height, img_msg.width, n_channels))

    # Convert BGR to RGB if needed
    if img_msg.encoding == 'bgr8':
        arr = arr[:, :, ::-1].copy()  # BGR -> RGB

    return arr


class PerceptionDataDisplay(Node):
    """Display complete PerceptionData with VLM integration"""

    def __init__(self, num_frames: int = 3):
        super().__init__('perception_data_display')

        self.num_frames = num_frames
        self.frames_captured = 0
        self.start_time = time.time()

        # VLM service
        try:
            self.vlm = VLMPerception()
            self.get_logger().info("âœ… VLM initialized (ollama llava:7b)")
        except Exception as e:
            self.get_logger().error(f"âŒ VLM initialization failed: {e}")
            self.vlm = None

        # Sensor data buffers
        self.current_odometry = None
        self.current_rgb_image_msg = None  # Store ROS2 message
        self.current_rgb_image_np = None  # Store numpy array for VLM
        self.current_pointcloud = None
        self.current_imu = None

        # ROS2 subscribers
        self._setup_subscribers()

        # Storage for captured PerceptionData
        self.captured_data: List[Dict[str, Any]] = []

        self.get_logger().info("âœ… PerceptionData display initialized")
        self.get_logger().info(f"ğŸ“ Will capture {self.num_frames} frames")

    def _setup_subscribers(self):
        """Setup ROS2 topic subscribers"""
        # Odometry subscriber
        self.create_subscription(
            Odometry,
            '/chassis/odom',
            self.odom_callback,
            10
        )

        # RGB image subscriber
        self.create_subscription(
            SensorImage,
            '/front_stereo_camera/left/image_raw',
            self.rgb_callback,
            10
        )

        # Pointcloud subscriber
        self.create_subscription(
            PointCloud2,
            '/front_3d_lidar/lidar_points',
            self.pointcloud_callback,
            10
        )

        # IMU subscriber
        self.create_subscription(
            SensorImu,
            '/chassis/imu',
            self.imu_callback,
            10
        )

        self.get_logger().info("ğŸ“¡ Subscribers created:")
        self.get_logger().info("   - /chassis/odom (Odometry)")
        self.get_logger().info("   - /front_stereo_camera/left/image_raw (RGB Image)")
        self.get_logger().info("   - /front_3d_lidar/lidar_points (Pointcloud)")
        self.get_logger().info("   - /chassis/imu (IMU)")

    def odom_callback(self, msg: Odometry):
        """Odometry callback"""
        self.current_odometry = msg
        self._try_capture_frame()

    def rgb_callback(self, msg: SensorImage):
        """RGB image callback"""
        try:
            self.current_rgb_image_msg = msg

            # Convert ROS2 Image to numpy array using our custom function
            self.current_rgb_image_np = ros_image_to_numpy(msg)

            # Ensure we have RGB format (3 channels)
            if len(self.current_rgb_image_np.shape) == 2:
                # Grayscale to RGB
                self.current_rgb_image_np = np.stack([self.current_rgb_image_np] * 3, axis=-1)

            self._try_capture_frame()
        except Exception as e:
            self.get_logger().error(f"RGB callback error: {e}")

    def pointcloud_callback(self, msg: PointCloud2):
        """Pointcloud callback"""
        self.current_pointcloud = msg
        self._try_capture_frame()

    def imu_callback(self, msg: SensorImu):
        """IMU callback"""
        self.current_imu = msg
        self._try_capture_frame()

    def _try_capture_frame(self):
        """Try to capture a complete frame"""
        if self.frames_captured >= self.num_frames:
            return

        # Wait for at least odometry and RGB image
        if self.current_odometry is None or self.current_rgb_image_np is None:
            return

        self.frames_captured += 1
        self.get_logger().info(f"ğŸ“¸ Capturing frame {self.frames_captured}/{self.num_frames}...")

        # Create PerceptionData object
        perception_data = self._create_perception_data()

        # Display PerceptionData structure and content
        self._display_perception_data(perception_data, self.frames_captured)

        # Store for later
        self.captured_data.append(self._perception_to_dict(perception_data))

        # Clear buffers for next frame
        self.current_odometry = None
        self.current_rgb_image_msg = None
        self.current_rgb_image_np = None
        self.current_pointcloud = None
        self.current_imu = None

        # Wait a bit between frames
        if self.frames_captured < self.num_frames:
            time.sleep(2.0)

    def _create_perception_data(self) -> PerceptionData:
        """Create a PerceptionData object from current sensor data"""
        # Extract pose from odometry
        pose = None
        if self.current_odometry:
            pos = self.current_odometry.pose.pose.position
            ori = self.current_odometry.pose.pose.orientation

            # Convert quaternion to Euler angles
            from brain.perception.utils.coordinates import quaternion_to_euler
            roll, pitch, yaw = quaternion_to_euler((ori.x, ori.y, ori.z, ori.w))

            pose = Pose3D(
                x=pos.x,
                y=pos.y,
                z=pos.z,
                roll=roll,
                pitch=pitch,
                yaw=yaw
            )

        # Extract velocity from odometry
        velocity = None
        if self.current_odometry:
            twist = self.current_odometry.twist.twist
            velocity = Velocity(
                linear_x=twist.linear.x,
                linear_y=twist.linear.y,
                linear_z=twist.linear.z,
                angular_x=twist.angular.x,
                angular_y=twist.angular.y,
                angular_z=twist.angular.z
            )

        # RGB image (already converted to numpy array by ros2_numpy)
        rgb_image = self.current_rgb_image_np

        # Pointcloud (extract basic info)
        pointcloud = None
        laser_ranges = None
        laser_angles = None
        if self.current_pointcloud:
            # For simplicity, just store metadata
            num_points = self.current_pointcloud.width * self.current_pointcloud.height
            if self.current_pointcloud.height == 0:
                num_points = self.current_pointcloud.width
            # Could parse full pointcloud here, but for display purposes metadata is enough

        # IMU
        imu_data = None
        if self.current_imu:
            imu_data = {
                'linear_acceleration': {
                    'x': self.current_imu.linear_acceleration.x,
                    'y': self.current_imu.linear_acceleration.y,
                    'z': self.current_imu.linear_acceleration.z
                },
                'angular_velocity': {
                    'x': self.current_imu.angular_velocity.x,
                    'y': self.current_imu.angular_velocity.y,
                    'z': self.current_imu.angular_velocity.z
                }
            }

        # Create PerceptionData
        perception = PerceptionData(timestamp=datetime.now())

        # Set basic fields
        perception.pose = pose
        perception.velocity = velocity
        perception.rgb_image = rgb_image

        # Process VLM if available
        if self.vlm and rgb_image is not None:
            print("\nğŸ¤– Triggering VLM scene analysis...")
            vlm_start = time.time()

            try:
                # Call VLM for scene understanding
                scene_result = self.vlm.understand_scene(rgb_image)

                vlm_time = time.time() - vlm_start
                print(f"   âœ… VLM analysis completed in {vlm_time:.2f} seconds")

                # Store VLM results
                if hasattr(scene_result, 'summary'):
                    perception.scene_description = scene_result

                if hasattr(scene_result, 'objects') and scene_result.objects:
                    perception.semantic_objects = scene_result.objects

                if hasattr(scene_result, 'spatial_relations'):
                    perception.spatial_relations = scene_result.spatial_relations

                if hasattr(scene_result, 'navigation_hints'):
                    perception.navigation_hints = scene_result.navigation_hints

            except Exception as e:
                print(f"   âš ï¸  VLM analysis failed: {e}")

        # Sensor status
        perception.sensor_status = {
            'odometry': self.current_odometry is not None,
            'rgb_camera': self.current_rgb_image_np is not None,
            'pointcloud': self.current_pointcloud is not None,
            'imu': self.current_imu is not None,
            'vlm': self.vlm is not None
        }

        return perception

    def _display_perception_data(self, data: PerceptionData, frame_id: int):
        """Display complete PerceptionData structure and content"""
        print("\n" + "=" * 70)
        print(f"PerceptionData Frame #{frame_id}")
        print("=" * 70)
        print(f"Timestamp: {data.timestamp}")

        # Display structure first
        print("\nğŸ“‹ PerceptionDataå­—æ®µåˆ—è¡¨:")
        fields = [
            ("timestamp", "datetime - æ•°æ®æ—¶é—´æˆ³"),
            ("pose", "Pose3D - 3Dä½ç½®å’Œå§¿æ€"),
            ("velocity", "Velocity - 6è‡ªç”±åº¦é€Ÿåº¦"),
            ("rgb_image", "np.ndarray - RGBå›¾åƒ (HÃ—WÃ—3)"),
            ("rgb_image_right", "np.ndarray - å³RGBå›¾åƒ"),
            ("depth_image", "np.ndarray - æ·±åº¦å›¾åƒ"),
            ("laser_ranges", "List[float] - æ¿€å…‰é›·è¾¾è·ç¦»æµ‹é‡"),
            ("laser_angles", "List[float] - æ¿€å…‰é›·è¾¾è§’åº¦"),
            ("pointcloud", "np.ndarray - 3Dç‚¹äº‘"),
            ("obstacles", "List[Dict] - éšœç¢ç‰©åˆ—è¡¨"),
            ("occupancy_grid", "np.ndarray - å æ®æ …æ ¼"),
            ("sensor_status", "Dict[str, bool] - ä¼ æ„Ÿå™¨çŠ¶æ€"),
            ("semantic_objects", "List[DetectedObject] - VLMè¯†åˆ«çš„ç‰©ä½“"),
            ("scene_description", "SceneDescription - VLMåœºæ™¯æè¿°"),
            ("spatial_relations", "List[Dict] - ç©ºé—´å…³ç³»"),
            ("navigation_hints", "List[str] - å¯¼èˆªæç¤º"),
        ]
        for i, (field_name, field_desc) in enumerate(fields, 1):
            print(f"{i:2d}. {field_name:20s} - {field_desc}")

        print("\n" + "-" * 70)
        print("å®é™…æ•°æ®å†…å®¹:")
        print("-" * 70)

        # Pose
        if data.pose:
            print(f"\nğŸ“ ä½å§¿ä¿¡æ¯ (pose):")
            print(f"   ä½ç½®: x={data.pose.x:.3f}, y={data.pose.y:.3f}, z={data.pose.z:.3f}")
            if hasattr(data.pose, 'roll'):
                print(f"   å§¿æ€: roll={data.pose.roll:.2f}, pitch={data.pose.pitch:.2f}, yaw={data.pose.yaw:.2f}")

        # Velocity
        if data.velocity:
            print(f"\nğŸš€ é€Ÿåº¦ä¿¡æ¯ (velocity):")
            print(f"   çº¿é€Ÿåº¦: x={data.velocity.linear_x:.3f}, y={data.velocity.linear_y:.3f}, z={data.velocity.linear_z:.3f} m/s")
            print(f"   è§’é€Ÿåº¦: x={data.velocity.angular_x:.3f}, y={data.velocity.angular_y:.3f}, z={data.velocity.angular_z:.3f} rad/s")

        # RGB Image
        if data.rgb_image is not None:
            print(f"\nğŸ“· RGBå›¾åƒ (rgb_image):")
            print(f"   å½¢çŠ¶: {data.rgb_image.shape}")
            print(f"   å¤§å°: {data.rgb_image.nbytes / (1024*1024):.2f} MB")
            print(f"   æ•°æ®ç±»å‹: {data.rgb_image.dtype}")
        else:
            print(f"\nğŸ“· RGBå›¾åƒ: âŒ None")

        # Laser/Pointcloud
        if data.pointcloud is not None:
            print(f"\nğŸ”¬ ç‚¹äº‘ (pointcloud):")
            print(f"   æ•°æ®: {type(data.pointcloud)}")
        else:
            print(f"\nğŸ”¬ ç‚¹äº‘: âŒ None")

        if data.laser_ranges is not None:
            print(f"\nğŸ”¬ æ¿€å…‰é›·è¾¾ (laser_ranges):")
            print(f"   æµ‹é‡ç‚¹æ•°: {len(data.laser_ranges)}")
            print(f"   å‰æ–¹è·ç¦»: {data.get_front_distance():.2f} m")
            print(f"   å·¦ä¾§è·ç¦»: {data.get_left_distance():.2f} m")
            print(f"   å³ä¾§è·ç¦»: {data.get_right_distance():.2f} m")
            print(f"   è·¯å¾„ç•…é€š: {data.is_path_clear('front', 1.0)}")

        # Obstacles
        if data.obstacles:
            print(f"\nâš ï¸  éšœç¢ç‰© (obstacles): {len(data.obstacles)} detected")
            for i, obs in enumerate(data.obstacles[:5]):
                print(f"   [{i}] {obs}")
        else:
            print(f"\nâš ï¸  éšœç¢ç‰©: æ— ")

        # VLM Scene Understanding
        if data.scene_description:
            print(f"\nğŸ¤– VLMåœºæ™¯ç†è§£ (scene_description):")
            if hasattr(data.scene_description, 'summary'):
                print(f"   åœºæ™¯æè¿°: {data.scene_description.summary}")

        # Semantic Objects
        if data.semantic_objects:
            print(f"\nğŸ¤– è¯†åˆ«çš„ç‰©ä½“ (semantic_objects): {len(data.semantic_objects)} ä¸ª")
            for i, obj in enumerate(data.semantic_objects[:5]):
                print(f"   [{i+1}] {obj.label} - ç½®ä¿¡åº¦: {obj.confidence:.2f}")
                if hasattr(obj, 'description') and obj.description:
                    print(f"       æè¿°: {obj.description}")
                if hasattr(obj, 'position_description') and obj.position_description:
                    print(f"       ä½ç½®: {obj.position_description}")
        else:
            print(f"\nğŸ¤– è¯†åˆ«çš„ç‰©ä½“: æ— ")

        # Spatial Relations
        if data.spatial_relations:
            print(f"\nğŸ¤– ç©ºé—´å…³ç³» (spatial_relations):")
            for rel in data.spatial_relations[:3]:
                print(f"   - {rel}")
        else:
            print(f"\nğŸ¤– ç©ºé—´å…³ç³»: æ— ")

        # Navigation Hints
        if data.navigation_hints:
            print(f"\nğŸ¤– å¯¼èˆªæç¤º (navigation_hints):")
            for hint in data.navigation_hints[:3]:
                print(f"   - {hint}")
        else:
            print(f"\nğŸ¤– å¯¼èˆªæç¤º: æ— ")

        # Sensor Status
        print(f"\nğŸ“¡ ä¼ æ„Ÿå™¨çŠ¶æ€ (sensor_status):")
        for sensor, status in data.sensor_status.items():
            status_icon = "âœ…" if status else "âŒ"
            print(f"   {status_icon} {sensor}")

        print("\n" + "=" * 70)

    def _perception_to_dict(self, data: PerceptionData) -> Dict[str, Any]:
        """Convert PerceptionData to dictionary for JSON serialization"""
        result = {
            "timestamp": data.timestamp.isoformat(),
        }

        # Pose
        if data.pose:
            result["pose"] = {
                "x": data.pose.x,
                "y": data.pose.y,
                "z": data.pose.z,
                "roll": data.pose.roll if hasattr(data.pose, 'roll') else 0.0,
                "pitch": data.pose.pitch if hasattr(data.pose, 'pitch') else 0.0,
                "yaw": data.pose.yaw if hasattr(data.pose, 'yaw') else 0.0,
            }

        # Velocity
        if data.velocity:
            result["velocity"] = {
                "linear_x": data.velocity.linear_x,
                "linear_y": data.velocity.linear_y,
                "linear_z": data.velocity.linear_z,
                "angular_x": data.velocity.angular_x,
                "angular_y": data.velocity.angular_y,
                "angular_z": data.velocity.angular_z,
            }

        # RGB Image
        if data.rgb_image is not None:
            result["rgb_image"] = {
                "shape": list(data.rgb_image.shape),
                "dtype": str(data.rgb_image.dtype),
                "size_mb": data.rgb_image.nbytes / (1024*1024),
            }

        # Semantic Objects
        if data.semantic_objects:
            result["semantic_objects"] = [
                {
                    "label": obj.label,
                    "confidence": obj.confidence,
                    "description": obj.description if hasattr(obj, 'description') else "",
                    "position": obj.position_description if hasattr(obj, 'position_description') else "",
                }
                for obj in data.semantic_objects
            ]

        # Scene Description
        if data.scene_description and hasattr(data.scene_description, 'summary'):
            result["scene_description"] = {
                "summary": data.scene_description.summary
            }

        # Spatial Relations
        if data.spatial_relations:
            result["spatial_relations"] = data.spatial_relations

        # Navigation Hints
        if data.navigation_hints:
            result["navigation_hints"] = data.navigation_hints

        # Sensor Status
        result["sensor_status"] = data.sensor_status

        return result

    def save_to_json(self, filename: str):
        """Save captured data to JSON file"""
        data = {
            "metadata": {
                "capture_time": datetime.now().isoformat(),
                "num_frames": len(self.captured_data),
                "vlm_enabled": self.vlm is not None,
            },
            "frames": self.captured_data
        }

        with open(filename, 'w') as f:
            json.dump(data, f, indent=2, default=str)

        self.get_logger().info(f"ğŸ’¾ Saved to {filename}")

    def save_to_markdown(self, filename: str):
        """Save captured data to Markdown file"""
        lines = []

        # Header
        lines.append("# PerceptionData Display Report\n")
        lines.append(f"**Capture Time**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        lines.append(f"**Total Frames**: {len(self.captured_data)}\n")
        lines.append(f"**VLM Enabled**: {self.vlm is not None}\n")
        lines.append("\n---\n")

        # Frames
        for i, frame_data in enumerate(self.captured_data):
            lines.append(f"## Frame {i+1}\n")
            lines.append(f"**Timestamp**: {frame_data['timestamp']}\n\n")

            # Add sections for each field
            if 'pose' in frame_data:
                pose = frame_data['pose']
                lines.append("### ğŸ“ ä½å§¿ä¿¡æ¯\n")
                lines.append(f"- ä½ç½®: x={pose['x']:.3f}, y={pose['y']:.3f}, z={pose['z']:.3f}\n")
                lines.append(f"- å§¿æ€: roll={pose['roll']:.2f}, pitch={pose['pitch']:.2f}, yaw={pose['yaw']:.2f}\n\n")

            if 'velocity' in frame_data:
                vel = frame_data['velocity']
                lines.append("### ğŸš€ é€Ÿåº¦ä¿¡æ¯\n")
                lines.append(f"- çº¿é€Ÿåº¦: x={vel['linear_x']:.3f}, y={vel['linear_y']:.3f}, z={vel['linear_z']:.3f} m/s\n")
                lines.append(f"- è§’é€Ÿåº¦: z={vel['angular_z']:.3f} rad/s\n\n")

            if 'rgb_image' in frame_data:
                img = frame_data['rgb_image']
                lines.append("### ğŸ“· RGBå›¾åƒ\n")
                lines.append(f"- å½¢çŠ¶: {img['shape']}\n")
                lines.append(f"- å¤§å°: {img['size_mb']:.2f} MB\n\n")

            if 'scene_description' in frame_data:
                lines.append("### ğŸ¤– VLMåœºæ™¯æè¿°\n")
                lines.append(f"{frame_data['scene_description']['summary']}\n\n")

            if 'semantic_objects' in frame_data:
                lines.append("### ğŸ¤– è¯†åˆ«çš„ç‰©ä½“\n")
                for obj in frame_data['semantic_objects']:
                    lines.append(f"- **{obj['label']}** (ç½®ä¿¡åº¦: {obj['confidence']:.2f})\n")
                    if obj['description']:
                        lines.append(f"  - {obj['description']}\n")
                lines.append("\n")

            if 'navigation_hints' in frame_data:
                lines.append("### ğŸ¤– å¯¼èˆªæç¤º\n")
                for hint in frame_data['navigation_hints']:
                    lines.append(f"- {hint}\n")
                lines.append("\n")

            lines.append("---\n")

        with open(filename, 'w') as f:
            f.write('\n'.join(lines))

        self.get_logger().info(f"ğŸ’¾ Saved to {filename}")


def main(args=None):
    """Main function"""
    # Set ROS_DOMAIN_ID
    os.environ['ROS_DOMAIN_ID'] = '42'

    rclpy.init(args=args)

    # Create display node
    display = PerceptionDataDisplay(num_frames=3)

    print("\n" + "=" * 70)
    print("ğŸ¯ PerceptionDataå®Œæ•´å±•ç¤ºæµ‹è¯• (3å¸§)")
    print("=" * 70)
    print("\nç¯å¢ƒé…ç½®:")
    print("  â€¢ ROS_DOMAIN_ID: 42")
    print("  â€¢ rosbag: /home/yangyuhui/sim_data_bag")
    print("  â€¢ VLM: ollama llava:7b")
    print("\næ­£åœ¨æ•è·æ•°æ®...\n")

    try:
        # Spin until all frames captured
        while display.frames_captured < display.num_frames:
            rclpy.spin_once(display, timeout_sec=0.1)

    except KeyboardInterrupt:
        print("\nâš ï¸  Test interrupted by user")
    finally:
        # Save results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_file = f"/media/yangyuhui/CODES1/Brain/tests/perception/e2e/perception_data_display_{timestamp}.json"
        md_file = f"/media/yangyuhui/CODES1/Brain/tests/perception/e2e/perception_data_display_{timestamp}.md"

        display.save_to_json(json_file)
        display.save_to_markdown(md_file)

        # Print summary
        print("\n" + "=" * 70)
        print("ğŸ“Š æµ‹è¯•å®Œæˆ")
        print("=" * 70)
        print(f"  â€¢ æ•è·å¸§æ•°: {display.frames_captured}")
        print(f"  â€¢ VLMå·²å¯ç”¨: {display.vlm is not None}")
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"  â€¢ {json_file}")
        print(f"  â€¢ {md_file}")
        print("=" * 70)

        # Cleanup
        display.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
