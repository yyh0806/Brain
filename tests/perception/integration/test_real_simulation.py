#!/usr/bin/env python3
"""
æ„ŸçŸ¥å±‚å®Œæ•´æµç¨‹æ¨¡æ‹Ÿæµ‹è¯•

æ¨¡æ‹Ÿå®Œæ•´çš„æ„ŸçŸ¥æ•°æ®å¤„ç†æµç¨‹ï¼š
- ç”Ÿæˆæ¨¡æ‹Ÿçš„RGB-Dæ•°æ®
- æµ‹è¯•ç›®æ ‡æ£€æµ‹å™¨
- æµ‹è¯•è·Ÿè¸ªåŠŸèƒ½
- ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š
"""

import sys
import os
import time
import asyncio
import numpy as np
from typing import List
from dataclasses import dataclass

sys.path.insert(0, '/media/yangyuhui/CODES1/Brain-Perception-Dev')

from brain.perception.detection.detector import ObjectDetector
from brain.cognitive.world_model.world_model import WorldModel
from brain.perception.core.types import Pose2D, DetectedObject, Position3D
from brain.perception.core.enums import ObjectType


@dataclass
class TestMetrics:
    """æµ‹è¯•æŒ‡æ ‡"""
    total_frames: int = 0
    total_detections: int = 0
    tracking_success: int = 0
    processing_times: List[float] = None
    world_model_updates: int = 0

    def __post_init__(self):
        if self.processing_times is None:
            self.processing_times = []


class PerceptionSimulator:
    """æ„ŸçŸ¥æ¨¡æ‹Ÿå™¨"""

    def __init__(self):
        # åˆå§‹åŒ–æ„ŸçŸ¥æ¨¡å—
        self.detector = ObjectDetector(config={
            "mode": "fast",
            "confidence_threshold": 0.5
        })

        self.world_model = WorldModel(
            resolution=0.1,
            map_size=20.0,
            config={}
        )

        # æµ‹è¯•æŒ‡æ ‡
        self.metrics = TestMetrics()

        # æ¨¡æ‹Ÿåœºæ™¯
        self.objects = [
            {
                "type": ObjectType.PERSON,
                "position": Position3D(x=2.0, y=1.0, z=0.0),
                "velocity": (0.1, 0.0, 0.0),
                "bbox": (100, 100, 50, 120)
            },
            {
                "type": ObjectType.VEHICLE,
                "position": Position3D(x=5.0, y=3.0, z=0.0),
                "velocity": (0.0, 0.2, 0.0),
                "bbox": (300, 200, 150, 100)
            }
        ]

        print("âœ… æ„ŸçŸ¥æ¨¡æ‹Ÿå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   â€¢ ç›®æ ‡æ£€æµ‹å™¨: {self.detector.mode.value}æ¨¡å¼")
        print(f"   â€¢ ä¸–ç•Œæ¨¡å‹: åˆ†è¾¨ç‡0.1m, åœ°å›¾å¤§å°20m")
        print(f"   â€¢ æ¨¡æ‹Ÿå¯¹è±¡: {len(self.objects)}ä¸ª")

    def generate_frame(self, frame_num: int) -> tuple:
        """ç”Ÿæˆä¸€å¸§æ¨¡æ‹Ÿæ•°æ®"""
        # æ¨¡æ‹ŸRGBå›¾åƒ (640x480x3)
        rgb_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)

        # æ¨¡æ‹Ÿæ·±åº¦å›¾ (640x480)
        depth_image = np.random.uniform(0.5, 10.0, (480, 640)).astype(np.float32)

        # åœ¨å›¾åƒä¸Šæ·»åŠ ä¸€äº›"ç‰¹å¾"ï¼ˆæ¨¡æ‹ŸçœŸå®ç‰©ä½“ï¼‰
        for obj in self.objects:
            # æ›´æ–°ç‰©ä½“ä½ç½®ï¼ˆæ¨¡æ‹Ÿè¿åŠ¨ï¼‰
            if frame_num > 0:
                obj["position"] = Position3D(
                    x=obj["position"].x + obj["velocity"][0] * 0.1,
                    y=obj["position"].y + obj["velocity"][1] * 0.1,
                    z=obj["position"].z + obj["velocity"][2] * 0.1
                )

            # è¾¹ç•Œå¤„ç†
            obj["position"].x = max(0.1, min(19.9, obj["position"].x))
            obj["position"].y = max(0.1, min(19.9, obj["position"].y))

        return rgb_image, depth_image

    async def run_detection_pipeline(self, frame_num: int) -> dict:
        """è¿è¡Œæ£€æµ‹ç®¡é“"""
        start_time = time.time()

        # ç”Ÿæˆå¸§æ•°æ®
        rgb_image, depth_image = self.generate_frame(frame_num)

        # æ‰§è¡Œæ£€æµ‹
        detections = await self.detector.detect(rgb_image, depth_image)

        # æ‰§è¡Œè·Ÿè¸ª
        tracks = await self.detector.detect_and_track(rgb_image, depth_image)

        # æ›´æ–°ä¸–ç•Œæ¨¡å‹
        @dataclass
        class MockPerceptionData:
            pose: Pose2D
            scene_description: object = None
            semantic_objects: list = None

        perception_data = MockPerceptionData(
            pose=Pose2D(x=0.0, y=0.0, theta=0.0)
        )

        # æ·»åŠ æ¨¡æ‹Ÿçš„è¯­ä¹‰å¯¹è±¡
        detected_objects = []
        for track in tracks:
            obj = DetectedObject(
                id=track.track_id,
                label=track.object_type.value,
                confidence=1.0 - track.lost_frames * 0.1,
                position=track.position,
                velocity=track.velocity
            )
            detected_objects.append(obj)

        perception_data.semantic_objects = detected_objects
        self.world_model.update_with_perception(perception_data)
        self.metrics.world_model_updates += 1

        processing_time = time.time() - start_time

        return {
            "detections": detections,
            "tracks": tracks,
            "processing_time": processing_time
        }

    async def run_test(self, num_frames: int = 20):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        print(f"\n{'='*70}")
        print(f"ğŸ¯ å¼€å§‹æ„ŸçŸ¥å±‚å®Œæ•´æµç¨‹æµ‹è¯•")
        print(f"{'='*70}")
        print(f"\næµ‹è¯•é…ç½®:")
        print(f"  â€¢ æµ‹è¯•å¸§æ•°: {num_frames}")
        print(f"  â€¢ æ¨¡æ‹Ÿå¯¹è±¡: {len(self.objects)}ä¸ª")
        print(f"  â€¢ åœºæ™¯å¤§å°: 20m x 20m")
        print(f"\nå¼€å§‹å¤„ç†...\n")

        for frame_num in range(num_frames):
            print(f"\n{'â”€'*70}")
            print(f"ğŸ“¹ å¸§ {frame_num + 1}/{num_frames}")
            print(f"{'â”€'*70}")

            # æ‰§è¡Œæ£€æµ‹ç®¡é“
            result = await self.run_detection_pipeline(frame_num)

            # æ›´æ–°æŒ‡æ ‡
            self.metrics.total_frames += 1
            self.metrics.total_detections += len(result["detections"])
            self.metrics.processing_times.append(result["processing_time"])

            if len(result["tracks"]) > 0:
                self.metrics.tracking_success += 1

            # è¾“å‡ºç»“æœ
            print(f"âœ… æ£€æµ‹åˆ° {len(result['detections'])} ä¸ªç›®æ ‡")
            print(f"ğŸ¯ è·Ÿè¸ª {len(result['tracks'])} ä¸ªç‰©ä½“")
            print(f"â±ï¸  è€—æ—¶: {result['processing_time']*1000:.1f}ms")
            print(f"ğŸ“Š FPS: {1.0/result['processing_time']:.1f}")

            # æ˜¾ç¤ºè·Ÿè¸ªè¯¦æƒ…
            if len(result["tracks"]) > 0:
                print(f"\nè·Ÿè¸ªè¯¦æƒ…:")
                for i, track in enumerate(result["tracks"]):
                    print(f"  [{i+1}] ID: {track.track_id}")
                    print(f"       ç±»å‹: {track.object_type.value}")
                    print(f"       ä½ç½®: ({track.position.x:.2f}, {track.position.y:.2f}, {track.position.z:.2f})")
                    print(f"       é€Ÿåº¦: ({track.velocity.linear_x:.2f}, {track.velocity.linear_y:.2f}, {track.velocity.linear_z:.2f})")
                    print(f"       å†å²ç‚¹: {len(track.history)}")
                    print(f"       ä¸¢å¤±å¸§: {track.lost_frames}")

        # è·å–ä¸–ç•Œæ¨¡å‹ç»Ÿè®¡
        world_stats = self.world_model.get_map_statistics()
        print(f"\n{'â”€'*70}")
        print(f"ğŸŒ ä¸–ç•Œæ¨¡å‹ç»Ÿè®¡:")
        print(f"  â€¢ æ›´æ–°æ¬¡æ•°: {self.metrics.world_model_updates}")
        print(f"  â€¢ è¯­ä¹‰å¯¹è±¡æ•°: {world_stats['semantic_objects_count']}")
        print(f"  â€¢ åœ°å›¾è¦†ç›–ç‡: {world_stats['occupied_ratio']*100:.1f}%")
        print(f"  â€¢ åœ°å›¾ç½®ä¿¡åº¦: {world_stats['confidence']:.2f}")

    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print(f"\n{'='*70}")
        print(f"ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
        print(f"{'='*70}")

        print(f"\nâœ… å¤„ç†ç»Ÿè®¡:")
        print(f"  â€¢ æ€»å¸§æ•°: {self.metrics.total_frames}")
        print(f"  â€¢ æ€»æ£€æµ‹æ•°: {self.metrics.total_detections}")
        print(f"  â€¢ å¹³å‡æ¯å¸§: {self.metrics.total_detections/self.metrics.total_frames:.1f} ä¸ªç›®æ ‡")
        print(f"  â€¢ è·Ÿè¸ªæˆåŠŸ: {self.metrics.tracking_success}/{self.metrics.total_frames}")

        if self.metrics.processing_times:
            avg_time = np.mean(self.metrics.processing_times)
            print(f"\nâ±ï¸  æ€§èƒ½æŒ‡æ ‡:")
            print(f"  â€¢ å¹³å‡è€—æ—¶: {avg_time*1000:.1f}ms")
            print(f"  â€¢ æœ€å¿«: {min(self.metrics.processing_times)*1000:.1f}ms")
            print(f"  â€¢ æœ€æ…¢: {max(self.metrics.processing_times)*1000:.1f}ms")
            print(f"  â€¢ å¹³å‡FPS: {1.0/avg_time:.1f}")

        print(f"\nğŸ¯ æ¨¡å—éªŒè¯:")
        print(f"  âœ… ObjectDetector: æ£€æµ‹å’Œè·Ÿè¸ªåŠŸèƒ½æ­£å¸¸")
        print(f"  âœ… WorldModel: åœ°å›¾æ›´æ–°å’ŒæŸ¥è¯¢æ­£å¸¸")
        print(f"  âœ… æ•°æ®æµ: RGB-D â†’ æ£€æµ‹ â†’ è·Ÿè¸ª â†’ ä¸–ç•Œæ¨¡å‹")
        print(f"  âœ… å¼‚æ­¥å¤„ç†: async/awaitæ­£å¸¸å·¥ä½œ")

        print(f"\nğŸ“ è¯´æ˜:")
        print(f"  â€¢ æœ¬æµ‹è¯•ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®éªŒè¯åŠŸèƒ½å®Œæ•´æ€§")
        print(f"  â€¢ å®é™…éƒ¨ç½²æ—¶éœ€æ¥å…¥YOLOç­‰çœŸå®æ£€æµ‹æ¨¡å‹")
        print(f"  â€¢ æ•°æ®å¤„ç†æµç¨‹å·²éªŒè¯é€šè¿‡")
        print(f"  â€¢ æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½å·¥ä½œæ­£å¸¸")

        print(f"\n{'='*70}")
        print(f"âœ¨ æµ‹è¯•å®Œæˆï¼æ„ŸçŸ¥å±‚å·¥ä½œæ­£å¸¸")
        print(f"{'='*70}\n")


async def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("ğŸš€ æ„ŸçŸ¥å±‚å®Œæ•´æµç¨‹æµ‹è¯•")
    print("="*70)

    # åˆ›å»ºæ¨¡æ‹Ÿå™¨
    simulator = PerceptionSimulator()

    # è¿è¡Œæµ‹è¯•
    await simulator.run_test(num_frames=20)

    # æ‰“å°æ‘˜è¦
    simulator.print_summary()

    # è¿”å›æˆåŠŸ
    return 0


if __name__ == '__main__':
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
